{"Abstract": "", "Introduction": "INTRODUCTION Adversarial robustness of deep learning models has received signi\ufb01cant attention in recent years (see Kolter & Madry (2018) and references therein). The classic threat model of adversarial robustness considers an inductive setting where a model is learned at the training time and \ufb01xed, and then at the test time, an attacker attempts to thwart the \ufb01xed model with adversarially perturbed input. This gives rise to the adversarial training (Madry et al., 2018; Sinha et al., 2018; Schmidt et al., 2018; Carmon et al., 2019) to enhance adversarial robustness. Going beyond the inductive threat model, there has been an emerging interest in using transductive learning (Vapnik, 1998)1 for adversarial robustness (Goldwasser et al., 2020; Wu et al., 2020b; Wang et al., 2021). In essence, these defenses attempt to leverage a batch of test-time inputs, which is common for ML pipelines deployed with batch predictions (bat, 2021), to learn an updated model. The hope is that this \u201ctest-time learning\u201d may be useful for adversarial robustness since the defender can adapt the model to the perturbed input from the adversary, which is distinct from the inductive threat model where a model is \ufb01xed after training. This paper examines these defenses from a principled threat analysis perspective. We \ufb01rst formulate and analyze rigorous threat models. Our basic 1-round threat model considers a single-round game between the attacker and the defender. Roughly speaking, the attacker uses an objective maxV \u2032\u2208N(V ) La(\u0393(U \u2032), V \u2032) (formula (2)), where V is the given test batch, N(V ) is a neighborhood around V , La is a loss function for attack gain, \u0393 is the transductive-learning based defense, and U \u2032 = V \u2032|X, the projection of V \u2032 to features, is the adversarially perturbed data for breaking \u0393. This Our code is available at: https://github.com/jfc43/eval-transductive-robustness. 1We note that this type of defense goes under different names such as \u201ctest-time adaptation\u201d or \u201cdynamic defenses\u201d. Nevertheless, they all fall into the classic transductive learning paradigm (Vapnik, 1998), which attempts to leverage test data for learning. We thus call them transductive-learning based defenses. The word \u201ctransductive\u201d is also adopted in Goldwasser et al. (2020). 1 ", "Related Work": "RELATED WORK Adversarial robustness in the inductive setting. Many attacks have been proposed to evaluate the adversarial robustness of the defenses in the inductive setting where the model is \ufb01xed during the evaluation phase (Goodfellow et al., 2015; Carlini & Wagner, 2017; Kurakin et al., 2017; MoosaviDezfooli et al., 2016; Croce & Hein, 2020b). Principles for adaptive attacks have been developed in Tram\u00e8r et al. (2020) and many existing defenses are shown to be broken based on attacks developed from these principles (Athalye et al., 2018). A fundamental method to obtain adversarial robustness in this setting is adversarial training (Madry et al., 2018; Zhang et al., 2019). A state-of-the-art attack in the inductive threat model is AutoAttack (Croce & Hein, 2020a). Adversarial robustness via test-time defenses. There have been various work which attempt to improve adversarial robustness by leveraging test-time data. Many of such work attempt to \u201csanitize\u201d test-time input using a non-differentiable function, and then send it to a pretrained model. Most of these proposals were broken by BPDA (Athalye et al., 2018). To this end, we note that a research agenda for \u201cdynamic model defense\u201d has been proposed in Goodfellow (2019). Adversarial robustness using transductive learning. There has been emerging interesting in using transductive learning to improve adversarial robustness. In view of \u201cdynamic defenses\u201d, these proposals attempt to apply transductive learning to the test data and update the model, and then use the updated model to predict on the test data. In this work we consider three such work (Wu et al., 2020b; Wang et al., 2021; Goldwasser et al., 2020). 3 PRELIMINARIES Let F be a model, and for a data point (xxx, y) \u2208 X \u00d7 Y, a loss function \u2113(F;xxx, y) gives the loss of F on the point. Let V be a set of labeled data points, and let L(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2113(F;xxx, y) denote 2 Published as a conference paper at ICLR 2022 the empirical loss of F on V . For example, if we use binary loss \u21130,1(F;xxx, y) = 1[F(xxx) \u0338= y], this gives the test error of F on V . We use the notation V |X to denote the projection of V to its features, that is {(xxxi, yi)}m i=1|X \ufffd\u2192 {xxxi}m i=1. Throughout the paper, we use N(\u00b7) to denote a neighborhood function for perturbing features: That is, N(xxx) = {xxx\u2032 | d(xxx\u2032,xxx) < \u03f5} is a set of examples that are close to xxx in terms of a distance metric d (e.g., d(xxx\u2032,xxx) = \u2225xxx\u2032 \u2212xxx \u2225p). Given U = {xxxi}m i=1, let N(U) = {{xxx\u2032 i}m i=1 | d(xxx\u2032 i,xxxi) < \u03f5, i = 0, . . . , m}. Since labels are not changed for adversarial examples, we also use the notation N(V ) to denote perturbations of features, with labels \ufb01xed. 4 MODELING TRANSDUCTIVE ROBUSTNESS In this section we formulate and analyze threat models for transductive defenses. We \ufb01rst formulate a threat model for a single-round game between the attacker and the defender. We then consider extensions of this threat model to multiple rounds, which are necessary when considering DENT (Wang et al., 2021) and RMC (Wu et al., 2020b), and point out important subtleties in modeling that were not articulated in previous work. We characterize previous test-time defenses using our threat models. 1-round game. In this case, the adversary \u201cintercepts\u201d a clean test data V (with clean features U = V |X, and labels V |Y ), adversarially perturbs it, and sends a perturbed features U \u2032 to the defender. The defender learns a new model based on U \u2032. A referee then evaluates the accuracy of the adapted model on U \u2032. Formally: De\ufb01nition 1 (1-round threat model for transductive adversarial robustness). Fix an adversarial perturbation type (e.g., \u2113\u221e perturbations with perturbation budget \u03b5). Let PX,Y be a data generation distribution. The attacker is an algorithm A, and the defender is a pair of algorithms (T , \u0393), where T is a supervised learning algorithm, and \u0393 is a transductive learning algorithm. A (clean) training set D is sampled i.i.d. from PX,Y . A (clean) test set V is sampled i.i.d. from PX,Y . \u2022 [Training time, defender] The defender trains an optional base model F = T (D), using the labeled source data D. \u2022 [Test time, attacker] The attacker receives V , and produces an (adversarial) unlabeled dataset U \u2032: 1. On input \u0393, F, D, and V , A perturbs each point (xxx, y) \u2208 V to (xxx\u2032, y) (subject to the agreed attack type), giving V \u2032 = A(\u0393, F, D, V ) (that is, V \u2032 \u2208 N(V )). 2. Send U \u2032 = V \u2032|X (the feature vectors of V \u2032) to the defender. \u2022 [Test time, defender] The defender produces a model as F \u2217 = \u0393(F, D, U \u2032). Multi-round games. The extension of 1-round games to multi-round contains several important considerations that were implicit or unclear in previous work, and is closely related to what it means by adaptive attacks. Speci\ufb01cally: Private randomness. Note that \u0393 uses randomness, such as random initialization and random restarts2 in adversarial training. Since these randomness are generated after the attacker\u2019s move, they are treated as private randomness, and not known to the adversary. Intermediate defender states leaking vs. Non-leaking. In a multi-round game, the defender may maintain states across rounds. For example, the defender may store test data and updated models from previous rounds, and use them in a new round. If these intermediate defender states are \u201cleaked\u201d to the attacker, we call it intermediate defender states leaking, or simply states leaking, otherwise we call it non states-leaking, or simply non-leaking. Note that the attacker cannot simply compute these information by simulating on the training and testing data, due to the use of private randomness. We note that, however, the initial pretrained model is assumed to be known by the attacker. The attacker can also of course maintain arbitrary states, and are assumed not known to the defender. Adaptive vs. Non-adaptive. Because transductive learning happens after the attacker produces U \u2032, the attacker may not be able to directly attack the model \u0393 produced. Nevertheless, the attacker is assumed to have full knowledge of the transductive mechanism \u0393, except the private randomness. In this paper we call an attack adaptive if it makes explicit use of the knowledge of \u0393. Naturally ordered vs. Adversarially ordered. Both RMC and DENT handle batches of \ufb01xed sizes. An intuitive setup for multi-round game is that the batches come in sequentially, and the attacker 2When perturbing a data point during adversarial training, one starts with a random point in the neighborhood. 3 Published as a conference paper at ICLR 2022 must forward perturbed versions of these batches in the same order to the defender, which we call the \u201cnaturally ordered\u201d game. However, this formulation does not capture an important scenario: An adversary can wait and pool a large amount of test data, then chooses a \u201cworst-case\u201d order of perturbed data points, and then sends them in batches one at a time for adaptation in order to maximize the breach. We call the latter \u201cadversarially ordered\u201d game. We note that all previous work only considered naturally-ordered game, which gives the defender more advantages, and is thus our focus in the rest of the paper. Adversarially-ordered game is evaluated for DENT in Appendix A.7. Modeling capacity of our threat models. Our threat models encompass a large family of defenses. For example, without using \u0393, the threat model degenerates to the classic inductive threat model. Our threat models also capture various \u201ctest-time defenses\u201d proposals (e.g., those broken by the BPDA (Athalye et al., 2018)), where \u0393 is a \u201cnon-differentiable\u201d function which \u201csanitizes\u201d the test data, instead of updating the model, before sending them to a \ufb01xed pretrained model. Therefore, in particular, these proposals are not transductive-learning based. Below we describe previous defenses which we study in the rest of this paper, where \u0393 is indeed transductive learning. Example 1 (Runtime masking and cleansing). Runtime masking and cleansing (RMC) (Wu et al., 2020b) is a recent transductive-learning defense. For RMC, the defender is stateful and adapted from the model learned in the last round, on a single test point (|U| = 1): The adaptation objective is F \u2217 = arg minF \ufffd (xxx,y)\u2208N \u2032(\ufffdxxx) L(F,xxx, y), where \ufffdxxx is the test time feature point, and N \u2032(\ufffdxxx) is the set of examples in the adversarial training dataset D\u2032 that are top-K nearest to \ufffdxxx in a distance measure. RMC paper considered two attacks: (1) Transfer attack, which generates perturbed data by attacking the initial base model, and (2) PGD-skip attack, which at round p+1, runs PGD attack on the model learned at round p. In our language, transfer attack is stateless (i.e. the adversary maintains no state) and non-adaptive, PGD-skip attack is state-leaking, but still non-adaptive. Example 2 (Defensive entropy minimization (DENT (Wang et al., 2021))). DENT adapts the model using test input, and can work with any training-time learning procedure. The DENT defender is stateless: It always starts the adaptation from the original pretrained model, \ufb01xed at the training time. During the test-time adaptation, only the af\ufb01ne parameters in batch normalization layers of the base model are updated, using entropy minimization with the information maximization regularization. In this paper, we show that with strong adaptive attacks under the naturally ordered setting, we are able to reduce the robustness to be almost the same as that of static models (see Section 6). Further, under the adversarially ordered setting, we can completely break DENT. Example 3 (Goldwasser et al.\u2019s transductive threat model). While seemingly our threat model is quite different from the one described in Goldwasser et al. (2020), one can indeed recover their threat model naturally as a 1-round game: First, for the perturbation type, we simply allow arbitrary perturbations in the threat model setup. Second, we have a \ufb01xed pretrained model F, and the adaptation algorithm \u0393 learns a set S which represents the set of \u201callowable\u201d points (so F|S yields a predictor with redaction, namely it outputs \u22a5 for points outside of S). Third, we de\ufb01ne two error functions as (5) and (6) in Goldwasser et al. (2020): err U \u2032 (F|S, f) \u2261 1 |U \u2032| \ufffd\ufffd\ufffd\ufffd \ufffd xxx\u2032 \u2208 U \u2032 \u2229 S \ufffd\ufffd\ufffd\ufffdF(xxx\u2032) \u0338= f(xxx\u2032) \ufffd\ufffd\ufffd\ufffd\ufffd, rej U (S) \u2261 |U \\ S| |U| (1) where f is the ground truth hypothesis. The \ufb01rst equation measures prediction errors in U \u2032 that passed through S, and the second equation measures the rejection rate of the clean input. The referee evaluates by measuring two errors: L(F|S, V \u2032) = (errU \u2032(F|S), rejU(S)). 5 ADAPTIVE ATTACKS IN ONE ROUND In this section we study a basic question: How to perform adaptive attacks against a transductivelearning based defense in one round? Note that, in each round of a multi-round game, an independent batch of test input U is sampled, and the defender can use transductive learning to produce a model speci\ufb01cally adapted to the adversarial input U \u2032, after the defender receives it. Therefore, it is of fundamental interest to attack this ad-hoc adaptation. We consider white-box attacks: The attacker knows all the details of \u0393, except private randomness, which is sampled after the attacker\u2019s move. We deduce a principle for adaptive attacks in one round, which we call the principle of attacking model space: Effective attacks against a transductive defense may need to consider attacking a set 4 Published as a conference paper at ICLR 2022 of representative models induced in the neighborhood of U. We give concrete instantiations of this principle, and show in experiments that they break previous transductive-learning based defenses. Attacks in multi-round. If the transductive-learning based defense is stateless, then we simply repeat one-round attack multiple times. If it is stateful, then we need to consider state-leaking setting or non-leaking setting. For all experiments in Section 6, we only evaluate non-leaking setting, which is more challenging for the adversary. 5.1 GOAL OF THE ATTACKER AND CHALLENGES To start with, given a defense mechanism \u0393, the objective of the attacker can be formulated as: max V \u2032\u2208N(V ),U \u2032=V \u2032|X La(\u0393(F, D, U \u2032), V \u2032). (2) where La is the loss function of the attacker. We make some notational simpli\ufb01cations: Since D is a constant, in the following we drop it and write \u0393(U \u2032). Also, since the attacker does not modify the labels in the threat model, we abuse the notation and write the objective as max V \u2032,U \u2032=V \u2032|X La(\u0393(U \u2032), U \u2032). (3) A generic attacker would proceed iteratively as follows: It starts with the clean test set V , and generates a sequence of (hopefully) increasingly stronger attack sets U (0) = V |X, U (1), . . . , U (i) (U (i) must satisfy the attack constraints at U, such as \u2113\u221e bound). We note several basic but important differences between transductive attacks and inductive attacks in the classic minimax threat model: (D1) \u0393(U \u2032) is not differentiable. For the scenarios we are interested in, \u0393 is an optimization algorithm to solve an objective F \u2217 \u2208 arg minF Ld(F, D, U \u2032). This renders (3) into a bilevel optimization problem (Colson et al., 2007): max V \u2032\u2208N(V );U \u2032=V \u2032|X La(F \u2217, V \u2032) subject to: F \u2217 \u2208 arg min F Ld(F, D, U \u2032), (4) In these cases, \u0393 is in general not (in fact far from) differentiable. A natural attempt is to approximate \u0393 with a differentiable function, using theories such as Neural Tangent Kernels (Jacot et al., 2018). Unfortunately no existing theory applies to the transductive learning, which deals with unlabeled data U \u2032 (also, as we have remarked previously, tricks such as BPDA (Athalye et al., 2018) also does not apply because transductive learning is much more complex than test-time defenses considered there). (D2) U \u2032 appears in both attack and defense. Another signi\ufb01cant difference is that the attack set U \u2032 also appears as the input for the defense (i.e. \u0393(U \u2032)). Therefore, while it is easy to \ufb01nd U \u2032 to fail \u0393(U) for any \ufb01xed U, it is much harder to \ufb01nd a good direction to update the attack and converge to an attack set U \u2217 that fails an entire model space induced by itself: \u0393(U \u2217). (D3) \u0393(U \u2032) can be a random variable. In the classic minimax threat model, the attacker faces a \ufb01xed model. However, the output of \u0393 can be a random variable of models due to its private randomness, such as the case of Randomized Smoothing (Cohen et al., 2019). In these cases, successfully attacking a single sample of this random variable does not suf\ufb01ce. Algorithm 1 FIXED POINT ATTACK (FPA) Require: A transductive learning algorithm \u0393, an optional training dataset D, a clean test set V , an initial model F (0), and an integer parameter T \u2265 0 (the number of iterations). 2:1: for i = 0, 1, . . . , T do Attack the model obtained in the last iteration to get the perturbed set: V (i) = arg max V \u2032\u2208N(V ) La(F (i), V \u2032) (5) where La is a loss function. Set U (i) = V (i) |X. 3: Run the transductive learning algorithm \u0393 to get the next model: F (i+1) = \u0393(D, U (i)). 4: end for 5: Select the best attack set U (k) as k = arg max0\u2264i\u2264T L(F (i+1), V (i)). 6: return U (k). 5 Published as a conference paper at ICLR 2022 Fixed Point Attack: A \ufb01rst attempt. We adapt previous literature for solving bilevel optimization in deep learning setting (Lorraine & Duvenaud, 2018) (designed for supervised learning). The idea is simple: At iteration i + 1, we \ufb01x U (i) and model space F (i) = \u0393(U (i)), and construct U (i+1) to fail it. We call this the Fixed Point Attack (FPA) (Algorithm 1), as one hopes that this process converges to a good \ufb01xed point U \u2217. Unfortunately, we found FPA to be weak in experiments. The reason is exactly (D2): U (i+1) failing F (i) may not give any indication that it can also fail F (i+1) induced by itself. Note that transfer attack is a special case of FPA by setting T = 0. 5.2 STRONG ADAPTIVE ATTACKS FROM ATTACKING MODEL SPACES To develop stronger adaptive attacks, we consider a key property of the adversarial attacks: The transferability of adversarial examples. Various previous work have identi\ufb01ed that adversarial examples transfer (Tram\u00e8r et al., 2017; Liu et al., 2016), even across vastly different architectures and models. Therefore, if U \u2032 is a good attack set, we would expect that U \u2032 also fails \u0393(U) for U close to U \u2032. This leads to the consideration of the following objective: max U \u2032 min U\u2208N (U \u2032) La(\u0393(U), U \u2032). (6) where N(\u00b7) is a neighborhood function (possibly different than N). It induces a family of models {\u0393(U) | U \u2208 N(U \u2032)}, which we call a model space. (in fact, this can be a family of random variables of models) This can be viewed as a natural robust version of (3) by considering the transferability of U \u2032. While this is seemingly even harder to solve, it has several bene\ufb01ts: (1) Considering a model space naturally strengthens FPA. FPA naturally falls into this formulation as a weak instantiation where we consider a single U = U (i). Also, considering a model space gives the attacker more information in dealing with the nondifferentiability of \u0393 (D1). (2) It relaxes the attacker-defender constraint (D2). Perhaps more importantly, for the robust objective, we no longer need the same U \u2032 to appear in both defender and attacker. Therefore it gives a natural relaxation which makes attack algorithm design easier. In summary, while \u201cbrittle\u201d U \u2032 that does not transfer may indeed exist theoretically, their identi\ufb01cation can be challenging algorithmically, and its robust variant provides a natural relaxation considering both algorithmic feasibility and attack strength. This thus leads us to the following principle: The Principle of Attacking Model Spaces. An effective adaptive attack against a transductive-learning based defense may need to consider a model space induced by a proper neighborhood of U. Algorithm 2 GREEDY MODEL SPACE ATTACK (GMSA) Require: A transductive learning algorithm \u0393, an optional training dataset D, a clean test set V , an initial model F (0), and an integer parameter T \u2265 0 (the number of iterations). 2:1: for i = 0, 1, . . . , T do Attack the previous models to get the perturbed set: V (i) = arg max V \u2032\u2208N(V ) LGMSA({F (j)}i j=0, V \u2032) (7) where LGMSA is a loss function. Set U (i) = V (i) |X. 3: Run the transductive learning algorithm \u0393 to get the next model: F (i+1) = \u0393(D, U (i)). 4: end for 5: Select the best attack U (k) as k = arg max0\u2264i\u2264T L(F (i+1), V (i)), 6: return U (k). An instantiation: Greedy Model Space Attack (GMSA). We give a simplest possible instantiation of the principle, which we call the Greedy Model Space Attack (Algorithm 2). In experiments we use this instantiation to break previous defenses. In this instantiation, the family of model spaces to consider is just all the model spaces constructed in previous iterations. LGMSA({F (j)}i j=0, V \u2032) is a loss function that the attacker uses to attack the history model spaces. We consider two instantiations: (1) LAVG GMSA({F (j)}i j=0, V \u2032) = 1 i+1 \ufffdi j=0 La(F (j), V \u2032), (2) LMIN GMSA({F (j)}i j=0, V \u2032) = min0\u2264j\u2264i La(F (j), V \u2032), where LAVG GMSA gives attack algorithm GMSA-AVG, and LMIN GMSA gives attack algorithm GMSA-MIN. We solve (7) via Projected Gradient Decent (PGD) (the implementation details of GMSA can be found in Appendix A.1.3). 6 Published as a conference paper at ICLR 2022 Dataset Base Model Accuracy Robustness Static RMC Static RMC AA AA PGD FPA GMSA-AVG GMSA-MIN MNIST Standard 99.50 99.00 0.00 97.70 98.30 0.60 0.50 1.10 Madry et al. 99.60 97.00 87.70 95.70 96.10 59.50 61.40 58.80 CIFAR-10 Standard 94.30 93.10 0.00 94.20 97.60 8.50 8.00 8.10 Madry et al. 83.20 90.90 44.30 77.90 71.70 40.80 42.50 39.60 Table 1: Results of evaluating RMC. We also evaluate the static base model for comparison. Bold numbers are worst results. Base Model Accuracy Robustness Static DENT Static DENT AA DENT-AA AA PGD FPA GMSA-AVG GMSA-MIN Wu et al. (2020a) 85.70 86.10 58.00 78.80 64.40 59.50 59.30 59.60 59.60 Carmon et al. (2019) 88.00 87.40 57.30 80.10 61.70 58.40 58.40 58.50 58.50 Sehwag et al. (2020) 87.30 86.90 54.90 76.50 59.60 55.80 55.80 55.80 55.80 Wang et al. (2020) 86.60 85.60 53.60 75.90 61.30 55.90 55.80 56.10 56.10 Hendrycks et al. (2019) 85.80 85.50 51.80 77.20 58.40 54.20 54.40 54.20 54.20 Wong et al. (2020) 81.20 81.00 42.40 69.70 48.90 44.10 44.30 44.50 44.30 Ding et al. (2020) 82.40 82.40 39.70 62.80 44.80 39.90 39.40 39.10 39.20 Table 2: Results of evaluating DENT on CIFAR-10. We also evaluate the static base model for comparison. Bold numbers are worst results. 6 EMPIRICAL STUDY This section evaluates various transductive-learning based defenses. Our main \ufb01ndings are: (1) The robustness of existing transductive defenses like RMC and DENT is overestimated. Under our evaluation framework, those defenses either have little robustness or have almost the same robustness as that of the static base model. To this end, we note that while AutoAttack is effective in evaluating the robustness of static models, it is not effective in evaluating the robustness of transductive defenses. In contrast, our GMSA attack is a strong baseline for attacking transductive defenses. (2) We experimented a novel idea of applying Domain Adversarial Neural Networks (Ajakan et al., 2014), an unsupervised domain adaptation technique (Wilson & Cook, 2020), as a transductive-learning based defense. We show that DANN has nontrivial and even better robustness compared to existing work, under AutoAttack, PGD attack, and FPA attack, even though it is broken by GMSA. (3) We report a somewhat surprising phenomenon on transductive adversarial training: Adversarially retraining the model using fresh private randomness on a new batch of test-time data gives a signi\ufb01cant increase in robustness, against all of our considered attacks. (4) Finally, we demonstrated that URejectron, while enjoying theoretical guarantees in the bounded-VC dimensions situation, can be broken in natural deep learning settings. Evaluation framework. For each defense, we report accuracy and robustness. The accuracy is the performance on the clean test inputs, and the robustness is the performance under adversarial attacks. The robustness of transductive defenses is estimated using AutoAttack (AA)3, PGD attack, FPA, GMSA-MIN and GMSA-AVG. We use PGD attack and AutoAttack in the transfer attack setting for the transductive defense: We generate adversarial examples by attacking a static model (e.g. the base model used by the transductive defense), and then evaluate the transductive defense on the generated adversarial examples. Accuracy and robustness of the static models are also reported for comparison. We always use AutoAttack to estimate the robustness of static models since it is the state-of-the-art for the inductive setting. For all experiments, the defender uses his own private randomness, which is different from the one used by the attacker. Without speci\ufb01ed otherwise, all reported values are percentages. Below we give details. Appendix A gives details for replicating the results. Runtime Masking and Cleansing (RMC (Wu et al., 2020b)). RMC adapts the network at test time, and was shown to achieve state-of-the-art robustness under several attacks that are unaware of the defense mechanism (thus these attacks are non-adaptive according to our de\ufb01nition). We follow the setup in Wu et al. (2020b) to perform experiments on MNIST and CIFAR-10 to evaluate the 3We use the standard version of AutoAttack: https://github.com/fra31/auto-attack/. 7 Published as a conference paper at ICLR 2022 Dataset Accuracy Robustness Standard Madry et al. DANN Standard Madry et al. DANN AA AA AA PGD FPA GMSA-AVG GMSA-MIN MNIST 99.42 99.16 99.27 0.00 88.92 97.59 96.66 96.81 79.37 6.17 CIFAR-10 93.95 86.06 89.61 0.00 39.49 66.61 60.54 53.98 5.53 8.56 Table 3: Results of evaluating DANN. Bold numbers are worst results. robustness of RMC. On MNIST, we consider L\u221e norm attack with \u03f5 = 0.3 and on CIFAR-10, we consider L\u221e norm attack with \u03f5 = 8/255. The performance of RMC is evaluated on a sequence of test points xxx(1), \u00b7 \u00b7 \u00b7 ,xxx(n) randomly sampled from the test dataset. So we have a n-round game. The FPA and GMSA attacks are applied on each round and the initial model F (0) used by the attacks at the (k + 1)-th round is the adapted model (with calibration in RMC) obtained at the k-th round. To save computational cost, we set n = 1000. The robustness of RMC is evaluated on a sequence of adversarial examples \u02c6xxx(1), \u00b7 \u00b7 \u00b7 , \u02c6xxx(n) generated by the attacker on the sequence of test points xxx(1), \u00b7 \u00b7 \u00b7 ,xxx(n). We evaluate the robustness of RMC in the non-state leaking setting with private randomness (both are in favor of the defender). Results. The results are in Table 1. RMC with the standard model is already broken by FPA attack (weaker than GSMA). Compared to the defense-unaware AutoAttack, our GMSA-AVG attack reduces the robustness from 97.70% to 0.50% on MNIST and from 94.20% to 8.00% on CIFAR-10. Further, RMC with adversarially trained model actually provides worse adversarial robustness than using adversarial training alone. Under our GMSA-MIN attack, the robustness is reduced from 96.10% to 58.80% on MNIST and from 71.70% to 39.60% on CIFAR-10. Defensive Entropy Minimization (DENT (Wang et al., 2021)). DENT performs test-time adaptation, and works for any training-time learner. It was shown that DENT improves the robustness of the state-of-the-art adversarial training defenses by 20+ points absolute against AutoAttack on CIFAR-10 under L\u221e norm attack with \u03f5 = 8/255 (DENT is implemented as a model module, and AutoAttack is directly applied to the module, and we denote this as DENT-AA). Wang et al. also considers adaptive attacks for DENT, such as attacking the static base model using AutoAttack to generate adversarial examples, which is the same as the AutoAttack (AA) in our evaluation. We evaluate the best version of DENT, called DENT+ in Wang et al., under their original settings on CIFAR-10: DENT is combined with various adversarial training defenses, and only the model adaptation is included without input adaptation. The model is adapted sample-wise for six steps by AdaMod (Ding et al., 2019) with learning rate of 0.006, batch size of 128 and no weight decay. The adaptation objective is entropy minimization with the information maximization regularization. To save computational cost, we only evaluate on 1000 examples randomly sampled from the test dataset. We consider L\u221e norm attack with \u03f5 = 8/255. We design loss functions for the attacks to generate adversarial examples with high con\ufb01dence (See Appendix A.3 for the details). Results. Table 2 shows that both DENT-AA and AA overestimate the robustness of DENT. Our PGD attack reduces the robustness of DENT to be almost the same as that of the static defenses. Further, our FPA, GMSA-AVG and GMSA-MIN have similar performance as the PGD attack. The results show that AutoAttack is not effective in evaluating the robustness of transductive defenses. Domain Adversarial Neural Network (DANN (Ajakan et al., 2014)). We consider DANN as a transductive defense for adversarial robustness. We train DANN on the labeled training dataset D (source domain) and unlabeled adversarial test dataset U \u2032 (target domain), and then evaluate DANN on U \u2032. For each adversarial set U \u2032, we train a new DANN model from scratch. We use the standard model trained on D as the base model for DANN. We perform experiments on MNIST and CIFAR-10 to evaluate the adversarial robustness of DANN. On MNIST, we consider L\u221e norm attack with \u03f5 = 0.3 and on CIFAR-10, we consider L\u221e norm attack with \u03f5 = 8/255. Results. Table 3 shows that DANN has non-trivial robustness under AutoAttack, PGD attack and FPA attack. However, under our GMSA attack, DANN has little robustness. Transductive Adversarial Training (TADV). We consider a simple but novel transductive-learning based defense called transductive adversarial training: After receiving a set of examples at the test time, we always adversarially retrain the model using fresh randomness. The key point of this transduction is that private randomness is sampled after the attacker\u2019s move, and so the attacker 8 ", "Conclusion": "CONCLUSION In this paper, we formulate threat models for transductive defenses and propose an attack framework called Greedy Model Space Attack (GMSA) that can serve as a new baseline for evaluating transductive defenses. We show that GMSA can break previous transductive defenses, which were resilient to previous attacks such as AutoAttack. On the positive side, we show that transductive adversarial training gives a signi\ufb01cant increase in robustness against attacks we consider. For the future work, one can explore transductive defenses that can be robust under our GMSA attacks, and can also explore even stronger adaptive attacks that are effective in evaluating transductive defenses. 9 ", "References": "REFERENCES Online versus batch prediction. https://cloud.google.com/ai-platform/prediction/docs/ online-vs-batch-prediction, 2021. Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, and Mario Marchand. Domain-adversarial neural networks. stat, 1050:15, 2014. Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 274\u2013283. PMLR, 2018. Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pp. 39\u201357. IEEE Computer Society, 2017. doi: 10.1109/SP.2017.49. URL https://doi.org/10.1109/SP.2017.49. Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, and Percy Liang. Unlabeled data improves adversarial robustness. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 11190\u201311201, 2019. Ching-Yao Chuang, Antonio Torralba, and Stefanie Jegelka. Estimating generalization under distribution shifts via domain-invariant representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 1984\u20131994. PMLR, 2020. URL http://proceedings.mlr.press/v119/chuang20a.html. Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certi\ufb01ed adversarial robustness via randomized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 1310\u20131320. PMLR, 2019. Beno\u00eet Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization, 2007. Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In ICML, 2020a. Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 2206\u20132216. PMLR, 2020b. URL http://proceedings.mlr.press/v119/croce20b.html. 10 Published as a conference paper at ICLR 2022 Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. MMA training: Direct input space margin maximization through adversarial training. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=HkeryxBtPB. Jianbang Ding, Xuancheng Ren, Ruixuan Luo, and Xu Sun. An adaptive and momental bound method for stochastic learning. CoRR, abs/1910.12249, 2019. URL http://arxiv.org/abs/1910.12249. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1\u201359:35, 2016. URL http://jmlr.org/papers/v17/15-239.html. Sha\ufb01 Goldwasser, Adam Tauman Kalai, Yael Tauman Kalai, and Omar Montasser. Beyond perturbations: Learning guarantees with arbitrary adversarial test examples. CoRR, abs/2007.05145, 2020. URL https: //arxiv.org/abs/2007.05145. Ian J. Goodfellow. A research agenda: Dynamic models to defend against correlated attacks. CoRR, abs/1903.06293, 2019. URL http://arxiv.org/abs/1903.06293. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6572. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https: //doi.org/10.1109/CVPR.2016.90. Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= HJz6tiCqYm. Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and uncertainty. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2712\u20132721. PMLR, 2019. URL http://proceedings. mlr.press/v97/hendrycks19a.html. Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ 5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf. Zico Kolter and Aleksander Madry. Adversarial Robustness - Theory and Practice. https:// adversarial-ml-tutorial.org/, 2018. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id= HJGU3Rodl. Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. CoRR, abs/1611.02770, 2016. URL http://arxiv.org/abs/1611.02770. Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernetworks. CoRR, abs/1802.09419, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1802. html#abs-1802-09419. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. 11 Published as a conference paper at ICLR 2022 Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2574\u20132582. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.282. URL https://doi.org/10.1109/CVPR.2016.282. Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. In Advances in Neural Information Processing Systems, pp. 5014\u20135026, 2018. Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana. HYDRA: pruning adversarially robust neural networks. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ e3a72c791a69f87b05ea7742e04430ed-Abstract.html. Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness with principled adversarial training. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traf\ufb01c sign recognition. Neural Networks, 32:323\u2013332, 2012. doi: 10.1016/j. neunet.2012.02.016. URL https://doi.org/10.1016/j.neunet.2012.02.016. Florian Tram\u00e8r, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 11f38f8ecd71867b42433548d1078e38-Abstract.html. Florian Tram\u00e8r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of transferable adversarial examples. arXiv, 2017. URL https://arxiv.org/abs/1704.03453. Vladimir Vapnik. Statistical learning theory. Wiley, 1998. ISBN 978-0-471-03003-4. Dequan Wang, An Ju, Evan Shelhamer, David Wagner, and Trevor Darrell. Fighting gradients with gradients: Dynamic defenses against adversarial attacks. arXiv preprint arXiv:2105.08714, 2021. Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassi\ufb01ed examples. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rklOg6EFwS. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1\u201346, 2020. Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=BJx040EFvH. Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020a. URL https://proceedings.neurips.cc/paper/2020/hash/ 1ef91c212e30e14bf125e9374262401f-Abstract.html. Yi-Hsuan Wu, Chia-Hung Yuan, and Shan-Hung Wu. Adversarial robustness via runtime masking and cleansing. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 10399\u201310409. PMLR, 2020b. URL http://proceedings.mlr.press/v119/wu20f.html. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 7472\u20137482. PMLR, 2019. URL http://proceedings.mlr.press/v97/zhang19p.html. 12 Published as a conference paper at ICLR 2022 Supplementary Material Towards Evaluating the Robustness of Neural Networks Learned by Transduction A EXPERIMENTAL DETAILS A.1 GENERAL SETUP A.1.1 COMPUTING INFRASTRUCTURE We run all experiments with PyTorch and NVIDIA GeForce RTX 2080Ti GPUs. A.1.2 DATASET We use three datasets MNIST, CIFAR-10 and GTSRB in our experiments. The details about these datasets are described below. MNIST. The MNIST (LeCun, 1998) is a large dataset of handwritten digits. Each digit has 5,500 training images and 1,000 test images. Each image is a 28 \u00d7 28 grayscale. We normalize the range of pixel values to [0, 1]. CIFAR-10. The CIFAR-10 (Krizhevsky et al., 2009) is a dataset of 32x32 color images with ten classes, each consisting of 5,000 training images and 1,000 test images. The classes correspond to dogs, frogs, ships, trucks, etc. We normalize the range of pixel values to [0, 1]. GTSRB. The German Traf\ufb01c Sign Recognition Benchmark (GTSRB) (Stallkamp et al., 2012) is a dataset of color images depicting 43 different traf\ufb01c signs. The images are not of a \ufb01xed dimensions and have rich background and varying light conditions as would be expected of photographed images of traf\ufb01c signs. There are about 34,799 training images, 4,410 validation images and 12,630 test images. We resize each image to 32 \u00d7 32. The dataset has a large imbalance in the number of sample occurrences across classes. We use data augmentation techniques to enlarge the training data and make the number of samples in each class balanced. We construct a class preserving data augmentation pipeline consisting of rotation, translation, and projection transforms and apply this pipeline to images in the training set until each class contained 10,000 examples. We also preprocess images via image brightness normalization and normalize the range of pixel values to [0, 1]. A.1.3 IMPLEMENTATION DETAILS OF THE ATTACKS We use Projected Gradient Descent (PGD) (Madry et al., 2018) to solve the attack objectives of PGD attack, FPA, GMSA-AVG and GMSA-MIN. For GMSA-AVG, at the i-th iteration, when applying PGD on the data point xxx to generate the perturbation \u03b4, we need to do one backpropagation operation for each model in {F (j)}i j=0 per PGD step. We do the backpropagation for each model sequentially and then accumulate the gradients to update the perturbation \u03b4 since we might not have enough memory to store all the models and compute the gradients at once, especially when i is large. For GMSA-MIN, we \ufb01nd that it requires more PGD steps to solve the attack objective at the i-th iteration where we need to attack i + 1 models simultaneously. Thus, we scale the number of PGD steps at the i-th iteration by a factor of i + 1 for GMSA-MIN. A.2 SETUP FOR RMC EXPERIMENTS We follow the original settings in Wu et al. (2020b) to perform experiments on MNIST and CIFAR-10 datasets to evaluate the adversarial robustness of RMC. We consider two kinds of base models for RMC: one is the model trained via standard supervised training; the other is the model trained using the adversarial training (Madry et al., 2018). We describe the settings for each dataset below. A.2.1 MNIST Model architecture and training con\ufb01guration. We use a neural network with two convolutional layers, two full connected layers and batch normalization layers. For both standard training and 13 Published as a conference paper at ICLR 2022 adversarial training, we train the model for 100 epochs using the Adam optimizer with a batch size of 128 and a learning rate of 10\u22123. We use the L\u221e norm PGD attack as the adversary for adversarial training with a perturbation budget \u03f5 of 0.3, a step size of 0.01, and number of steps of 40. RMC con\ufb01guration. We set K = 1024. Suppose the clean training set is D. Let D\u2032 contain |D| clean inputs and |D| adversarial examples. So N \u2032 = 2|D|. We generate the adversarial examples using the L\u221e norm PGD attack with a perturbation budget \u03f5 of 0.3, a step size of 0.01, and number of steps of 100. We extract the features from the penultimate layer of the model and use the Euclidean distance in the feature space of the model to \ufb01nd the top-K nearest neighbors of the inputs. When adapting the model, we use Adam as the optimizer and set the learning rate to be 2 \u00d7 10\u22124. We train the model until the early-stop condition holds. That is the training epoch reaches 100 or the validation loss doesn\u2019t decrease for 5 epochs. Attack con\ufb01guration. We use the same threat model for all attacks: L\u221e norm perturbation with a perturbation budget \u03f5 of 0.3. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: La(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212 log f(xxx)y, where f(xxx) is the softmax output of the model F. We use PGD with a step size of 0.01, the number of steps of 100, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN. A.2.2 CIFAR-10 Model architecture and training con\ufb01guration. We use the ResNet-32 network (He et al., 2016). For both standard training and adversarial training, we train the model for 100 epochs using Stochastic Gradient Decent (SGD) optimizer with Nesterov momentum and learning rate schedule. We set momentum 0.9 and \u21132 weight decay with a coef\ufb01cient of 10\u22124. The initial learning rate is 0.1 and it decreases by 0.1 at 50, 75 and 90 epoch respectively. The batch size is 128. We augment the training images using random crop and random horizontal \ufb02ip. We use the L\u221e norm PGD attack as the adversary for adversarial training with a perturbation budget \u03f5 of 8 255, a step size of 2 255, and number of steps of 10. RMC con\ufb01guration. We set K = 1024. Suppose the clean training set is D. Let D\u2032 contain |D| clean inputs and 4|D| adversarial examples. So N \u2032 = 5|D|. We generate the adversarial examples using the L\u221e norm PGD attack with a perturbation budget \u03f5 of 8 255, a step size of 1 255, and number of steps of 40. We extract the features from the penultimate layer of the model and use the Euclidean distance in the feature space of the model to \ufb01nd the top-K nearest neighbors of the inputs. We use Adam as the optimizer and set the learning rate to be 2.5 \u00d7 10\u22125. Attack con\ufb01guration. We use the same threat model for all attacks: L\u221e norm perturbation with a perturbation budget \u03f5 of 8 255. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: La(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212 log f(xxx)y, where f(xxx) is the softmax output of the model F. We use PGD with a step size of 1 255, the number of steps of 40, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN. A.3 SETUP FOR DENT EXPERIMENTS DENT con\ufb01guration. We perform experiments to evaluate the best version of DENT (DENT+ in Wang et al. (2021)) on CIFAR-10 following the experimental settings in Wang et al. (2021). We use the pre-trained robust models on CIFAR-10 under the L\u221e norm perturbation threat model from RobustBench Model Zoo4 as the static base models for DENT, including models with model ID Wu2020Adversarial_extra (Wu et al., 2020a), Carmon2019Unlabeled (Carmon et al., 2019), Sehwag2020Hydra (Sehwag et al., 2020), Wang2020Improving (Wang et al., 2020), Hendrycks2019Using (Hendrycks et al., 2019), Wong2020Fast (Wong et al., 2020), and Ding2020MMA (Ding et al., 2020). For the test-time adaptation, only the af\ufb01ne scale \u03b3 and shift \u03b2 parameters in the batch normalization layers of the base model are updated. DENT updates sample-wise with different af\ufb01ne parameters (\u03b3i, \u03b2i) for each input xxxi. The input adaptation of \u03a3 is not used as suggested in Wang et al. (2021). The model is adapted for six steps by AdaMod (Ding et al., 2019) with learning rate of 0.006, batch size of 128 and no weight decay. The adaptation 4https://github.com/RobustBench/robustbench 14 Published as a conference paper at ICLR 2022 objective is entropy minimization with the information maximization regularization: min \u03b8i b \ufffd i=1 \u2212 C \ufffd c=1 f(xxxi; \u03b8i)c \u00b7 log f(xxxi; \u03b8i)c + C \ufffd c=1 b \ufffd i=1 f(xxxi; \u03b8i)c \u00b7 log b \ufffd i=1 f(xxxi; \u03b8i)c (8) where b is the batch size, C is the number of classes and f(xxxi; \u03b8i) is the softmax output of the model f with the af\ufb01ne parameters \u03b8i = (\u03b3i, \u03b2i) for the input xxxi. Attack con\ufb01guration. We use the same threat model for all attacks: L\u221e norm perturbation with a perturbation budget \u03f5 of 8 255. For PGD attack, FPA, GMSA-AVG and GMSA-MIN, we use the following loss function to \ufb01nd adversarial examples with high con\ufb01dence: La(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V maxk\u0338=y f(xxx)k, where f(xxx) is the softmax output of the model F. However, it is hard to optimize this loss function. Thus, we use two alternative loss functions to \ufb01nd adversarial examples. One is the untargeted CW loss (Carlini & Wagner, 2017): L1 a(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212Z(xxx)y + maxk\u0338=y Z(xxx)k, where Z(xxx) is the logits of the model F (the output of the layer before the softmax layer). The other is the targeted CW loss: L2 a(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212Z(xxx)y + Z(xxx)t, where t is the targeted label and t \u0338= y. For each attack, we use 14 PGD subroutines to solve its attack objective, including 5 PGD subroutines using the untargeted CW loss L1 a with different random restarts and 9 PGD subroutines using the targeted CW loss L2 a with different targeted labels. So for each clean test input xxx, these PGD subroutines will return 14 adversarial examples xxx\u2032 1, . . . ,xxx\u2032 14. Among these adversarial examples, we select the one that maximizes the attack loss with the loss function La(F, V ) as the \ufb01nal adversarial example xxx\u2032 for xxx. We use the same hyper-parameters for all PGD subroutines: the step size is 1 255, the number of steps is 100, and the random start is used. We set T = 2 for FPA, GMSA-AVG and GMSA-MIN. A.4 SETUP FOR DANN EXPERIMENTS We perform experiments on MNIST and CIFAR-10 datasets. We describe the settings for each dataset below. A.4.1 MNIST Model architecture. We use the same model architecture as the one used in Chuang et al. (2020), which is shown below. Encoder nn.Conv2d(3, 64, kernel_size=5) nn.BatchNorm2d nn.MaxPool2d(2) nn.ReLU nn.Conv2d(64, 128, kernel_size=5) nn.BatchNorm2d nn.Dropout2d nn.MaxPool2d(2) nn.ReLU nn.Conv2d(128, 128, kernel_size=3, padding=1) nn.BatchNorm2d nn.ReLU \u00d72 15 Published as a conference paper at ICLR 2022 Predictor nn.Conv2d(128, 128, kernel_size=3, padding=1) nn.BatchNorm2d nn.ReLU \u00d73 \ufb02atten nn.Linear(2048, 256) nn.BatchNorm1d nn.ReLU nn.Linear(256, 10) nn.Softmax Discriminator nn.Conv2d(128, 128, kernel_size=3, padding=1) nn.ReLU \u00d75 Flatten nn.Linear(2048, 256) nn.ReLU nn.Linear(256, 2) nn.Softmax Training con\ufb01guration. We train the models for 100 epochs using the Adam optimizer with a batch size of 128 and a learning rate of 10\u22123. For the representation matching in DANN, we adopt the original progressive training strategy for the discriminator (Ganin et al., 2016) where the weight \u03b1 for the domain-invariant loss is initiated at 0 and is gradually changed to 0.1 using the schedule \u03b1 = ( 2 1+exp(\u221210\u00b7p) \u2212 1) \u00b7 0.1, where p is the training progress linearly changing from 0 to 1. Attack con\ufb01guration. We use the same threat model for all attacks: L\u221e norm perturbation with a perturbation budget \u03f5 of 0.3. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: La(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212 log f(xxx)y, where f(xxx) is the softmax output of the model F. We use PGD with a step size of 0.01, the number of steps of 200, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN. A.4.2 CIFAR-10 Model architecture. We use the ResNet-18 network (He et al., 2016) and extract the features from the third basic block for representation matching. The detailed model architecture is shown below. Encoder nn.Conv2d(3, 64, kernel_size=3) nn.BatchNorm2d nn.ReLU BasicBlock(in_planes=64, planes=2, stride=1) BasicBlock(in_planes=128, planes=2, stride=2) BasicBlock(in_planes=256, planes=2, stride=2) Predictor BasicBlock(in_planes=512, planes=2, stride=2) avg_pool2d \ufb02atten nn.Linear(512, 10) nn.Softmax Discriminator BasicBlock(in_planes=512, planes=2, stride=2) avg_pool2d \ufb02atten nn.Linear(512, 2) nn.Softmax Training con\ufb01guration. We train the models for 100 epochs using stochastic gradient decent (SGD) optimizer with Nesterov momentum and learning rate schedule. We set momentum 0.9 and \u21132 weight decay with a coef\ufb01cient of 10\u22124. The initial learning rate is 0.1 and it decreases by 0.1 at 50, 75 and 90 epoch respectively. The batch size is 64. We augment the training images using random crop and random horizontal \ufb02ip. For the representation matching in DANN, we adopt the original progressive training strategy for the discriminator (Ganin et al., 2016) where the weight \u03b1 for the domain-invariant loss is initiated at 0 and is gradually changed to 1 using the schedule \u03b1 = 2 1+exp(\u221210\u00b7p) \u2212 1, where p is the training progress linearly changing from 0 to 1. Attack con\ufb01guration. We use the same threat model for all attacks: L\u221e norm perturbation with a perturbation budget \u03f5 of 8 255. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: La(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212 log f(xxx)y, where f(xxx) is the softmax output of the model F. We use PGD with a step size of 1 255, the number of steps of 100, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN. 16 ", "Experiments": "EXPERIMENTS We perform experiments on MNIST and CIFAR-10 datasets. We describe the settings for each dataset below. A.5.1 MNIST Model architecture and Training con\ufb01guration. We use the LeNet network architecture. We train the models for 100 epochs using the Adam optimizer with a batch size of 128 and a learning rate of 10\u22123. We use the L\u221e norm PGD attack as the adversary to generate adversarial training examples with a perturbation budget \u03f5 of 0.3, a step size of 0.01, and number of steps of 40. We train on 50% clean and 50% adversarial examples per batch. Attack con\ufb01guration. We use the same threat model for all attacks: L\u221e norm perturbation with a perturbation budget \u03f5 of 0.3. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: La(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212 log f(xxx)y, where f(xxx) is the softmax output of the model F. We use PGD with a step size of 0.01, the number of steps of 200, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN. A.5.2 CIFAR-10 Model architecture and Training con\ufb01guration. We use the ResNet-20 network architecture (He et al., 2016). We train the models for 110 epochs using stochastic gradient decent (SGD) optimizer with Nesterov momentum and learning rate schedule. We set momentum 0.9 and \u21132 weight decay with a coef\ufb01cient of 5 \u00d7 10\u22124. The initial learning rate is 0.1 and it decreases by 0.1 at 100 and 105 epoch respectively. The batch size is 128. We augment the training images using random crop and random horizontal \ufb02ip. We use the L\u221e norm PGD attack as the adversary to generate adversarial training examples with a perturbation budget \u03f5 of 8 255, a step size of 2 255, and number of steps of 10. We train on 50% clean and 50% adversarial examples per batch. Attack con\ufb01guration. We use the same threat model for all attacks: L\u221e norm perturbation with a perturbation budget \u03f5 of 8 255. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: La(F, V ) = 1 |V | \ufffd (xxx,y)\u2208V \u2212 log f(xxx)y, where f(xxx) is the softmax output of the model F. We use PGD with a step size of 1 255, the number of steps of 100, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN. A.6 SETUP FOR UREJECTRON EXPERIMENTS We use a subset of the GTSRB augmented training data for our experiments, which has 10 classes and contains 10,000 images for each class. We implement URejectron (Goldwasser et al., 2020) on this dataset using the ResNet18 network (He et al., 2016) in the transductive setting. Following Goldwasser et al. (2020), we implement the basic form of the URejectron algorithm, with T = 1 iteration. That is we train a discriminator h to distinguish between examples from P and Q, and train a classi\ufb01er F on P. Speci\ufb01cally, we randomly split the data into a training set Dtrain containing 63,000 images, a validation set Dval containing 7,000 images and a test set Dtest containing 30,000 images. We then use the training set Dtrain to train a classi\ufb01er F using the ResNet18 network. We train the classi\ufb01er F for 10 epochs using Adam optimizer with a batch size of 128 and a learning rate of 10\u22123. The accuracy of the classi\ufb01er on the training set Dtrain is 99.90% and its accuracy on the validation set Dval is 99.63%. We construct a set \u02dcx consisting of 50% normal examples and 50% adversarial examples. The normal examples in the set \u02dcx form a set z. We train the discriminator h on the set Dtrain (with label 0) and the set \u02dcx (with label 1). We then evaluate URejectron\u2019s performance on \u02dcx: under a certain threshold used by the discriminator h, we measure the fraction of normal examples in z that are rejected by the discriminator h and the error rate of the classi\ufb01er F on the examples in the set \u02dcx that are accepted by the discriminator h. The set z can be Dtest or a set of corrupted images generated on Dtest. We use the method proposed in Hendrycks & Dietterich (2019) to generate corrupted images with the corruption type of brightness and the severity level of 1. The accuracy of the classi\ufb01er on the corrupted images is 98.90%. The adversarial examples in \u02dcx are generated by the PGD attack (Madry et al., 2018) or the CW attack (Carlini & Wagner, 2017). For PGD attack, we use L\u221e norm with perturbation budget \u03f5 = 8 255 and random initialization. The number of iterations is 40 and the step 17 ", "Experiment": "EXPERIMENT In Section 6, we describe the experimental setup for evaluating RMC. The performance of RMC is evaluated on a sequence of test points xxx(1), \u00b7 \u00b7 \u00b7 ,xxx(n) randomly sampled from the test dataset. We repeat this experiment \ufb01ve times with different random seeds and report the mean and standard deviation of the results over the multiple random runs of the experiment. When evaluating the robustness of RMC, we only use the GMSA-AVG attack and the GMSA-MIN attack since they are the strongest attacks. From Table 6, we can see that the results don\u2019t vary much across different random runs and the conclusion that the proposed GMSA attacks can break RMC still holds. 18 ", "title": "Supplementary Material", "paper_info": "Published as a conference paper at ICLR 2022\nSupplementary Material\nTowards Evaluating the Robustness of Neural Networks Learned by\nTransduction\nA\nEXPERIMENTAL DETAILS\nA.1\nGENERAL SETUP\nA.1.1\nCOMPUTING INFRASTRUCTURE\nWe run all experiments with PyTorch and NVIDIA GeForce RTX 2080Ti GPUs.\nA.1.2\nDATASET\nWe use three datasets MNIST, CIFAR-10 and GTSRB in our experiments. The details about these\ndatasets are described below.\nMNIST. The MNIST (LeCun, 1998) is a large dataset of handwritten digits. Each digit has 5,500\ntraining images and 1,000 test images. Each image is a 28 \u00d7 28 grayscale. We normalize the range of\npixel values to [0, 1].\nCIFAR-10. The CIFAR-10 (Krizhevsky et al., 2009) is a dataset of 32x32 color images with ten\nclasses, each consisting of 5,000 training images and 1,000 test images. The classes correspond to\ndogs, frogs, ships, trucks, etc. We normalize the range of pixel values to [0, 1].\nGTSRB. The German Traf\ufb01c Sign Recognition Benchmark (GTSRB) (Stallkamp et al., 2012) is a\ndataset of color images depicting 43 different traf\ufb01c signs. The images are not of a \ufb01xed dimensions\nand have rich background and varying light conditions as would be expected of photographed images\nof traf\ufb01c signs. There are about 34,799 training images, 4,410 validation images and 12,630 test\nimages. We resize each image to 32 \u00d7 32. The dataset has a large imbalance in the number of\nsample occurrences across classes. We use data augmentation techniques to enlarge the training\ndata and make the number of samples in each class balanced. We construct a class preserving data\naugmentation pipeline consisting of rotation, translation, and projection transforms and apply this\npipeline to images in the training set until each class contained 10,000 examples. We also preprocess\nimages via image brightness normalization and normalize the range of pixel values to [0, 1].\nA.1.3\nIMPLEMENTATION DETAILS OF THE ATTACKS\nWe use Projected Gradient Descent (PGD) (Madry et al., 2018) to solve the attack objectives of PGD\nattack, FPA, GMSA-AVG and GMSA-MIN. For GMSA-AVG, at the i-th iteration, when applying\nPGD on the data point xxx to generate the perturbation \u03b4, we need to do one backpropagation operation\nfor each model in {F (j)}i\nj=0 per PGD step. We do the backpropagation for each model sequentially\nand then accumulate the gradients to update the perturbation \u03b4 since we might not have enough\nmemory to store all the models and compute the gradients at once, especially when i is large. For\nGMSA-MIN, we \ufb01nd that it requires more PGD steps to solve the attack objective at the i-th iteration\nwhere we need to attack i + 1 models simultaneously. Thus, we scale the number of PGD steps at the\ni-th iteration by a factor of i + 1 for GMSA-MIN.\nA.2\nSETUP FOR RMC EXPERIMENTS\nWe follow the original settings in Wu et al. (2020b) to perform experiments on MNIST and CIFAR-10\ndatasets to evaluate the adversarial robustness of RMC. We consider two kinds of base models for\nRMC: one is the model trained via standard supervised training; the other is the model trained using\nthe adversarial training (Madry et al., 2018). We describe the settings for each dataset below.\nA.2.1\nMNIST\nModel architecture and training con\ufb01guration. We use a neural network with two convolutional\nlayers, two full connected layers and batch normalization layers. For both standard training and\n13\n"}
{"basic_dict": {"forum": "ryzm6BATZ", "title": "Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks", "url": "https://openreview.net/forum?id=ryzm6BATZ", "pub_date": "--", "abstract": "We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature. Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions. We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function. We then provide a set of systematic experiments that explore its hyper-parameters. We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned. We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.", "TL;DR": "Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks", "authors": "Michael O. Vertolli,Jim Davies", "keywords": "generative adversarial networks,gans,deep learning,image modeling,image generation,energy based models", "venue": "--", "venue_id": "--", "number": 93, "pdf_url": "https://openreview.net/pdf?id=ryzm6BATZ", "signatures": ["ICLR.cc/2018/Conference"], "bibtex": "@misc{\no.2018image,\ntitle={Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks},\nauthor={Michael O. Vertolli and Jim Davies},\nyear={2018},\nurl={https://openreview.net/forum?id=ryzm6BATZ},\n}", "from_venue_id": "ICLR.cc/2018/Conference"}, "reviews_msg": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1515642532004, "tcdate": 1512090412089, "number": 3, "cdate": 1512090412089, "id": "H1NEs7Clz", "invitation": "ICLR.cc/2018/Conference/-/Paper93/Official_Review", "forum": "ryzm6BATZ", "replyto": "ryzm6BATZ", "signatures": ["ICLR.cc/2018/Conference/Paper93/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "An incremental paper with moderately interesting results on a single dataset", "rating": "6: Marginally above acceptance threshold", "review": "Summary: \nThe paper extends the the recently proposed Boundary Equilibrium Generative Adversarial Networks (BEGANs), with the hope of generating images which are more realistic. In particular, the authors propose to change the energy function associated with the auto-encoder, from an L2 norm (a single number) to an energy function with multiple components. Their energy function is inspired by the structured similarity index (SSIM), and the three components they use are the L1 score, the gradient magnitude similarity score, and the chromium score. Using this energy function, the authors hypothesize, that it will force the generator to generate realistic images. They test their hypothesis on a single dataset, namely, the CelebA dataset. \n\nReview: \nWhile the idea proposed in the paper is somewhat novel and there is nothing obviously wrong about the proposed approach, I thought the paper is somewhat incremental. As a result I kind of question the impact of this result. My suspicion is reinforced by the fact that the experimental section is extremely weak. In particular the authors test their model on a single relatively straightforward dataset. Any reason why the authors did not try on other datasets involving natural images? As a result I feel that the title and the claims in the paper are somewhat misleading and premature: that the proposed techniques improves the training and evaluation of energy based gans. \n\nOver all the paper is clearly written and easy to understand. \n\nBased on its incremental nature and weak experiments, I'm on the margin with regards to its acceptance. Happy to change my opinion if other reviewers strongly think otherwise with good reason and are convinced about its impact. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260083557, "tcdate": 1517249946746, "number": 638, "cdate": 1517249946730, "id": "rk7jH16BM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "ryzm6BATZ", "replyto": "ryzm6BATZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper received borderline-negative scores (6,5,5) with R1 and R2 having significant difficulty with the clarity of the paper. Although R3 was marginally positive, they pointed out that the experiments are \"extremely weak\". The AC look at the paper and agrees with R3 on this point. Therefore the paper cannot be accepted in its current form. The experiments and clarity need work before resubmission to another venue.  "}, "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642532053, "tcdate": 1511807049322, "number": 2, "cdate": 1511807049322, "id": "HJZIu0Kef", "invitation": "ICLR.cc/2018/Conference/-/Paper93/Official_Review", "forum": "ryzm6BATZ", "replyto": "ryzm6BATZ", "signatures": ["ICLR.cc/2018/Conference/Paper93/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Novelty of the paper is a bit restricted, and design choices appear to be lacking strong justifications.", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposed some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score, which are motivated and borrowed from the image quality assessment techniques. These energy component in the objective function allows learning of different set of features and determination on whether the features are adequately represented. experiments on the using different hyper-parameters of the energy function, as well as visual inspections on the quality of the learned images, are presented. \n\nIt appears to me that the novelty of the paper is limited, in that the main approach is built on the existing BEGAN framework with certain modifications. For example, the new energy function in equation (4) larges achieves similar goal as the original energy (1) proposed by Zhao et. al (2016), except that the margin loss in (1) is changed to a re-weighted linear loss, where the dynamic weighting scheme of k_t is borrowed  from the work of Berthelot et. al (2017). It is not very clear why making such changes in the energy would supposedly make the results better, and no further discussions are provided.  On the other hand, the several energy component introduced are simply choices of the similarity measures as motivated from the image quality assessment, and there are probably a lot more in the literature whose application can not be deemed as a significant contribution to either theories or algorithm designs in GAN.\n\nMany results from the experimental section rely on visual evaluations, such as in Figure~4 or 5; from these figures, it is difficult to clearly pick out the winning images. In Figure~5, for a fair  evaluation on the performance of model interploations, the same human model should be used for competing methods, instead of applying different human models and different interpolation tasks in different methods. \n ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642532091, "tcdate": 1511438445887, "number": 1, "cdate": 1511438445887, "id": "Bk8udEEeM", "invitation": "ICLR.cc/2018/Conference/-/Paper93/Official_Review", "forum": "ryzm6BATZ", "replyto": "ryzm6BATZ", "signatures": ["ICLR.cc/2018/Conference/Paper93/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "A very technical paper with unclear significance.", "rating": "5: Marginally below acceptance threshold", "review": "Quick summary:\nThis paper proposes an energy based formulation to the BEGAN model and modifies it to include an image quality assessment based term. The model is then trained with CelebA under different parameters settings and results are analyzed.\n\nQuality and significance:\nThis is quite a technical paper, written in a very compressed form and is a bit hard to follow. Mostly it is hard to estimate what is the contribution of the model and how the results differ from baseline models.\n\nClarity:\nI would say this is one of the weak points of the paper - the paper is not well motivated and the results are not clearly presented. \n\nOriginality:\nSeems original.\n\nPros:\n* Interesting energy formulation and variation over BEGAN\n\nCons:\n* Not a clear paper\n* results are only partially motivated and analyzed", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": []}]}
{"basic_dict": {"forum": "ryykVe-0W", "title": "Learning Independent Features with Adversarial Nets for Non-linear ICA", "url": "https://openreview.net/forum?id=ryykVe-0W", "pub_date": "--", "abstract": "Reliable measures of statistical dependence could potentially be useful tools for learning independent features and performing tasks like source separation using Independent Component Analysis (ICA).  Unfortunately, many of such measures, like the mutual information, are hard to estimate and optimize directly.  We propose to learn independent features with adversarial objectives (Goodfellow et al. 2014, Arjovsky et al. 2017) which optimize such measures implicitly.  These objectives compare samples from the joint distribution and the product of the marginals without the need to compute any probability densities. We also propose two methods for obtaining samples from the product of the marginals using either a simple resampling trick or a separate parametric distribution.  Our experiments show that this strategy can easily be applied to different types of model architectures and solve both linear and non-linear ICA problems.\n", "TL;DR": "--", "authors": "Philemon Brakel,Yoshua Bengio", "keywords": "adversarial networks,ica,unsupervised,independence", "venue": "--", "venue_id": "--", "number": 573, "pdf_url": "https://openreview.net/pdf?id=ryykVe-0W", "signatures": ["ICLR.cc/2018/Conference"], "bibtex": "@misc{\nbrakel2018learning,\ntitle={Learning Independent Features with Adversarial Nets for Non-linear {ICA}},\nauthor={Philemon Brakel and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=ryykVe-0W},\n}", "from_venue_id": "ICLR.cc/2018/Conference"}, "reviews_msg": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1515642471890, "tcdate": 1511651123102, "number": 1, "cdate": 1511651123102, "id": "HyoEDdvxG", "invitation": "ICLR.cc/2018/Conference/-/Paper573/Official_Review", "forum": "ryykVe-0W", "replyto": "ryykVe-0W", "signatures": ["ICLR.cc/2018/Conference/Paper573/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Proposed Wasserstein GAN: not well-suited to ICA", "rating": "3: Clear rejection", "review": "The focus of the paper is independent component analysis (ICA) and its nonlinear variants such as the post non-linear (PNL) ICA model. Motivated by the fact that estimating mutual information and similar dependency measures require density estimates and hard to optimize, the authors propose a Wasserstein GAN (generative adversarial network) based solution to tackle the problem, with illustrations on 6 (synthetic) and 3-dimemensional (audio) examples. The primary idea of the paper is to use the Wasserstein distance as an independence measure of the estimated source coordinates, and optimize it in a neural network (NN) framework.\n\nAlthough finding novel GAN applications is an exciting topic, I am not really convinced that ICA with the proposed Wasserstein GAN based technique fulfills this goal.\n \nBelow I detail my reasons:\n\n1)The ICA problem can be formulated as the minimization of pairwise mutual information [1] or one-dimensional entropy [2]. In other words, estimating the joint dependence of the source coordinates is not necessary; it is worthwhile to avoid it.\n\n2)The PNL ICA task can be efficiently tackled by first 'removing' the nonlinearity followed by classical linear ICA; see for example [3].\n\n3)Estimating information theoretic (IT) measures (mutual information, divergence) is a quite mature field with off-the-self techniques, see for example [4,5,6,8]. These methods do not estimate the underlying densities; it would be superfluous (and hard).\n\n4)Optimizing non-differentiable IT measures can computationally quite efficiently carried out in the ICA context by e.g., Givens rotations [7]; differentiable ICA cost functions can be robustly handled by Stiefel manifold methods; see for example [8,9].\n\n5)Section 3.1: This section is devoted to generating samples from the product of the marginals, even using separate generator networks. I do not see the necessity of these solutions; the subtask can be solved by independently shuffling all the coordinates of the sample.\n\n6)Experiments (Section 6): \ni) It seems to me that the proposed NN-based technique has some quite serious divergence issues: 'After discarding diverged models, ...' or 'Unfortunately, the model selection procedure also didn't identify good settings for the Anica-g model...'.\nii) The proposed method gives pretty comparable results to the chosen baselines (fastICA, PNLMISEP) on the selected small-dimensional tasks. In fact, [7,8,9] are likely to provide more accurate (fastICA is a simple kurtosis based method, which is \na somewhat crude 'estimate' of entropy) and faster estimates; see also 2).\n\nReferences:\n[1] Pierre Comon. Independent component analysis, a new concept? Signal Processing, 36:287-314, 1994.\n[2] Aapo Hyvarinen and Erkki Oja. Independent Component Analysis: Algorithms and Applications. Neural Networks, 13(4-5):411-30, 2000. \n[3] Andreas Ziehe, Motoaki Kawanabe, Stefan Harmeling, and Klaus-Robert Muller. Blind separation of postnonlinear mixtures using linearizing transformations and temporal decorrelation. Journal of Machine Learning Research, 4:1319-1338, 2003.\n[4] Barnabas Poczos, Liang Xiong, and Jeff Schneider. Nonparametric divergence: Estimation with applications to machine learning on distributions. In Conference on Uncertainty in Artificial Intelligence, pages 599-608, 2011.\n[5] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, Alexander Smola. A Kernel Two-Sample Test. Journal of Machine Learning Research, 13:723-773, 2012.\n[6] Alan Wisler, Visar Berisha, Andreas Spanias, Alfred O. Hero. A data-driven basis for direct estimation of functionals of distributions. TR, 2017. (https://arxiv.org/abs/1702.06516) \n[7] Erik G. Learned-Miller, John W. Fisher III. ICA using spacings estimates of entropy. Journal of Machine Learning Research, 4:1271-1295, 2003.\n[8] Francis R. Bach. Michael I. Jordan. Kernel Independent Component Analysis. Journal of Machine Learning Research 3: 1-48, 2002.\n[9] Hao Shen, Stefanie Jegelka and Arthur Gretton. Fast Kernel-Based Independent Component Analysis, IEEE Transactions on Signal Processing, 57:3498-3511, 2009.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260082563, "tcdate": 1517249990963, "number": 676, "cdate": 1517249990947, "id": "SJy0B1pBz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "ryykVe-0W", "replyto": "ryykVe-0W", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper proposes the use of GANs to match the joint distribution of features to the product of their marginals for ICA. The approach is totally plausible but reviewers have complaints about lack of rigor and analysis in terms of (i) mixing conditions under which the proposed GAN based approach will work, given that ICA is ill-posed for general nonlinear mixing  (ii) comparison with prior work on linear and PNL ICA.\n\nFurther, in most scenarios where GANs are used, one of the distributions is fixed (say, the real distribution) and the other is dynamic (fake distribution) trying to come close to the fixed distribution during optimization. In the proposed method, the discriminator encodes the distance b/w joint and product of marginals which are both dynamic during the learning. It might be useful to comment whether or not it has any implications wrt increased instability of training, etc. "}, "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515761722048, "tcdate": 1511731443944, "number": 2, "cdate": 1511731443944, "id": "H1hlWndxM", "invitation": "ICLR.cc/2018/Conference/-/Paper573/Official_Review", "forum": "ryykVe-0W", "replyto": "ryykVe-0W", "signatures": ["ICLR.cc/2018/Conference/Paper573/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting nonlinear ICA method, but unfocused presentation and poor comparisons", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a GAN variant for solving the nonlinear independent component analysis (ICA) problem. The method seems interesting, but the presentation has a severe lack of focus.\n\nFirst, the authors should focus their discussion instead of trying to address a broad range of ICA problems from linear to post-nonlinear (PNL) to nonlinear. I would highly recommend the authors to study the review \"Advances in Nonlinear Blind Source Separation\" by Jutten and Karhunen (2003/2004) to understand the problems they are trying to solve.\n\nLinear ICA is a solved problem and the authors do not seem to be able to add anything there, so I would recommend dropping that to save space for the more interesting material.\n\nPNL ICA is solvable and there are a number of algorithms proposed for it, some cited already in the above review, but also more recent ones. From this perspective, the presented comparison seems quite inadequate.\n\nFully general nonlinear ICA is ill-posed, as shown already by Darmois (1953, doi:10.2307/1401511). Given this, the authors should indicate more clearly what is their method expected to do. There are an infinite number of nonlinear ICA solutions - which one is the proposed method going to return and why is that relevant? There are fewer relevant comparisons here, but at least Lappalainen and Honkela (2000) seem to target the same problem as the proposed method.\n\nThe use of 6 dimensional example in the experiments is a very good start, as higher dimensions are quite different and much more interesting than very commonly used 2D examples.\n\nOne idea for evaluation: comparison with ground truth makes sense for PNL, but not so much for general nonlinear because of unidentifiability. For general nonlinear ICA you could consider evaluating the quality of the estimated low-dimensional data manifold or evaluating the mutual information of separated sources on new test data.\n\nUpdate after author feedback: thanks for the response and the revision. The revision seems more cosmetic and does not address the most significant issues so I do not see a need to change my evaluation.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642471817, "tcdate": 1511738612355, "number": 3, "cdate": 1511738612355, "id": "ry2lpp_ez", "invitation": "ICLR.cc/2018/Conference/-/Paper573/Official_Review", "forum": "ryykVe-0W", "replyto": "ryykVe-0W", "signatures": ["ICLR.cc/2018/Conference/Paper573/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Thought provoking paper but lacks more detailed analysis", "rating": "6: Marginally above acceptance threshold", "review": "\nThe idea of ICA is constructing a mapping from dependent inputs to outputs (=the derived features) such that the outputs are as independent as possible. As the input/output densities are often not known and/or are intractable, natural independence measures such as mutual information are hard to estimate. In practice, the independence is characterized by certain functions of higher order moments -- leading to several alternatives in a zoo of independence objectives.  \n\nThe current paper makes the iteresting observation that independent features can also be computed via adversarial objectives. The key idea of adversarial training is adapted in this context as comparing samples from the joint distribution and the product of the marginals. \n\nTwo methods are proposed for drawing samples from the products of marginals. \nOne method is generating samples but permuting randomly the sample indices for individual marginals - this resampling mechanism generates approximately independent samples from the product distribution. The second method is essentially samples each marginal separately. \n\nThe approach is demonstrated in the solution of both linear and non-linear ICA problems.\n\nPositive:\nThe paper is well written and easy to follow on a higher level. GAN's provide a fresh look at nonlinear ICA and the paper is certainly thought provoking. \n\n\nNegative:\nMost of the space is devoted for reviewing related work and motivations, while the specifics of the method are described relatively short in section 4. There is no analysis and the paper is \nsomewhat anecdotal. The simulation results section is limited in scope. The sampling from product distribution method is somewhat obvious.\n\n\nQuestions:\n\n- The overcomplete audio source separation case is well known for audio and I could not understand why a convincing baseline can not be found. Is this due to nonlinear mixing?\nAs 26 channels and 6 channels are given, a simple regularization based method can be easily developed to provide a baseline performance, \n\n\n- The need for normalization in section 4 is surprising, as it obviously renders the outputs dependent. \n\n- Figure 1 may be misleading as h are not defined \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": []}]}
{"basic_dict": {"forum": "rywHCPkAW", "title": "Noisy Networks For Exploration", "url": "https://openreview.net/forum?id=rywHCPkAW", "pub_date": "--", "abstract": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights.  NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.", "TL;DR": "A deep reinforcement learning agent with parametric noise added to its weights can be used to aid efficient exploration.", "authors": "Meire Fortunato,Mohammad Gheshlaghi Azar,Bilal Piot,Jacob Menick,Matteo Hessel,Ian Osband,Alex Graves,Volodymyr Mnih,Remi Munos,Demis Hassabis,Olivier Pietquin,Charles Blundell,Shane Legg", "keywords": "Deep Reinforcement Learning,Exploration,Neural Networks", "venue": "--", "venue_id": "--", "number": 134, "pdf_url": "https://openreview.net/pdf?id=rywHCPkAW", "signatures": ["ICLR.cc/2018/Conference"], "bibtex": "@inproceedings{\nfortunato2018noisy,\ntitle={Noisy Networks For Exploration},\nauthor={Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Matteo Hessel and Ian Osband and Alex Graves and Volodymyr Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rywHCPkAW},\n}", "from_venue_id": "ICLR.cc/2018/Conference"}, "reviews_msg": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1515642397208, "tcdate": 1511801835995, "number": 3, "cdate": 1511801835995, "id": "H14gEaFxG", "invitation": "ICLR.cc/2018/Conference/-/Paper134/Official_Review", "forum": "rywHCPkAW", "replyto": "rywHCPkAW", "signatures": ["ICLR.cc/2018/Conference/Paper134/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Good paper but lack of empirical comparison & analysis", "rating": "6: Marginally above acceptance threshold", "review": "A new exploration method for deep RL is presented, based on the idea of injecting noise into the deep networks’ weights. The noise may take various forms (either uncorrelated or factored) and its magnitude is trained by gradient descent along other parameters. It is shown how to implement this idea both in DQN (and its dueling variant) and A3C, with experiments on Atari games showing a significant improvement on average compared to these baseline algorithms.\n\nThis definitely looks like a worthy direction of research, and experiments are convincing enough to show that the proposed algorithms indeed improve on their baseline version. The specific proposed algorithm is close in spirit to the one from “Parameter space noise for exploration”, but there are significant differences. It is also interesting to see (Section 4.1) that the noise evolves in non-obvious ways across different games.\n\nI have two main concerns about this submission. The first one is the absence of a comparison to the method from “Parameter space noise for exploration”, which shares similar key ideas (and was published in early June, so there was enough time to add this comparison by the ICLR deadline). A comparison to the paper(s) by Osband et al (2016, 2017) would have also been worth adding. My second concern is that I find the title and overall discussion in the paper potentially misleading, by focusing only on the “exploration” part of the proposed algorithm(s). Although the noise injected in the parameters is indeed responsible for the exploration behavior of the agent, it may also have an important effect on the optimization process: in both DQN and A3C it modifies the cost function being optimized, both through the “target” values (respectively Q_hat and advantage) and the parameters of the policy (respectively Q and pi). Since there is no attempt to disentangle these exploration and optimization effects, it is unclear if one is more important than the other to explain the success of the approach. It also sheds doubt on the interpretation that the agent somehow learns some kind of optimal exploration behavior through gradient descent (something I believe is far from obvious).\n\nEstimating the impact of a paper on future research is an important factor in evaluating it. Here, I find myself in the akward (and unusual to me) situation where I know the proposed approach has been shown to bring a meaningful improvement, more precisely in Rainbow (“Rainbow: Combining Improvements in Deep Reinforcement Learning”). I am unsure whether I should take it into account in this review, but in doubt I am choosing to, which is why I am advocating for acceptance in spite of the above-mentioned concerns.\n\nA few small remarks / questions / typos:\n- In eq. 3 A(...) is missing the action a as input\n- Just below: “the the”\n- Last sentence of p. 3 can be misleading because the gradient is not back-propagated through all paths in the defined cost\n- “In our experiments we used f(x) = sgn(x) p |x|”: this makes sense to me for eq. 9 but why not use f(x) = x in eq. 10?\n- Why use factored noise in DQN and independent noise in A3C? This is presented like an arbitrary choice here.\n- What is the justification for using epsilon’ instead of epsilon in eq. 15? My interpretation of double DQN is that we want to evaluate (with the target network) the action chosen by the Q network, which here is perturbed with epsilon (NB: eq. 15 should have b in the argmax, not b*)\n- Section 4 should say explicitly that results are over 200M frames\n- Assuming the noise is sampled similarly doing evaluation (= as in training), please mention it clearly.\n- In paragraph below eq. 18: “superior performance compare to their corresponding baselines”: compared\n- There is a Section 4.1 but no 4.2\n- Appendix has a lot of redundant material with the main text, for instance it seems to me that A.1 is useless.\n- In appendix B: “σi,j is simply set to 0.017 for all parameters” => where does this magic value come from?\n- List x seems useless in C.1 and C.2\n- C.1 and C.2 should be combined in a single algorithm with a simple “if dueling” on l. 24\n- In C.3: (1) missing pi subscript for zeta in the “Output:” line, (2) it is not clear what the zeta’ parameters are for, in particular should they be used in l. 12 and 22?\n- The paper “Dropout as a Bayesian approximation” seems worth at least  adding to the list of related work in the introduction.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642397286, "tcdate": 1511448010505, "number": 1, "cdate": 1511448010505, "id": "Hyf0aUVeM", "invitation": "ICLR.cc/2018/Conference/-/Paper134/Official_Review", "forum": "rywHCPkAW", "replyto": "rywHCPkAW", "signatures": ["ICLR.cc/2018/Conference/Paper134/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The proposed approach is interesting and has strengths, but the paper has weaknesses. I am somewhat divided for acceptance.", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, a new heuristic is introduced with the purpose of controlling the exploration in deep reinforcement learning. \n\nThe proposed approach, NoisyNet, seems very simple and smart: a noise of zero mean and unknown variance is added to each weight of the deep network. The matrices of unknown variances are considered as parameters and are learned with a standard gradient descent. The strengths of the proposed approach are the following:\n1 NoisyNet is generic: it is applied to A3C, DQN and Dueling agents. \n2 NoisyNet reduces the number of hyperparameters. NoisyNet does not need hyperparameters (only the kind of the noise distribution has to be defined), and replacing the usual exploration heuristics by NoisyNet, a hyperparameter is suppressed (for instance \\epsilon in the case of epsilon-greedy exploration).\n3 NoisyNet exhibits impressive experimental results in comparison to the usual exploration heuristics for to A3C, DQN and Dueling agents.\n\nThe weakness of the proposed approach is the lack of explanation and investigation (experimental or theoretical) of why does Noisy work so well. At the end of the paper a single experiment investigates the behavior of weights of noise during the learning.  Unfortunately this experiment seems to be done in a hurry. Indeed, the confidence intervals are not plotted, and probably no conclusion can be reached because the curves are averaged only across three seeds! It’s disappointing.  As expected for an exploration heuristic, it seems that the noise weights of the last layer (slowly) tend to zero. However for some games, the weights of the penultimate layer seem to increase. Is it due to NoisyNet or to the lack of seeds? \n\nIn the same vein, in section 3, two kinds of noise are proposed: independent or factorized Gaussian noise. The factorized Gaussian noise, which reduces the number of parameters, is associated with DQN and Dueling agents, while the independent noise is associated with A3C agent. Why? \n\nOverall the proposed approach is interesting and has strengths, but the paper has weaknesses. I am somewhat divided for acceptance. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642397246, "tcdate": 1511539460976, "number": 2, "cdate": 1511539460976, "id": "rJ6Z7prxf", "invitation": "ICLR.cc/2018/Conference/-/Paper134/Official_Review", "forum": "rywHCPkAW", "replyto": "rywHCPkAW", "signatures": ["ICLR.cc/2018/Conference/Paper134/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "A good paper, despite a weak analysis", "rating": "7: Good paper, accept", "review": "This paper introdues NoisyNets, that are neural networks whose parameters are perturbed by a parametric noise function, and they apply them to 3 state-of-the-art deep reinforcement learning algorithms: DQN, Dueling networks and A3C. They obtain a substantial performance improvement over the baseline algorithms, without explaining clearly why.\n\nThe general concept is nice, the paper is well written and the experiments are convincing, so to me this paper should be accepted, despite a weak analysis.\n\nBelow are my comments for the authors.\n\n---------------------------------\nGeneral, conceptual comments:\n\nThe second paragraph of the intro is rather nice, but it might be updated with recent work about exploration in RL.\nNote that more than 30 papers are submitted to ICLR 2018 mentionning this topic, and many things have happened since this paper was\nposted on arxiv (see the \"official comments\" too).\n\np2: \"our NoisyNet approach requires only one extra parameter per weight\" Parameters in a NN are mostly weights and biases, so from this sentence\none may understand that you close-to-double the number of parameters, which is not so few! If this is not what you mean, you should reformulate...\n\np2: \"Though these methods often rely on a non-trainable noise of vanishing size as opposed to NoisyNet which tunes the parameter of noise by gradient descent.\"\nTwo ideas seem to be collapsed here: the idea of diminishing noise over an experiment, exploring first and exploiting later, and the idea of\nadapting the amount of noise to a specific problem. It should be made clearer whether NoisyNet can address both issues and whether other\nalgorithms do so too...\n\nIn particular, an algorithm may adapt noise along an experiment or from an experiment to the next.\nFrom Fig.3, one can see that having the same initial noise in all environments is not a good idea, so the second mechanism may help much.\n\nBTW, the short section in Appendix B about initialization of noisy networks should be moved into the main text.\n\np4: the presentation of NoisyNets is not so easy to follow and could be clarified in several respects:\n- a picture could be given to better explain the structure of parameters, particularly in the case of factorised (factorized, factored?) Gaussian noise.\n- I would start with the paragraph \"Considering a linear layer [...] below)\" and only after this I would introduce \\theta and \\xi as a more synthetic notation.\nLater in the paper, you then have to state \"...are now noted \\xi\" several times, which I found rather clumsy.\n\np5: Why do you use option (b) for DQN and Dueling and option (a) for A3C? The reason why (if any) should be made clear from the clearer presentation required above.\n\nBy the way, a wild question: if you wanted to use NoisyNets in an actor-critic architecture like DDPG, would you put noise both in the actor and the critic?\n\nThe paragraph above Fig3 raises important questions which do not get a satisfactory answer.\nWhy is it that, in deterministic environments, the network does not converge to a deterministic policy, which should be able to perform better?\nWhy is it that the adequate level of noise changes depending on the environment? By the way, are we sure that the curves of Fig3 correspond to some progress\nin noise tuning (that is, is the level of noise really \"better\" through time with these curves, or they they show something poorly correlated with the true reasons of success?)?\n\nFinally, I would be glad to see the effect of your technique on algorithms like TRPO and PPO which require a stochastic policy for exploration, and where I believe that the role of the KL divergence bound is mostly to prevent the level of stochasticity from collasping too quickly.\n\n-----------------------------------\nLocal comments:\n\nThe first sentence may make the reader think you only know about 4-5 old works about exploration.\n\nPp. 1-2 : \"the approach differs ... from variational inference. [...] It also differs variational inference...\"\nIf you mean it differs from variational inference in two ways, the paragraph should be reorganized.\n\np2: \"At a high level our algorithm induces a randomised network for exploration, with care exploration\nvia randomised value functions can be provably-efficient with suitable linear basis (Osband et al., 2014)\"\n=> I don't understand this sentence at all.\n\nAt the top of p3, you may update your list with PPO and ACKTR, which are now \"classical\" baselines too.\n\nAppendices A1 and A2 are a lot redundant with the main text (some sentences and equations are just copy-pasted), this should be improved.\nThe best would be to need to reject nothing to the Appendix.\n\n---------------------------------------\nTypos, language issues:\n\np2\nthe idea ... the optimization process have been => has\n\np2\nThough these methods often rely on a non-trainable noise of vanishing size as opposed to NoisyNet which tunes the parameter of noise by gradient descent.\n=> you should make a sentence...\n\np3\nthe the double-DQN\n\nseveral times, an equation is cut over two lines, a line finishing with \"=\", which is inelegant\n\nYou should deal better with appendices: Every \"Sec. Ax/By/Cz\" should be replaced by \"Appendix Ax/By/Cz\".\nBesides, the big table and the list of performances figures should themselves be put in two additional appendices\nand you should refer to them as Appendix D or E rather than \"the Appendix\".\n\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": []}, {"tddate": null, "ddate": null, "tmdate": 1509969109846, "tcdate": 1509969109846, "number": 1, "cdate": 1509969109846, "id": "BJRR3paAZ", "invitation": "ICLR.cc/2018/Conference/-/Paper134/Public_Comment", "forum": "rywHCPkAW", "replyto": "rywHCPkAW", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Comparison with \"Parameter space noise for exploration\" results", "comment": "Very interesting paper and results, thanks for the paper! I have a few questions:\n\nEarlier this year, before \"Noisy Networks For Exploration\", a paper with very similar approach, \"Parameter space noise for exploration\" has been published. It has already reported a number of improvements compare to the baseline, action space noise implementations of different variants of DQN as well as in the continuous domain. So it would be very nice to see in the paper comparison of the \"Noisy Networks For Exploration\" not only against the baseline but also against parameter space noise approach to understand if noisy networks can provide any benefits - better exploration, larger maximum reward achieved, or noisy networks showed comparable to the parameter space approach results but also with the cost of additional computational complexity.\n\nAlso it would be nice if similar to OpenAI you can release your \"noisy network\" implementation to help with independent reproduction and of the results described in paper."}, "nonreaders": []}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260097150, "tcdate": 1517249382700, "number": 165, "cdate": 1517249382683, "id": "B1k_QJarf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rywHCPkAW", "replyto": "rywHCPkAW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper proposes to add noise to the weights of a policy network during learning in Deep-RL settings and finds that this results in better performance on DQN, A3C and other algorithms that use other exploration strategies. Unfortunately, the paper does not do a thorough job of exploring the reasons and doesn't offer a comparison to other methods that have been out on arxiv for several months before the submission, in spite of reviewers and anonymous requests. Otherwise I might have supported recommending the paper for a talk. ", "decision": "Accept (Poster)"}, "nonreaders": []}, {"tddate": null, "ddate": null, "tmdate": 1514934290431, "tcdate": 1513804135896, "number": 3, "cdate": 1513804135896, "id": "B1e_W8uMM", "invitation": "ICLR.cc/2018/Conference/-/Paper134/Official_Comment", "forum": "rywHCPkAW", "replyto": "rywHCPkAW", "signatures": ["ICLR.cc/2018/Conference/Paper134/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper134/Authors"], "content": {"title": "General response to the reviewers", "comment": "We like to thank the anonymous reviewers for their helpful and constructive comments. We provide individual response to each reviewer's comments. Here we report the list of main changes which we have added to the new revision.\n\n1-A discussion on the optimisation aspects of NoisyNet (Section 5, Paragraph 1). \n2- Further clarifications on why factorised  noise is used in some agents as opposed to independent noise in the case of A3C (Section 3, Paragraph 3).\n3- Reporting the learning curves and the scores for NoisyNet-A3C with factorised noise, showing that a similar performance to the case of independent noise can be achieved with significantly less noisy variables (Appendix D).\n4-Adding error bars to the learning curves of Fig. 3  and error bounds to the scores  of Table 3.\n5-Adding a graphical representation of noisy linear layer (Appendix B). \n6- Correcting  the inconsistencies between  the  description of the algorithm in the original submission and our implementation (Appendix C Algo. 1 line 13, 14 and 16  and Eq. 16)\n"}, "nonreaders": []}]}

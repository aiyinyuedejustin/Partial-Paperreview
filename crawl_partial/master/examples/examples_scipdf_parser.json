{"title": "TOWARDS EVALUATING THE ROBUSTNESS OF NEURAL NETWORKS LEARNED BY TRANSDUCTION", "authors": "Jiefeng Chen; Xi Wu; Yang Guo; Yingyu Liang; Somesh Jha;  -Madison;  Google;  Xaipient", "pub_date": "", "abstract": "There has been an emerging interest in using transductive learning for adversarial robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020; Wang et  al., ArXiv 2021). Compared to traditional defenses, these defense mechanisms \"dynamically learn\" the model based on test-time input; and theoretically, attacking these defenses reduces to solving a bilevel optimization problem, which poses difficulty in crafting adaptive attacks. In this paper, we examine these defense mechanisms from a principled threat analysis perspective. We formulate and analyze threat models for transductive-learning based defenses, and point out important subtleties. We propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating transductivelearning based defenses. Through systematic evaluation, we show that GMSA, even with weak instantiations, can break previous transductive-learning based defenses, which were resilient to previous attacks, such as AutoAttack. On the positive side, we report a somewhat surprising empirical result of \"transductive adversarial training\": Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks we consider.", "sections": [{"heading": "INTRODUCTION", "text": "Adversarial robustness of deep learning models has received significant attention in recent years (see Kolter & Madry (2018) and references therein). The classic threat model of adversarial robustness considers an inductive setting where a model is learned at the training time and fixed, and then at the test time, an attacker attempts to thwart the fixed model with adversarially perturbed input. This gives rise to the adversarial training Sinha et al., 2018;Carmon et al., 2019) to enhance adversarial robustness.\nGoing beyond the inductive threat model, there has been an emerging interest in using transductive learning (Vapnik, 1998) 1 for adversarial robustness (Goldwasser et al., 2020;Wu et al., 2020b;Wang et al., 2021). In essence, these defenses attempt to leverage a batch of test-time inputs, which is common for ML pipelines deployed with batch predictions (bat, 2021), to learn an updated model. The hope is that this \"test-time learning\" may be useful for adversarial robustness since the defender can adapt the model to the perturbed input from the adversary, which is distinct from the inductive threat model where a model is fixed after training. This paper examines these defenses from a principled threat analysis perspective. We first formulate and analyze rigorous threat models. Our basic 1-round threat model considers a single-round game between the attacker and the defender. Roughly speaking, the attacker uses an objective max V \u2208N (V ) L a (\u0393(U ), V ) (formula (2)), where V is the given test batch, N (V ) is a neighborhood around V , L a is a loss function for attack gain, \u0393 is the transductive-learning based defense, and U = V | X , the projection of V to features, is the adversarially perturbed data for breaking \u0393. This Our code is available at: https://github.com/jfc43/eval-transductive-robustness. 1 We note that this type of defense goes under different names such as \"test-time adaptation\" or \"dynamic defenses\". Nevertheless, they all fall into the classic transductive learning paradigm (Vapnik, 1998), which attempts to leverage test data for learning. We thus call them transductive-learning based defenses. The word \"transductive\" is also adopted in Goldwasser et al. (2020).\nobjective is transductive as U , the attacker's output, appears in both attack (V in L a ) and defense (U in \u0393). We extend this threat model to multiple rounds, which is necessary when considering DENT (Wang et al., 2021) and RMC (Wu et al., 2020b). We point out important subtleties in the modeling that were unclear or overlooked in previous work.\nWe then study adaptive attacks, that is to leverage the knowledge about \u0393 to construct attacks. Compared to situations considered in BPDA (Athalye et al., 2018), a transductive learner \u0393 is even further from being differentiable, and theoretically the attack objective is a bilevel optimization (Colson et al., 2007). To address these difficulties, our key observation is to consider the transferability of adversarial examples, and consider a robust version of (2): max U min U \u2208N (U ) L a (\u0393(U ), V ) (formula ( 6)), where we want to find a single attack set U to thwart a family of models, induced by U \"around\" U . This objective relaxes the attacker-defender constraint, and provides more information in dealing with nondifferentiability. To solve the robust objective, we propose Greedy Model Space Attack (GMSA), a general attack framework which attempts to solve the robust objective in a greedy manner. GMSA can serve as a new baseline for evaluating transductive-learning based defenses.\nWe perform a systematic empirical study on various defenses. For RMC (Wu et al., 2020b), DENT (Wang et al., 2021), and URejectron (Goldwasser et al., 2020), we show that even weak instantiations of GMSA can break respective defenses. Specifically, for defenses based on adversarially training, we reduce the robust accuracy to that of adversarial training alone. We note that, under AutoAttack (Croce & Hein, 2020a), the state-of-the-art adaptive attack for the inductive threat model, some of these defenses have claimed to achieve substantial improvements compared to adversarial training alone. For example, Wang et al. show that DENT can improve the robustness of the state-of-the-art adversarial training defenses by more than 20% absolutely against AutoAttack on CIFAR-10. However, under our adaptive attacks, DENT only has minor improvement: less than 3% improvement over adversarial training alone. Our results thus demonstrates significant differences between attacking transductive-learning based defenses and attacking in the inductive setting, and significant difficulties in the use of transductive learning to improve adversarial robustness. On the positive side, we report a somewhat surprising empirical result of transductive adversarial training: Adversarially retraining the model using fresh private randomness on a new batch of test-time data gives a significant increase in robustness against all of our considered attacks.", "n_publication_ref": 21, "n_figure_ref": 0}, {"heading": "RELATED WORK", "text": "Adversarial robustness in the inductive setting. Many attacks have been proposed to evaluate the adversarial robustness of the defenses in the inductive setting where the model is fixed during the evaluation phase (Goodfellow et al., 2015;Carlini & Wagner, 2017;Kurakin et al., 2017;Moosavi-Dezfooli et al., 2016;Croce & Hein, 2020b). Principles for adaptive attacks have been developed in Tram\u00e8r et al. (2020) and many existing defenses are shown to be broken based on attacks developed from these principles (Athalye et al., 2018). A fundamental method to obtain adversarial robustness in this setting is adversarial training Zhang et al., 2019). A state-of-the-art attack in the inductive threat model is AutoAttack (Croce & Hein, 2020a). Adversarial robustness via test-time defenses. There have been various work which attempt to improve adversarial robustness by leveraging test-time data. Many of such work attempt to \"sanitize\" test-time input using a non-differentiable function, and then send it to a pretrained model. Most of these proposals were broken by BPDA (Athalye et al., 2018). To this end, we note that a research agenda for \"dynamic model defense\" has been proposed in Goodfellow (2019).\nAdversarial robustness using transductive learning. There has been emerging interesting in using transductive learning to improve adversarial robustness. In view of \"dynamic defenses\", these proposals attempt to apply transductive learning to the test data and update the model, and then use the updated model to predict on the test data. In this work we consider three such work (Wu et al., 2020b;Wang et al., 2021;Goldwasser et al., 2020).", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "PRELIMINARIES", "text": "Let F be a model, and for a data point (x x x, y) \u2208 X \u00d7 Y, a loss function (F ; x x x, y) gives the loss of F on the point. Let V be a set of labeled data points, and let L(F, V ) = 1 |V | (x x x,y)\u2208V (F ; x x x, y) denote the empirical loss of F on V . For example, if we use binary loss 0,1 (F ; x x x, y) = 1[F (x x x) = y], this gives the test error of F on V . We use the notation V | X to denote the projection of V to its features, that is {(x\nx x i , y i )} m i=1 | X \u2192 {x x x i } m i=1 .\nThroughout the paper, we use N (\u2022) to denote a neighborhood function for perturbing features: That is, N\n(x x x) = {x x x | d(x x x , x x x) < } is a set of examples that are close to x x x in terms of a distance metric d (e.g., d(x x x , x x x) = x x x \u2212 x x x p ). Given U = {x x x i } m i=1 , let N (U ) = {{x x x i } m i=1 | d(x x x i , x x x i ) < , i = 0, .\n. . , m}. Since labels are not changed for adversarial examples, we also use the notation N (V ) to denote perturbations of features, with labels fixed.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "MODELING TRANSDUCTIVE ROBUSTNESS", "text": "In this section we formulate and analyze threat models for transductive defenses. We first formulate a threat model for a single-round game between the attacker and the defender. We then consider extensions of this threat model to multiple rounds, which are necessary when considering DENT (Wang et al., 2021) and RMC (Wu et al., 2020b), and point out important subtleties in modeling that were not articulated in previous work. We characterize previous test-time defenses using our threat models.\n1-round game. In this case, the adversary \"intercepts\" a clean test data V (with clean features U = V | X , and labels V | Y ), adversarially perturbs it, and sends a perturbed features U to the defender. The defender learns a new model based on U . A referee then evaluates the accuracy of the adapted model on U . Formally: Definition 1 (1-round threat model for transductive adversarial robustness). Fix an adversarial perturbation type (e.g., \u221e perturbations with perturbation budget \u03b5). Let P X,Y be a data generation distribution. The attacker is an algorithm A, and the defender is a pair of algorithms (T , \u0393), where T is a supervised learning algorithm, and \u0393 is a transductive learning algorithm. A (clean) training set D is sampled i.i.d. from P X,Y . A (clean) test set V is sampled i.i.d. from P X,Y .\n\u2022 [Training time, defender] The defender trains an optional base model F = T (D), using the labeled source data D.\n\u2022 [Test time, attacker] The attacker receives V , and produces an (adversarial) unlabeled dataset U :\n1. On input \u0393, F , D, and V , A perturbs each point (x x x, y) \u2208 V to (x x x , y) (subject to the agreed attack type), giving V = A(\u0393, F, D, V ) (that is, V \u2208 N (V )). 2. Send U = V | X (the feature vectors of V ) to the defender.\n\u2022 [Test time, defender] The defender produces a model as F * = \u0393(F, D, U ).\nMulti-round games. The extension of 1-round games to multi-round contains several important considerations that were implicit or unclear in previous work, and is closely related to what it means by adaptive attacks. Specifically: Private randomness. Note that \u0393 uses randomness, such as random initialization and random restarts 2 in adversarial training. Since these randomness are generated after the attacker's move, they are treated as private randomness, and not known to the adversary.\nIntermediate defender states leaking vs. Non-leaking. In a multi-round game, the defender may maintain states across rounds. For example, the defender may store test data and updated models from previous rounds, and use them in a new round. If these intermediate defender states are \"leaked\" to the attacker, we call it intermediate defender states leaking, or simply states leaking, otherwise we call it non states-leaking, or simply non-leaking. Note that the attacker cannot simply compute these information by simulating on the training and testing data, due to the use of private randomness. We note that, however, the initial pretrained model is assumed to be known by the attacker. The attacker can also of course maintain arbitrary states, and are assumed not known to the defender.\nAdaptive vs. Non-adaptive. Because transductive learning happens after the attacker produces U , the attacker may not be able to directly attack the model \u0393 produced. Nevertheless, the attacker is assumed to have full knowledge of the transductive mechanism \u0393, except the private randomness. In this paper we call an attack adaptive if it makes explicit use of the knowledge of \u0393.\nNaturally ordered vs. Adversarially ordered. Both RMC and DENT handle batches of fixed sizes. An intuitive setup for multi-round game is that the batches come in sequentially, and the attacker must forward perturbed versions of these batches in the same order to the defender, which we call the \"naturally ordered\" game. However, this formulation does not capture an important scenario: An adversary can wait and pool a large amount of test data, then chooses a \"worst-case\" order of perturbed data points, and then sends them in batches one at a time for adaptation in order to maximize the breach. We call the latter \"adversarially ordered\" game. We note that all previous work only considered naturally-ordered game, which gives the defender more advantages, and is thus our focus in the rest of the paper. Adversarially-ordered game is evaluated for DENT in Appendix A.7.\nModeling capacity of our threat models. Our threat models encompass a large family of defenses. For example, without using \u0393, the threat model degenerates to the classic inductive threat model. Our threat models also capture various \"test-time defenses\" proposals (e.g., those broken by the BPDA (Athalye et al., 2018)), where \u0393 is a \"non-differentiable\" function which \"sanitizes\" the test data, instead of updating the model, before sending them to a fixed pretrained model. Therefore, in particular, these proposals are not transductive-learning based. Below we describe previous defenses which we study in the rest of this paper, where \u0393 is indeed transductive learning.\nExample 1 (Runtime masking and cleansing). Runtime masking and cleansing (RMC) (Wu et al., 2020b) is a recent transductive-learning defense. For RMC, the defender is stateful and adapted from the model learned in the last round, on a single test point (|U | = 1): The adaptation objective is F * = arg min F (x x x,y)\u2208N ( x x x) L(F, x x x, y), where x x x is the test time feature point, and N ( x x x) is the set of examples in the adversarial training dataset D that are top-K nearest to x x x in a distance measure. RMC paper considered two attacks: (1) Transfer attack, which generates perturbed data by attacking the initial base model, and (2) PGD-skip attack, which at round p + 1, runs PGD attack on the model learned at round p. In our language, transfer attack is stateless (i.e. the adversary maintains no state) and non-adaptive, PGD-skip attack is state-leaking, but still non-adaptive.\nExample 2 (Defensive entropy minimization (DENT (Wang et al., 2021))). DENT adapts the model using test input, and can work with any training-time learning procedure. The DENT defender is stateless: It always starts the adaptation from the original pretrained model, fixed at the training time. During the test-time adaptation, only the affine parameters in batch normalization layers of the base model are updated, using entropy minimization with the information maximization regularization. In this paper, we show that with strong adaptive attacks under the naturally ordered setting, we are able to reduce the robustness to be almost the same as that of static models (see Section 6). Further, under the adversarially ordered setting, we can completely break DENT.\nExample 3 (Goldwasser et al.'s transductive threat model). While seemingly our threat model is quite different from the one described in Goldwasser et al. (2020), one can indeed recover their threat model naturally as a 1-round game: First, for the perturbation type, we simply allow arbitrary perturbations in the threat model setup. Second, we have a fixed pretrained model F , and the adaptation algorithm \u0393 learns a set S which represents the set of \"allowable\" points (so F | S yields a predictor with redaction, namely it outputs \u22a5 for points outside of S). Third, we define two error functions as ( 5) and ( 6) in Goldwasser et al. (2020):\nerr U (F | S , f ) \u2261 1 |U | x x x \u2208 U \u2229 S F (x x x ) = f (x x x ) , rej U (S) \u2261 |U \\ S| |U | (1)\nwhere f is the ground truth hypothesis. The first equation measures prediction errors in U that passed through S, and the second equation measures the rejection rate of the clean input. The referee evaluates by measuring two errors:\nL(F | S , V ) = (err U (F | S ), rej U (S)).", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "ADAPTIVE ATTACKS IN ONE ROUND", "text": "In this section we study a basic question: How to perform adaptive attacks against a transductivelearning based defense in one round? Note that, in each round of a multi-round game, an independent batch of test input U is sampled, and the defender can use transductive learning to produce a model specifically adapted to the adversarial input U , after the defender receives it. Therefore, it is of fundamental interest to attack this ad-hoc adaptation. We consider white-box attacks: The attacker knows all the details of \u0393, except private randomness, which is sampled after the attacker's move.\nWe deduce a principle for adaptive attacks in one round, which we call the principle of attacking model space: Effective attacks against a transductive defense may need to consider attacking a set of representative models induced in the neighborhood of U . We give concrete instantiations of this principle, and show in experiments that they break previous transductive-learning based defenses.\nAttacks in multi-round. If the transductive-learning based defense is stateless, then we simply repeat one-round attack multiple times. If it is stateful, then we need to consider state-leaking setting or non-leaking setting. For all experiments in Section 6, we only evaluate non-leaking setting, which is more challenging for the adversary.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "GOAL OF THE ATTACKER AND CHALLENGES", "text": "To start with, given a defense mechanism \u0393, the objective of the attacker can be formulated as:\nmax V \u2208N (V ),U =V | X L a (\u0393(F, D, U ), V ). (2\n)\nwhere L a is the loss function of the attacker. We make some notational simplifications: Since D is a constant, in the following we drop it and write \u0393(U ). Also, since the attacker does not modify the labels in the threat model, we abuse the notation and write the objective as\nmax V ,U =V | X L a (\u0393(U ), U ).(3)\nA generic attacker would proceed iteratively as follows: It starts with the clean test set V , and generates a sequence of (hopefully) increasingly stronger attack sets U (0) = V | X , U (1) , . . . , U (i) (U (i) must satisfy the attack constraints at U , such as \u221e bound). We note several basic but important differences between transductive attacks and inductive attacks in the classic minimax threat model:\n(D1) \u0393(U ) is not differentiable.\nFor the scenarios we are interested in, \u0393 is an optimization algorithm to solve an objective F * \u2208 arg min F L d (F, D, U ). This renders (3) into a bilevel optimization problem (Colson et al., 2007):\nmax V \u2208N (V );U =V | X L a (F * , V ) subject to: F * \u2208 arg min F L d (F, D, U ),(4)\nIn these cases, \u0393 is in general not (in fact far from) differentiable. A natural attempt is to approximate \u0393 with a differentiable function, using theories such as Neural Tangent Kernels (Jacot et al., 2018).\nUnfortunately no existing theory applies to the transductive learning, which deals with unlabeled data U (also, as we have remarked previously, tricks such as BPDA (Athalye et al., 2018) also does not apply because transductive learning is much more complex than test-time defenses considered there).\n(D2) U appears in both attack and defense. Another significant difference is that the attack set U also appears as the input for the defense (i.e. \u0393(U )). Therefore, while it is easy to find U to fail \u0393(U ) for any fixed U , it is much harder to find a good direction to update the attack and converge to an attack set U * that fails an entire model space induced by itself: \u0393(U * ).\n(D3) \u0393(U ) can be a random variable. In the classic minimax threat model, the attacker faces a fixed model. However, the output of \u0393 can be a random variable of models due to its private randomness, such as the case of Randomized Smoothing (Cohen et al., 2019). In these cases, successfully attacking a single sample of this random variable does not suffice.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Algorithm 1 FIXED POINT ATTACK (FPA)", "text": "Require: A transductive learning algorithm \u0393, an optional training dataset D, a clean test set V , an initial model F (0) , and an integer parameter T \u2265 0 (the number of iterations). 1: for i = 0, 1, . . . , T do 2:\nAttack the model obtained in the last iteration to get the perturbed set:\nV (i) = arg max V \u2208N (V ) La(F (i) , V ) (5)\nwhere La is a loss function. Set U (i) = V (i) |X .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "3:", "text": "Run the transductive learning algorithm \u0393 to get the next model: F (i+1) = \u0393(D, U (i) ). 4: end for 5: Select the best attack set U (k) as k = arg max 0\u2264i\u2264T L(F (i+1) , V (i) ). 6: return U (k) .\nFixed Point Attack: A first attempt. We adapt previous literature for solving bilevel optimization in deep learning setting (Lorraine & Duvenaud, 2018) (designed for supervised learning). The idea is simple: At iteration i + 1, we fix U (i) and model space F (i) = \u0393(U (i) ), and construct U (i+1) to fail it. We call this the Fixed Point Attack (FPA) (Algorithm 1), as one hopes that this process converges to a good fixed point U * . Unfortunately, we found FPA to be weak in experiments. The reason is exactly (D2): U (i+1) failing F (i) may not give any indication that it can also fail F (i+1) induced by itself. Note that transfer attack is a special case of FPA by setting T = 0.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "STRONG ADAPTIVE ATTACKS FROM ATTACKING MODEL SPACES", "text": "To develop stronger adaptive attacks, we consider a key property of the adversarial attacks: The transferability of adversarial examples. Various previous work have identified that adversarial examples transfer (Tram\u00e8r et al., 2017;Liu et al., 2016), even across vastly different architectures and models. Therefore, if U is a good attack set, we would expect that U also fails \u0393(U ) for U close to U . This leads to the consideration of the following objective:  (i) . Also, considering a model space gives the attacker more information in dealing with the nondifferentiability of \u0393 (D1).\nmax U min U \u2208N (U ) L a (\u0393(U ), U ). (6\n(2) It relaxes the attacker-defender constraint (D2). Perhaps more importantly, for the robust objective, we no longer need the same U to appear in both defender and attacker. Therefore it gives a natural relaxation which makes attack algorithm design easier.\nIn summary, while \"brittle\" U that does not transfer may indeed exist theoretically, their identification can be challenging algorithmically, and its robust variant provides a natural relaxation considering both algorithmic feasibility and attack strength. This thus leads us to the following principle:\nThe Principle of Attacking Model Spaces. An effective adaptive attack against a transductive-learning based defense may need to consider a model space induced by a proper neighborhood of U .", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Algorithm 2 GREEDY MODEL SPACE ATTACK (GMSA)", "text": "Require: A transductive learning algorithm \u0393, an optional training dataset D, a clean test set V , an initial model F (0) , and an integer parameter T \u2265 0 (the number of iterations). 1: for i = 0, 1, . . . , T do 2:\nAttack the previous models to get the perturbed set:\nV (i) = arg max V \u2208N (V )\nLGMSA({F\n(j) } i j=0 , V ) (7)\nwhere\nLGMSA is a loss function. Set\nU (i) = V (i) |X . 3:\nRun the transductive learning algorithm \u0393 to get the next model:\nF (i+1) = \u0393(D, U (i) ). 4: end for 5: Select the best attack U (k) as k = arg max 0\u2264i\u2264T L(F (i+1) , V (i) ), 6: return U (k) .\nAn instantiation: Greedy Model Space Attack (GMSA). We give a simplest possible instantiation of the principle, which we call the Greedy Model Space Attack (Algorithm 2). In experiments we use this instantiation to break previous defenses. In this instantiation, the family of model spaces to consider is just all the model spaces constructed in previous iterations. L GMSA ({F (j) } i j=0 , V ) is a loss function that the attacker uses to attack the history model spaces. We consider two instantiations: (1)   \nL AVG GMSA ({F (j) } i j=0 , V ) = 1 i+1 i j=0 L a (F (j) , V ), (2) L MIN GMSA ({F (j) } i j=0 , V ) = min 0\u2264j\u2264i L a (F (j) , V ),", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "EMPIRICAL STUDY", "text": "This section evaluates various transductive-learning based defenses. Our main findings are: (1)\nThe robustness of existing transductive defenses like RMC and DENT is overestimated. Under our evaluation framework, those defenses either have little robustness or have almost the same robustness as that of the static base model. To this end, we note that while AutoAttack is effective in evaluating the robustness of static models, it is not effective in evaluating the robustness of transductive defenses. In contrast, our GMSA attack is a strong baseline for attacking transductive defenses.\n(2) We experimented a novel idea of applying Domain Adversarial Neural Networks (Ajakan et al., 2014), an unsupervised domain adaptation technique (Wilson & Cook, 2020), as a transductive-learning based defense. We show that DANN has nontrivial and even better robustness compared to existing work, under AutoAttack, PGD attack, and FPA attack, even though it is broken by GMSA.\n(3) We report a somewhat surprising phenomenon on transductive adversarial training: Adversarially retraining the model using fresh private randomness on a new batch of test-time data gives a significant increase in robustness, against all of our considered attacks. (4) Finally, we demonstrated that URejectron, while enjoying theoretical guarantees in the bounded-VC dimensions situation, can be broken in natural deep learning settings.\nEvaluation framework. For each defense, we report accuracy and robustness. The accuracy is the performance on the clean test inputs, and the robustness is the performance under adversarial attacks. The robustness of transductive defenses is estimated using AutoAttack (AA) 3 , PGD attack, FPA, GMSA-MIN and GMSA-AVG. We use PGD attack and AutoAttack in the transfer attack setting for the transductive defense: We generate adversarial examples by attacking a static model (e.g. the base model used by the transductive defense), and then evaluate the transductive defense on the generated adversarial examples. Accuracy and robustness of the static models are also reported for comparison. We always use AutoAttack to estimate the robustness of static models since it is the state-of-the-art for the inductive setting. For all experiments, the defender uses his own private randomness, which is different from the one used by the attacker. Without specified otherwise, all reported values are percentages. Below we give details. Appendix A gives details for replicating the results.\nRuntime Masking and Cleansing (RMC (Wu et al., 2020b)). RMC adapts the network at test time, and was shown to achieve state-of-the-art robustness under several attacks that are unaware of the defense mechanism (thus these attacks are non-adaptive according to our definition). We follow the setup in Wu et al. (2020b) to perform experiments on MNIST and CIFAR-10 to evaluate the  robustness of RMC. On MNIST, we consider L \u221e norm attack with = 0.3 and on CIFAR-10, we consider L \u221e norm attack with = 8/255. The performance of RMC is evaluated on a sequence of test points x x x (1) , \u2022 \u2022 \u2022 , x x x (n) randomly sampled from the test dataset. So we have a n-round game. The FPA and GMSA attacks are applied on each round and the initial model F (0) used by the attacks at the (k + 1)-th round is the adapted model (with calibration in RMC) obtained at the k-th round.\nTo save computational cost, we set n = 1000. The robustness of RMC is evaluated on a sequence of adversarial examplesx x x (1) , \u2022 \u2022 \u2022 ,x x x (n) generated by the attacker on the sequence of test points n) . We evaluate the robustness of RMC in the non-state leaking setting with private randomness (both are in favor of the defender).\nx x x (1) , \u2022 \u2022 \u2022 , x x x (\nResults. The results are in Table 1. RMC with the standard model is already broken by FPA attack (weaker than GSMA). Compared to the defense-unaware AutoAttack, our GMSA-AVG attack reduces the robustness from 97.70% to 0.50% on MNIST and from 94.20% to 8.00% on CIFAR-10. Further, RMC with adversarially trained model actually provides worse adversarial robustness than using adversarial training alone. Under our GMSA-MIN attack, the robustness is reduced from 96.10% to 58.80% on MNIST and from 71.70% to 39.60% on CIFAR-10.\nDefensive Entropy Minimization (DENT (Wang et al., 2021)   cannot directly attack the resulting model as in the inductive case. Specifically, for our GMSA attacks, we attack (with loss L AVG GMSA or L MIN GMSA ) an ensemble of T = 10 models, adversarially trained with independent randomness, and generate a perturbed test set U . Then we adversarially train another model from scratch with independent randomness, and check whether U transfers to the new model (this thus captures the scenario described earlier). Somewhat surprisingly, we show that U does not transfer very well, and the TADV improves robustness significantly.\nResults. Table 4 shows that transductive adversarial training significantly improves the robustness of adversarial training . On MNIST, the robustness is improved from 86.61% to 94.27%. On CIFAR-10, the robustness is improved from 45.29% to 54.12%.\nURejectron in deep learning settings. URejectron performs transductive learning for defense, and has theoretical guarantees under bounded VC dimension case. We evaluated URejectron on GTSRB dataset using ResNet18 network. We used the same implementation by Goldwasser et al.. Results. Figure 1(a) shows that for transfer attacks generated by PGD attack , URejectron can indeed work as expected. However, by using different attack algorithms, such as CW attack (Carlini & Wagner, 2017), we observe two failure modes: (1) Imperceptible adversarial perturbations that slip through. Figure 1(b) shows that one can construct adversarial examples that are very similar to the clean test inputs that can slip through their URejectron construction of S (in the deep learning setting), and cause large errors. (2) Benign perturbations that get rejected. Figure 1(c) shows that we can generate \"benign\" perturbed examples using image corruptions, such as slightly increased brightness, but URejectron rejects all.", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "CONCLUSION", "text": "In this paper, we formulate threat models for transductive defenses and propose an attack framework called Greedy Model Space Attack (GMSA) that can serve as a new baseline for evaluating transductive defenses. We show that GMSA can break previous transductive defenses, which were resilient to previous attacks such as AutoAttack. On the positive side, we show that transductive adversarial training gives a significant increase in robustness against attacks we consider. For the future work, one can explore transductive defenses that can be robust under our GMSA attacks, and can also explore even stronger adaptive attacks that are effective in evaluating transductive defenses.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "The work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, IIS-2008559, SaTC-Frontiers-1804648, CCF-2046710 and CCF-1652140, and ARO grant number W911NF-17-1-0405. Jiefeng Chen and Somesh Jha are partially supported by the DARPA-GARD problem under agreement number 885000.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "ETHICS STATEMENT", "text": "We believe that our work gives positive societal impact in the long term. In the short term potentially some services deploying existing transductive defenses may be broken by adversaries who leverage our new attacks. However, our findings give a necessary step to identify transductive defenses that really work and deepened our understanding of this matter. It also gives positive impact by advancing the science for trustworthy machine learning and potentially how deep transductive learning works.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "REPRODUCIBILITY STATEMENT", "text": "We have included enough experimental details to ensure reproducibility in Section 6 and Appendix A. Also, the source code for the experiments is submitted as supplemental materials.\nadversarial training, we train the model for 100 epochs using the Adam optimizer with a batch size of 128 and a learning rate of 10 \u22123 . We use the L \u221e norm PGD attack as the adversary for adversarial training with a perturbation budget of 0.3, a step size of 0.01, and number of steps of 40.\nRMC configuration. We set K = 1024. Suppose the clean training set is D. Let D contain |D| clean inputs and |D| adversarial examples. So N = 2|D|. We generate the adversarial examples using the L \u221e norm PGD attack with a perturbation budget of 0.3, a step size of 0.01, and number of steps of 100. We extract the features from the penultimate layer of the model and use the Euclidean distance in the feature space of the model to find the top-K nearest neighbors of the inputs. When adapting the model, we use Adam as the optimizer and set the learning rate to be 2 \u00d7 10 \u22124 . We train the model until the early-stop condition holds. That is the training epoch reaches 100 or the validation loss doesn't decrease for 5 epochs.\nAttack configuration. We use the same threat model for all attacks: L \u221e norm perturbation with a perturbation budget of 0.3. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN:\nL a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212 log f (x x x) y , where f (x x x)\nis the softmax output of the model F . We use PGD with a step size of 0.01, the number of steps of 100, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2.2 CIFAR-10", "text": "Model architecture and training configuration. We use the ResNet-32 network (He et al., 2016). For both standard training and adversarial training, we train the model for 100 epochs using Stochastic Gradient Decent (SGD) optimizer with Nesterov momentum and learning rate schedule. We set momentum 0.9 and 2 weight decay with a coefficient of 10 \u22124 . The initial learning rate is 0. 255 , a step size of 1 255 , and number of steps of 40. We extract the features from the penultimate layer of the model and use the Euclidean distance in the feature space of the model to find the top-K nearest neighbors of the inputs. We use Adam as the optimizer and set the learning rate to be 2.5 \u00d7 10 \u22125 . Attack configuration. We use the same threat model for all attacks: L \u221e norm perturbation with a perturbation budget of 8 255 . Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN:\nL a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212 log f (x x x) y ,\nwhere f (x x x) is the softmax output of the model F . We use PGD with a step size of 1 255 , the number of steps of 40, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.3 SETUP FOR DENT EXPERIMENTS", "text": "DENT configuration. We perform experiments to evaluate the best version of DENT (DENT+ in Wang et al. (2021)) on CIFAR-10 following the experimental settings in Wang et al. (2021). We use the pre-trained robust models on CIFAR-10 under the L \u221e norm perturbation threat model from RobustBench Model Zoo 4 as the static base models for DENT, including models with model ID Wu2020Adversarial_extra (Wu et al., 2020a), Carmon2019Unlabeled (Carmon et al., 2019, Sehwag2020Hydra (Sehwag et al., 2020), Wang2020Improving , Hendrycks2019Using , Wong2020Fast (Wong et al., 2020), and Ding2020MMA (Ding et al., 2020). For the test-time adaptation, only the affine scale \u03b3 and shift \u03b2 parameters in the batch normalization layers of the base model are updated. DENT updates sample-wise with different affine parameters (\u03b3 i , \u03b2 i ) for each input x x x i . The input adaptation of \u03a3 is not used as suggested in Wang et al. (2021). The model is adapted for six steps by AdaMod (Ding et al., 2019) with learning rate of 0.006, batch size of 128 and no weight decay. The adaptation objective is entropy minimization with the information maximization regularization:\nmin \u03b8i b i=1 \u2212 C c=1 f (x x x i ; \u03b8 i ) c \u2022 log f (x x x i ; \u03b8 i ) c + C c=1 b i=1 f (x x x i ; \u03b8 i ) c \u2022 log b i=1 f (x x x i ; \u03b8 i ) c (8\n)\nwhere b is the batch size, C is the number of classes and f (x x x i ; \u03b8 i ) is the softmax output of the model f with the affine parameters \u03b8 i = (\u03b3 i , \u03b2 i ) for the input x x x i .\nAttack configuration. We use the same threat model for all attacks: L \u221e norm perturbation with a perturbation budget of 8 255 . For PGD attack, FPA, GMSA-AVG and GMSA-MIN, we use the following loss function to find adversarial examples with high confidence:\nL a (F, V ) = 1 |V | (x x x,y)\u2208V max k =y f (x x x) k , where f (x x x)\nis the softmax output of the model F . However, it is hard to optimize this loss function. Thus, we use two alternative loss functions to find adversarial examples. One is the untargeted CW loss (Carlini & Wagner, 2017):\nL 1 a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212Z(x x x) y + max k =y Z(x x x) k , where Z(x x x)\nis the logits of the model F (the output of the layer before the softmax layer). The other is the targeted CW loss:\nL 2 a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212Z(x x x) y + Z(x x x) t ,\nwhere t is the targeted label and t = y. For each attack, we use 14 PGD subroutines to solve its attack objective, including 5 PGD subroutines using the untargeted CW loss L 1 a with different random restarts and 9 PGD subroutines using the targeted CW loss L 2 a with different targeted labels. So for each clean test input x x x, these PGD subroutines will return 14 adversarial examples x x x 1 , . . . , x x x 14 . Among these adversarial examples, we select the one that maximizes the attack loss with the loss function L a (F, V ) as the final adversarial example x x x for x x x. We use the same hyper-parameters for all PGD subroutines: the step size is 1 255 , the number of steps is 100, and the random start is used. We set T = 2 for FPA, GMSA-AVG and GMSA-MIN.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "A.4 SETUP FOR DANN EXPERIMENTS", "text": "We perform experiments on MNIST and CIFAR-10 datasets. We describe the settings for each dataset below.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.4.1 MNIST", "text": "Model architecture. We use the same model architecture as the one used in Chuang et al. (2020) Training configuration. We train the models for 100 epochs using the Adam optimizer with a batch size of 128 and a learning rate of 10 \u22123 . For the representation matching in DANN, we adopt the original progressive training strategy for the discriminator (Ganin et al., 2016) where the weight \u03b1 for the domain-invariant loss is initiated at 0 and is gradually changed to 0.1 using the schedule \u03b1 = ( 2 1+exp(\u221210\u2022p) \u2212 1) \u2022 0.1, where p is the training progress linearly changing from 0 to 1. Attack configuration. We use the same threat model for all attacks: L \u221e norm perturbation with a perturbation budget of 0.3. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN:\nL a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212 log f (x x x) y , where f (x x x)\nis the softmax output of the model F . We use PGD with a step size of 0.01, the number of steps of 200, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A.4.2 CIFAR-10", "text": "Model architecture. We use the ResNet-18 network (He et al., 2016)  Training configuration. We train the models for 100 epochs using stochastic gradient decent (SGD) optimizer with Nesterov momentum and learning rate schedule. We set momentum 0.9 and 2 weight decay with a coefficient of 10 \u22124 . The initial learning rate is 0.1 and it decreases by 0.1 at 50, 75 and 90 epoch respectively. The batch size is 64. We augment the training images using random crop and random horizontal flip. For the representation matching in DANN, we adopt the original progressive training strategy for the discriminator (Ganin et al., 2016) where the weight \u03b1 for the domain-invariant loss is initiated at 0 and is gradually changed to 1 using the schedule \u03b1 = 255 , the number of steps of 100, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A.5 SETUP FOR TADV EXPERIMENTS", "text": "We perform experiments on MNIST and CIFAR-10 datasets. We describe the settings for each dataset below.\nA.5.1 MNIST Model architecture and Training configuration. We use the LeNet network architecture. We train the models for 100 epochs using the Adam optimizer with a batch size of 128 and a learning rate of 10 \u22123 . We use the L \u221e norm PGD attack as the adversary to generate adversarial training examples with a perturbation budget of 0.3, a step size of 0.01, and number of steps of 40. We train on 50% clean and 50% adversarial examples per batch.\nAttack configuration. We use the same threat model for all attacks: L \u221e norm perturbation with a perturbation budget of 0.3. Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: L a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212 log f (x x x) y , where f (x x x) is the softmax output of the model F . We use PGD with a step size of 0.01, the number of steps of 200, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.5.2 CIFAR-10", "text": "Model architecture and Training configuration. We use the ResNet-20 network architecture (He et al., 2016). We train the models for 110 epochs using stochastic gradient decent (SGD) optimizer with Nesterov momentum and learning rate schedule. We set momentum 0.9 and 2 weight decay with a coefficient of 5 \u00d7 10 \u22124 . The initial learning rate is 0.1 and it decreases by 0.1 at 100 and 105 epoch respectively. The batch size is 128. We augment the training images using random crop and random horizontal flip. We use the L \u221e norm PGD attack as the adversary to generate adversarial training examples with a perturbation budget of 8 255 , a step size of 2 255 , and number of steps of 10. We train on 50% clean and 50% adversarial examples per batch.\nAttack configuration. We use the same threat model for all attacks: L \u221e norm perturbation with a perturbation budget of 8 255 . Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN: L a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212 log f (x x x) y , where f (x x x) is the softmax output of the model F . We use PGD with a step size of 1 255 , the number of steps of 100, random start and no restarts. We set T = 9 for FPA, GMSA-AVG and GMSA-MIN.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.6 SETUP FOR UREJECTRON EXPERIMENTS", "text": "We use a subset of the GTSRB augmented training data for our experiments, which has 10 classes and contains 10,000 images for each class. We implement URejectron (Goldwasser et al., 2020) on this dataset using the ResNet18 network (He et al., 2016) in the transductive setting. Following Goldwasser et al. (2020), we implement the basic form of the URejectron algorithm, with T = 1 iteration. That is we train a discriminator h to distinguish between examples from P and Q, and train a classifier F on P . Specifically, we randomly split the data into a training set D train containing 63,000 images, a validation set D val containing 7,000 images and a test set D test containing 30,000 images. We then use the training set D train to train a classifier F using the ResNet18 network. We train the classifier F for 10 epochs using Adam optimizer with a batch size of 128 and a learning rate of 10 \u22123 . The accuracy of the classifier on the training set D train is 99.90% and its accuracy on the validation set D val is 99.63%. We construct a setx consisting of 50% normal examples and 50% adversarial examples.\nThe normal examples in the setx form a set z. We train the discriminator h on the set D train (with label 0) and the setx (with label 1). We then evaluate URejectron's performance onx: under a certain threshold used by the discriminator h, we measure the fraction of normal examples in z that are rejected by the discriminator h and the error rate of the classifier F on the examples in the setx that are accepted by the discriminator h. The set z can be D test or a set of corrupted images generated on D test . We use the method proposed in Hendrycks & Dietterich (2019) to generate corrupted images with the corruption type of brightness and the severity level of 1. The accuracy of the classifier on the corrupted images is 98.90%. The adversarial examples inx are generated by the PGD attack  or the CW attack (Carlini & Wagner, 2017). For PGD attack, we use L \u221e norm with perturbation budget = 8 255 and random initialization. The number of iterations is 40 and the step", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Supplementary Material Towards Evaluating the Robustness of Neural Networks Learned by Transduction", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A EXPERIMENTAL DETAILS", "text": "A.1 GENERAL SETUP", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1.1 COMPUTING INFRASTRUCTURE", "text": "We run all experiments with PyTorch and NVIDIA GeForce RTX 2080Ti GPUs.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1.2 DATASET", "text": "We use three datasets MNIST, CIFAR-10 and GTSRB in our experiments. The details about these datasets are described below.\nMNIST. The MNIST (LeCun, 1998) is a large dataset of handwritten digits. Each digit has 5,500 training images and 1,000 test images. Each image is a 28 \u00d7 28 grayscale. We normalize the range of pixel values to [0, 1].\nCIFAR-10. The CIFAR-10 ( Krizhevsky et al., 2009) is a dataset of 32x32 color images with ten classes, each consisting of 5,000 training images and 1,000 test images. The classes correspond to dogs, frogs, ships, trucks, etc. We normalize the range of pixel values to [0, 1].\nGTSRB. The German Traffic Sign Recognition Benchmark (GTSRB) (Stallkamp et al., 2012) ", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "A.1.3 IMPLEMENTATION DETAILS OF THE ATTACKS", "text": "We use Projected Gradient Descent (PGD)  to solve the attack objectives of PGD attack, FPA, GMSA-AVG and GMSA-MIN. For GMSA-AVG, at the i-th iteration, when applying PGD on the data point x x x to generate the perturbation \u03b4, we need to do one backpropagation operation for each model in {F (j) } i j=0 per PGD step. We do the backpropagation for each model sequentially and then accumulate the gradients to update the perturbation \u03b4 since we might not have enough memory to store all the models and compute the gradients at once, especially when i is large. For GMSA-MIN, we find that it requires more PGD steps to solve the attack objective at the i-th iteration where we need to attack i + 1 models simultaneously. Thus, we scale the number of PGD steps at the i-th iteration by a factor of i + 1 for GMSA-MIN.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 SETUP FOR RMC EXPERIMENTS", "text": "We follow the original settings in Wu et al. (2020b) to perform experiments on MNIST and CIFAR-10 datasets to evaluate the adversarial robustness of RMC. We consider two kinds of base models for RMC: one is the model trained via standard supervised training; the other is the model trained using the adversarial training . We describe the settings for each dataset below.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.2.1 MNIST", "text": "Model architecture and training configuration. We use a neural network with two convolutional layers, two full connected layers and batch normalization layers. For both standard training and  size is 1 255 . The robustness of the classifier under the PGD attack is 3.66%. For CW attack, we use L 2 norm as distance measure and set c = 1 and \u03ba = 0. The learning rate is 0.01 and the number of steps is 100. The robustness of the classifier under the CW attack is 0.00%.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.7 EVALUATE DENT UNDER THE ADVERSARIALLY-ORDERED GAME", "text": "We evaluate the robustness of DENT under the adversarially-ordered game where the adversary can choose a \"worst-case\" order of perturbed data points after receiving a large amount of test data and then sends them in batches one at a time to the defender. Specifically, each time the attacker will generate adversarial examples on up to 256 data points, and then sort the adversarial examples by their labels from lowest to highest, and finally send the sorted adversarial examples in batches one at a time to the defender. Other experimental settings are the same as those described in Appendix A.3. The results in Table 5 show that under the adversarially-ordered game, we can reduce the robustness of DENT to be lower than that of static base models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.8 MULTIPLE RANDOM RUNS OF THE RMC EXPERIMENT", "text": "In Section 6, we describe the experimental setup for evaluating RMC. The performance of RMC is evaluated on a sequence of test points x x x (1) , \u2022 \u2022 \u2022 , x x x (n) randomly sampled from the test dataset. We repeat this experiment five times with different random seeds and report the mean and standard deviation of the results over the multiple random runs of the experiment. When evaluating the robustness of RMC, we only use the GMSA-AVG attack and the GMSA-MIN attack since they are the strongest attacks. From Table 6, we can see that the results don't vary much across different random runs and the conclusion that the proposed GMSA attacks can break RMC still holds.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Online versus batch prediction", "journal": "", "year": "2021", "authors": ""}, {"title": "Domain-adversarial neural networks", "journal": "stat", "year": "2014", "authors": "Hana Ajakan; Pascal Germain; Hugo Larochelle; Fran\u00e7ois Laviolette; Mario Marchand"}, {"title": "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples", "journal": "PMLR", "year": "2018-07-10", "authors": "Anish Athalye; Nicholas Carlini; David A Wagner"}, {"title": "Towards evaluating the robustness of neural networks", "journal": "IEEE Computer Society", "year": "2017-05-22", "authors": "Nicholas Carlini; David A Wagner"}, {"title": "Unlabeled data improves adversarial robustness", "journal": "", "year": "2019-12-14", "authors": "Yair Carmon; Aditi Raghunathan; Ludwig Schmidt; John C Duchi; Percy Liang"}, {"title": "Estimating generalization under distribution shifts via domain-invariant representations", "journal": "PMLR", "year": "2020-07", "authors": "Ching-Yao Chuang; Antonio Torralba; Stefanie Jegelka"}, {"title": "Certified adversarial robustness via randomized smoothing", "journal": "PMLR", "year": "2019-06-15", "authors": "Jeremy M Cohen; Elan Rosenfeld; J Zico Kolter"}, {"title": "An overview of bilevel optimization", "journal": "", "year": "2007", "authors": "Beno\u00eet Colson; Patrice Marcotte; Gilles Savard"}, {"title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks", "journal": "", "year": "2020", "authors": "Francesco Croce; Matthias Hein"}, {"title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks", "journal": "PMLR", "year": "2020-07", "authors": "Francesco Croce; Matthias Hein"}, {"title": "MMA training: Direct input space margin maximization through adversarial training", "journal": "", "year": "2020", "authors": "Yash Gavin Weiguang Ding; Kry Yik Chau Sharma; Ruitong Lui;  Huang"}, {"title": "An adaptive and momental bound method for stochastic learning. CoRR, abs", "journal": "", "year": "1910", "authors": "Jianbang Ding; Xuancheng Ren; Ruixuan Luo; Xu Sun"}, {"title": "Domain-adversarial training of neural networks", "journal": "J. Mach. Learn. Res", "year": "2016", "authors": "Yaroslav Ganin; Evgeniya Ustinova; Hana Ajakan; Pascal Germain; Hugo Larochelle; Fran\u00e7ois Laviolette; Mario Marchand; Victor S Lempitsky"}, {"title": "Beyond perturbations: Learning guarantees with arbitrary adversarial test examples. CoRR, abs", "journal": "", "year": "2007", "authors": "Shafi Goldwasser; Adam Tauman Kalai; Yael Tauman Kalai; Omar Montasser"}, {"title": "A research agenda: Dynamic models to defend against correlated attacks. CoRR, abs", "journal": "", "year": "1903", "authors": "Ian J Goodfellow"}, {"title": "Explaining and harnessing adversarial examples", "journal": "", "year": "2015", "authors": "Ian J Goodfellow; Jonathon Shlens; Christian Szegedy"}, {"title": "Deep residual learning for image recognition", "journal": "IEEE Computer Society", "year": "2016-06-27", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"title": "Benchmarking neural network robustness to common corruptions and perturbations", "journal": "", "year": "2019", "authors": "Dan Hendrycks; Thomas G Dietterich"}, {"title": "Using pre-training can improve model robustness and uncertainty", "journal": "PMLR", "year": "2019-06-15", "authors": "Dan Hendrycks; Kimin Lee; Mantas Mazeika"}, {"title": "Neural tangent kernel: Convergence and generalization in neural networks", "journal": "", "year": "2018", "authors": "Arthur Jacot; Franck Gabriel; Clement Hongler"}, {"title": "Adversarial Robustness -Theory and Practice", "journal": "", "year": "2018", "authors": "Zico Kolter; Aleksander Madry"}, {"title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "Alex Krizhevsky; Geoffrey Hinton"}, {"title": "Adversarial examples in the physical world", "journal": "", "year": "2017-04-24", "authors": "Alexey Kurakin; Ian J Goodfellow; Samy Bengio"}, {"title": "The mnist database of handwritten digits", "journal": "", "year": "1998", "authors": "Yann Lecun"}, {"title": "Delving into transferable adversarial examples and black-box attacks", "journal": "", "year": "2016", "authors": "Yanpei Liu; Xinyun Chen; Chang Liu; Dawn Song"}, {"title": "Stochastic hyperparameter optimization through hypernetworks", "journal": "", "year": "2018", "authors": "Jonathan Lorraine; David Duvenaud"}, {"title": "Towards deep learning models resistant to adversarial attacks", "journal": "", "year": "2018-04-30", "authors": "Aleksander Madry; Aleksandar Makelov; Ludwig Schmidt; Dimitris Tsipras; Adrian Vladu"}, {"title": "Deepfool: A simple and accurate method to fool deep neural networks", "journal": "IEEE Computer Society", "year": "2016-06-27", "authors": "Alhussein Seyed-Mohsen Moosavi-Dezfooli; Pascal Fawzi;  Frossard"}, {"title": "Adversarially robust generalization requires more data", "journal": "", "year": "2018", "authors": "Ludwig Schmidt; Shibani Santurkar; Dimitris Tsipras; Kunal Talwar; Aleksander Madry"}, {"title": "HYDRA: pruning adversarially robust neural networks", "journal": "", "year": "2020", "authors": "Vikash Sehwag; Shiqi Wang; Prateek Mittal; Suman Jana"}, {"title": "Certifying some distributional robustness with principled adversarial training", "journal": "", "year": "2018-04-30", "authors": "Aman Sinha; Hongseok Namkoong; John C Duchi"}, {"title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "journal": "Neural Networks", "year": "2012", "authors": "Johannes Stallkamp; Marc Schlipsing; Jan Salmen; Christian Igel"}, {"title": "On adaptive attacks to adversarial example defenses", "journal": "", "year": "2020", "authors": "Florian Tram\u00e8r; Nicholas Carlini; Wieland Brendel; Aleksander Madry"}, {"title": "The space of transferable adversarial examples", "journal": "", "year": "2017", "authors": "Florian Tram\u00e8r; Nicolas Papernot; Ian Goodfellow; Dan Boneh; Patrick Mcdaniel"}, {"title": "Statistical learning theory", "journal": "Wiley", "year": "1998", "authors": "Vladimir Vapnik"}, {"title": "Fighting gradients with gradients: Dynamic defenses against adversarial attacks", "journal": "", "year": "2021", "authors": "Dequan Wang; An Ju; Evan Shelhamer; David Wagner; Trevor Darrell"}, {"title": "Improving adversarial robustness requires revisiting misclassified examples", "journal": "", "year": "2020", "authors": "Yisen Wang; Difan Zou; Jinfeng Yi; James Bailey; Xingjun Ma; Quanquan Gu"}, {"title": "A survey of unsupervised deep domain adaptation", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": "2020", "authors": "Garrett Wilson; Diane J Cook"}, {"title": "Fast is better than free: Revisiting adversarial training", "journal": "", "year": "2020", "authors": "Eric Wong; Leslie Rice; J Zico Kolter"}, {"title": "Adversarial weight perturbation helps robust generalization", "journal": "", "year": "2020", "authors": "Dongxian Wu; Shu-Tao Xia; Yisen Wang"}, {"title": "Adversarial robustness via runtime masking and cleansing", "journal": "PMLR", "year": "2020-07", "authors": "Yi-Hsuan Wu; Chia-Hung Yuan; Shan-Hung Wu"}, {"title": "Theoretically principled trade-off between robustness and accuracy", "journal": "PMLR", "year": "2019-06-15", "authors": "Hongyang Zhang; Yaodong Yu; Jiantao Jiao; Eric P Xing; Laurent El Ghaoui; Michael I Jordan"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: URejectron in three settings. z contains \"normal\" examples on which the classifier can have high accuracy.x includes z and consists of a mix of 50% \"normal\" examples and 50% adversarial examples. In (a), the normal examples are clean test inputs and the adversarial examples are generated by PGD attack. In (b), the \"normal\" examples are still clean test inputs but adversarial examples are generated by CW attack. In (c), the \"normal\" examples are generated by image corruptions (adversarial examples are generated by PGD attacks).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "1 and it decreases by 0.1 at 50, 75 and 90 epoch respectively. The batch size is 128. We augment the training images using random crop and random horizontal flip. We use the L \u221e norm PGD attack as the adversary for adversarial training with a perturbation budget of 8 255 , a step size of 2 255 , and number of steps of 10. RMC configuration. We set K = 1024. Suppose the clean training set is D. Let D contain |D| clean inputs and 4|D| adversarial examples. So N = 5|D|. We generate the adversarial examples using the L \u221e norm PGD attack with a perturbation budget of 8", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "10\u2022p) \u2212 1, where p is the training progress linearly changing from 0 to 1.Attack configuration. We use the same threat model for all attacks: L \u221e norm perturbation with a perturbation budget of 8 255 . Cross entropy loss is used as the loss function for PGD attack, FPA, GMSA-AVG and GMSA-MIN:L a (F, V ) = 1 |V | (x x x,y)\u2208V \u2212 log f (x x x) y ,where f (x x x) is the softmax output of the model F . We use PGD with a step size of 1", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Madry et al. 99.60 97.00 87.70 95.70 96.10 59.50   ", "figure_data": "DatasetBase ModelAccuracy Static RMCStatic AAAAPGDRobustness RMC FPA GMSA-AVG GMSA-MINMNISTStandard99.50 99.000.00 97.70 98.30 0.600.50 61.401.10 58.80CIFAR-10Standard Madry et al. 83.20 90.90 44.30 77.90 71.70 40.80 94.30 93.10 0.00 94.20 97.60 8.508.00 42.508.10 39.60"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results of evaluating RMC. We also evaluate the static base model for comparison. Bold numbers are worst results.", "figure_data": "Base ModelAccuracy Static DENTStatic AADENT-AAAARobustness DENT PGD FPA GMSA-AVG GMSA-MINWu et al. (2020a)85.7086.1058.0078.8064.40 59.50 59.3059.6059.60Carmon et al. (2019)88.0087.4057.3080.1061.70 58.40 58.4058.5058.50Sehwag et al. (2020)87.3086.9054.9076.5059.60 55.80 55.8055.8055.80Wang et al. (2020)86.6085.6053.6075.9061.30 55.90 55.8056.1056.10Hendrycks et al. (2019)85.8085.5051.8077.2058.40 54.20 54.4054.2054.20Wong et al. (2020)81.2081.0042.4069.7048.90 44.10 44.3044.5044.30Ding et al. (2020)82.4082.4039.7062.8044.80 39.90 39.4039.1039.20"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results of evaluating DANN. Bold numbers are worst results.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Wang et al. also   considers adaptive attacks for DENT, such as attacking the static base model using AutoAttack to generate adversarial examples, which is the same as the AutoAttack (AA) in our evaluation.We evaluate the best version of DENT, called DENT+ in Wang et al., under their original settings on CIFAR-10: DENT is combined with various adversarial training defenses, and only the model adaptation is included without input adaptation. The model is adapted sample-wise for six steps by AdaMod(Ding et al., 2019) with learning rate of 0.006, batch size of 128 and no weight decay. The adaptation objective is entropy minimization with the information maximization regularization. To save computational cost, we only evaluate on 1000 examples randomly sampled from the test dataset. We consider L \u221e norm attack with = 8/255. We design loss functions for the attacks to generate adversarial examples with high confidence (See Appendix A.3 for the details).Results. Table2shows that both DENT-AA and AA overestimate the robustness of DENT. Our PGD attack reduces the robustness of DENT to be almost the same as that of the static defenses. Further, our FPA, GMSA-AVG and GMSA-MIN have similar performance as the PGD attack. The results show that AutoAttack is not effective in evaluating the robustness of transductive defenses. Domain Adversarial Neural Network (DANN(Ajakan et al., 2014)). We consider DANN as a transductive defense for adversarial robustness. We train DANN on the labeled training dataset D (source domain) and unlabeled adversarial test dataset U (target domain), and then evaluate DANN on U . For each adversarial set U , we train a new DANN model from scratch. We use the standard model trained on D as the base model for DANN. We perform experiments on MNIST and CIFAR-10 to evaluate the adversarial robustness of DANN. On MNIST, we consider L \u221e norm attack with = 0.3 and on CIFAR-10, we consider L \u221e norm attack with = 8/255.", "figure_data": "AccuracyRobustnessDatasetMadry et al. TADVMadry et al. AAAAPGDTADV FPA GMSA-AVG GMSA-MINMNIST99.0199.0586.6196.07 96.48 95.4794.2795.48CIFAR-1087.6988.5145.2972.12 59.05 58.6454.1257.77"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results of evaluating TADV. Bold numbers are worst results.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "and extract the features from the third basic block for representation matching. The detailed model architecture is shown below.", "figure_data": "Encodernn.Conv2d(3, 64, kernel_size=3)nn.BatchNorm2dnn.ReLUBasicBlock(in_planes=64, planes=2, stride=1)BasicBlock(in_planes=128, planes=2, stride=2)BasicBlock(in_planes=256, planes=2, stride=2)PredictorDiscriminatorBasicBlock(in_planes=512, planes=2, stride=2)BasicBlock(in_planes=512, planes=2, stride=2)avg_pool2davg_pool2dflattenflattennn.Linear(512, 10)nn.Linear(512, 2)nn.Softmaxnn.Softmax"}], "formulas": [], "doi": "10.1109/SP.2017.49"}
b_forum,b_title,b_url,b_pub_date,b_abstract,b_TL;DR,b_authors,b_keywords,b_venue,b_venue_id,b_number,b_pdf_url,b_signatures,b_bibtex,b_from_venue_id,r_id,r_number,r_replyto,r_invitation,r_signatures,r_readers,r_nonreaders,r_writers,c_content,c_title,c_rating,c_review,c_confidence,c_decision,c_comment,c_metareview,c_recommendation,c_withdrawal confirmation,c_experience_assessment,c_review_assessment:_checking_correctness_of_experiments,c_review_assessment:_thoroughness_in_paper_reading,c_review_assessment:_checking_correctness_of_derivations_and_theory,c_ethics_review,c_summary_of_the_paper,c_main_review,c_summary_of_the_review,c_correctness,c_technical_novelty_and_significance,c_empirical_novelty_and_significance,c_flag_for_ethics_review,c_details_of_ethics_concerns,c_best_paper_award,c_Q1 Summary and contributions,c_Q2 Assessment of the paper,c_Q2(1) Originality/Novelty,c_Q2(2) Significance/Impact,c_Q2(3) Correctness/Technical quality,c_Q2(4) Quality of experiments (Optional),c_Q2(5) Reproducibility,c_Q2(6) Clarity of writing,c_Q3 Main strengths,c_Q4 Main weakness,c_Q5 Detailed comments to the authors,c_Q6 Overall score,c_Q7 Justification for your score,c_Q8 Confidence in your score,c_Q9 Complying with reviewing instructions,c_Q10 Ethical concerns (Optional),c_summary,c_Justification for rating,c_[Optional] Respond to feedback request by the authors,c_nominate_for_a_reproducibility_award,c_paper_type,c_strengths,c_weaknesses,c_justification_of_rating,c_special_issue,c_questions_to_address_in_the_rebuttal,c_detailed_comments,c_metaReview,c_recommendation_for_accepted_papers,c_overall evaluation,c_reviewer's confidence,c_significance,c_soundness,c_scholarship,c_clarity,c_reproducibility,c_Overall Score,c_evaluation,c_intersection,c_importance_comment,c_technical_rigor,c_intersection_comment,c_rigor_comment,c_importance,c_category,c_clarity_comment,c_Bio_Award,c_limitations_and_societal_impact,c_needs_ethics_review,c_ethics_review_area,c_time_spent_reviewing,c_code_of_conduct,c_consistency_experiment,c_ethical_concerns,c_ethical_issues,c_issues_acknowledged,c_issues_acknowledged_description,c_score,c_strengths_and_weaknesses,c_questions,c_limitations,c_ethics_flag,c_presentation,c_contribution,c_award,c_reviews_visibility,c_Brief summary,c_Paper strengths,c_Paper weaknesses,c_Detailed comments,c_Overall Merit Score,c_Poster (if paper is rejected),c_Reviewer Confidence,c_originality,c_technical_quality,c_clarity_of_presentation,c_impact,c_summary_of_recommendation,c_issues,c_reviewer_expertise,c_novelty,c_robotics_focus,c_quality_of_the_limitations_section,c_best_paper_nomination,c_pdf,c_zip_file,c_Summary,c_Main review,c_Overall score,c_your_profile_and_conflicts_of_interest,c_anonymity_and_confidentiality,c_strengths_weaknesses,c_deanonymize_review,c_summary_of_contributions,c_potential_impact_on_the_field_of_AutoML,c_potential_impact_on_the_field_of_AutoML_rating,c_technical_quality_and_correctness,c_technical_quality_and_correctness_rating,c_clarity_rating,c_ethics_rating,c_overall_review,c_review_summary,c_review_rating,c_review_confidence,c_ethics_details_(optional),c_Ethical concerns,c_Confidence,c_Code of conduct acknowledgment,c_Meta Review,c_Recommendation,"c_If Yes, please provide a more detailed review of the ethical concerns ",c_responsible_NLP_research,c_previous_URL,c_previous_PDF,c_response_PDF,c_note_from_EiCs,c_expertise,c_useful,c_writing,c_superlative,c_Sufficiently Alt,c_Conflicts,c_Review Inclusion,c_reproducibility_summary,c_familiar_with_the_original_paper,c_reviewtext,c_problemstatement,c_litreview,c_relevance,c_accessibility,c_results,c_reviewerconfidence,c_groundsforrejection,c_strength_and_weaknesses,"c_clarity,_quality,_novelty_and_reproducibility","c_metareview:_summary,_strengths_and_weaknesses",c_summary_of_AC-reviewer_meeting,c_justification_for_why_not_higher_score,c_justification_for_why_not_lower_score,c_note_from_PC,c_submission_track,c_overall_rating,c_recommended_decision,c_overall_recommendation,c_grade,c_preliminary_rating,c_suggested_changes,c_reason_for_not_giving_a_higher_recommendation,c_reason_for_not_giving_a_lower_recommendation,c_comments_and_feedback_to_the_authors,c_consent_to_archive,c_summary of review,c_detailed comments,c_Reviewer expertise,c_Review (Strengths/Weaknesses),c_final_decision
SkNksoRctQ,Fluctuation-dissipation relations for stochastic gradient descent,https://openreview.net/forum?id=SkNksoRctQ,2018-12-21,"The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.","We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.",Sho Yaida,"stochastic gradient descent,adaptive method,loss surface,Hessian",--,--,588,https://openreview.net/pdf?id=SkNksoRctQ,['ICLR.cc/2019/Conference'],"@inproceedings{
yaida2018fluctuationdissipation,
title={Fluctuation-dissipation relations for stochastic gradient descent},
author={Sho Yaida},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkNksoRctQ},
}",ICLR.cc/2019/Conference,Bkxa5zr52Q,3.0,SkNksoRctQ,ICLR.cc/2019/Conference/-/Paper588/Official_Review,ICLR.cc/2019/Conference/Paper588/AnonReviewer2,['everyone'],[],['ICLR.cc/2019/Conference'],"{'title': 'An innovative paper to assess equilibration in SGD', 'review': 'The paper introduces the concept of fluctuation-dissipation relations to stochastic gradient descent. These relations hold for certain observables in physical systems in equilibrium. In the context of SGD as a non-equilibrium process with a stationary density, they allow to quantify how far away this process is from its stationary state. \n\nOne of the strengths of the paper is that it works in the discrete-time formalism and uses the master equation, as opposed to other recent works that used the continuous-time limit of SGD to derive related (yet different) results. Furthermore, the formalism does not even rely on a locally quadratic approximation of the loss function, or on any Gaussian assumptions of the SGD noise. To the best of my knowledge, all of this is very innovative. Ultimately, the authors propose a practical algorithm to adaptively lowering the learning rate based on testing fluctuation-dissipation relations.\n\nThis is an interesting paper which I recommend to accept. It not only shows new theoretical results, but also conforms their validity in real-world experiments.\n\nI have only a few questions / comments:\n\n1. In Eq. 17 and others where the scalar product of theta and grad(f) occurs, is it implicitly assumed that the optimum of f is at theta=0?\n2. In Fig. 2, the distinction between solid and dotted curves could be made better visible.\n3. For completeness, it would be good to add the following citation:\nStephan Mandt, Matthew D. Hoffman, and David M. Blei. ""Continuous-time limit of stochastic gradient descent revisited.""\xa0NIPS 2015 Workshop on Optimization for Machine Learning.', 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}",An innovative paper to assess equilibration in SGD,"8: Top 50% of accepted papers, clear accept","The paper introduces the concept of fluctuation-dissipation relations to stochastic gradient descent. These relations hold for certain observables in physical systems in equilibrium. In the context of SGD as a non-equilibrium process with a stationary density, they allow to quantify how far away this process is from its stationary state. 

One of the strengths of the paper is that it works in the discrete-time formalism and uses the master equation, as opposed to other recent works that used the continuous-time limit of SGD to derive related (yet different) results. Furthermore, the formalism does not even rely on a locally quadratic approximation of the loss function, or on any Gaussian assumptions of the SGD noise. To the best of my knowledge, all of this is very innovative. Ultimately, the authors propose a practical algorithm to adaptively lowering the learning rate based on testing fluctuation-dissipation relations.

This is an interesting paper which I recommend to accept. It not only shows new theoretical results, but also conforms their validity in real-world experiments.

I have only a few questions / comments:

1. In Eq. 17 and others where the scalar product of theta and grad(f) occurs, is it implicitly assumed that the optimum of f is at theta=0?
2. In Fig. 2, the distinction between solid and dotted curves could be made better visible.
3. For completeness, it would be good to add the following citation:
Stephan Mandt, Matthew D. Hoffman, and David M. Blei. ""Continuous-time limit of stochastic gradient descent revisited.""Â NIPS 2015 Workshop on Optimization for Machine Learning.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown
Byg9AR4YDB,Exploring Cellular Protein Localization Through Semantic Image Synthesis,https://openreview.net/forum?id=Byg9AR4YDB,--,"Cell-cell interactions have an integral role in tumorigenesis as they are critical in governing immune responses. As such, investigating specific cell-cell interactions has the potential to not only expand upon the understanding of tumorigenesis, but also guide clinical management of patient responses to cancer immunotherapies. A recent imaging technique for exploring cell-cell interactions, multiplexed ion beam imaging by time-of-flight (MIBI-TOF), allows for cells to be quantified in 36 different protein markers at sub-cellular resolutions in situ as high resolution multiplexed images. To explore the MIBI images, we propose a GAN for multiplexed data with protein specific attention. By conditioning image generation on cell types, sizes, and neighborhoods through semantic segmentation maps, we are able to observe how these factors affect cell-cell interactions simultaneously in different protein channels. Furthermore, we design a set of metrics and offer the first insights towards cell spatial orientations, cell protein expressions, and cell neighborhoods. Our model, cell-cell interaction GAN (CCIGAN), outperforms or matches existing image synthesis methods on all conventional measures and significantly outperforms on biologically motivated metrics. To our knowledge, we are the first to systematically model multiple cellular protein behaviors and interactions under simulated conditions through image synthesis.","We explore cell-cell interactions across tumor environment contexts observed in highly multiplexed images, by image synthesis using a novel attention GAN architecture.","Daniel Li,Qiang Ma,Andrew Liu,Justin Cheung,Dana Peâer,Itsik Peâer","Computational biology,image synthesis,GANs,exploring multiplex images,attention,interpretability",--,--,1442,https://openreview.net/pdf?id=Byg9AR4YDB,['ICLR.cc/2020/Conference'],"@misc{
li2020exploring,
title={Exploring Cellular Protein Localization Through Semantic Image Synthesis},
author={Daniel Li and Qiang Ma and Andrew Liu and Justin Cheung and Dana Pe{\textquoteright}er and Itsik Pe{\textquoteright}er},
year={2020},
url={https://openreview.net/forum?id=Byg9AR4YDB}
}",ICLR.cc/2020/Conference,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown
SyAbZb-0Z,Transfer Learning to Learn with Multitask Neural Model Search,https://openreview.net/forum?id=SyAbZb-0Z,--,"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.
In this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","Catherine Wong,Andrea Gesmundo","Learning to Learn,Meta learning,Reinforcement learning,Transfer learning",--,--,649,https://openreview.net/pdf?id=SyAbZb-0Z,['ICLR.cc/2018/Conference'],"@misc{
wong2018transfer,
title={Transfer Learning to Learn with Multitask Neural Model Search},
author={Catherine Wong and Andrea Gesmundo},
year={2018},
url={https://openreview.net/forum?id=SyAbZb-0Z},
}",ICLR.cc/2018/Conference,ByU2NJ6Hf,438.0,SyAbZb-0Z,ICLR.cc/2018/Conference/-/Acceptance_Decision,ICLR.cc/2018/Conference/Program_Chairs,['everyone'],[],['ICLR.cc/2018/Conference/Program_Chairs'],"{'decision': 'Reject', 'title': 'ICLR 2018 Conference Acceptance Decision', 'comment': ""This paper presents a sensible, but somewhat incremental, generalization of neural architecture search.  However, the experiments are only done in a single artificial setting (albeit composed of real, large-scale subtasks).  It's also not clear that such an expensive meta-learning based approach is even necessary, compared to more traditional approaches.\n\nIf this paper was less about proposing a single new extension, and more about putting that extension in a larger context, (either conceptually or experimentally), it would be above the bar.\n""}",ICLR 2018 Conference Acceptance Decision,,,,Reject,"This paper presents a sensible, but somewhat incremental, generalization of neural architecture search.  However, the experiments are only done in a single artificial setting (albeit composed of real, large-scale subtasks).  It's also not clear that such an expensive meta-learning based approach is even necessary, compared to more traditional approaches.

If this paper was less about proposing a single new extension, and more about putting that extension in a larger context, (either conceptually or experimentally), it would be above the bar.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rejected

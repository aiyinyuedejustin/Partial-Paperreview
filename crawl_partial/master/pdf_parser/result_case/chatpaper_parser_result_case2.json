{"Abstract": "ABSTRACT The discovery of large-scale discrete latent structures is crucial for understanding the fundamental generative processes of language. In this work, we use structured latent variables to study the representation space of contextualized embeddings and gain insight into the hidden topology of pretrained language models. However, existing methods are severely limited by issues of scalability and ef\ufb01ciency as working with large combinatorial spaces requires expensive memory consumption. We address this challenge by proposing a Randomized Dynamic Programming (RDP) algorithm for the approximate inference of structured models with DP-style exact computation (e.g., Forward-Backward). Our technique samples a subset of DP paths reducing memory complexity to as small as one percent. We use RDP to analyze the representation space of pretrained language models, discovering a large-scale latent network in a fully unsupervised way. The induced latent states not only serve as anchors marking the topology of the space (neighbors and connectivity), but also reveal linguistic properties related to syntax, morphology, and semantics. We also show that traversing this latent network yields unsupervised paraphrase generation. 1 ", "Introduction": "INTRODUCTION The discovery of large-scale discrete latent structures is crucial for understanding the fundamental generative processes of language, and has been shown useful to various NLP tasks ranging from data-to-text generation (Li & Rush, 2020), summarization (Angelidis et al., 2021), syntactic parsing (Kim et al., 2019), and knowledge graph reasoning (Qu et al., 2020). In this work, we use latent structures to analyze geometric properties of representation space of pretrained language models (PLMs). Despite the large volume of recent work analyzing PLMs and proposing various improvements (Rogers et al., 2020), little is known about the topological structure of their representation manifold. Since such structure cannot be easily observed, it is only natural to resort to latent variables. Yet scaling discrete combinatorial structures is extremely dif\ufb01cult with multiple modeling and computational challenges (Wainwright & Jordan, 2008). In this work, we address the computational challenges arising from working with combinatorial structures. We consider linear-chain CRFs, a popular structured model family (Ma & Hovy, 2016; Sutton & McCallum, 2006) that uses dynamic programming for exact inference. Speci\ufb01cally, we focus on the forward algorithm (Rabiner, 1989), which is widely used to compute the partition function. Space complexity for this algorithm is O(TN 2) where N is the number of latent states and T the length of the sequence. It is precisely the N 2 term that becomes problematic when we construct the adjacent gradient graph with automatic differentiation. DP-based inference algorithms are not optimized for modern computational devices like GPUs and typically work under small-data regimes, with N in the range [10, 100] (Ma & Hovy, 2016; Wiseman et al., 2018). With larger N, inference becomes intractable since gradients do not easily \ufb01t into GPU memory (Sun et al., 2019). Our algorithmic contribution is a randomization technique for dynamic programming which allows us to scale N to thousands (possibly more) latent states. Speci\ufb01cally, to approximate the partition function, instead of summing over all possible combinations of latent states, we only sum over paths with most probable states, and sample a subset of less likely paths to correct the bias according 1 Under review as a conference paper at ICLR 2021 to a reasonable proposal. Since we only calculate the sampled path, memory consumption can be reduced to a small controllable budget which is scale invariant. With a larger memory budget, our method becomes more accurate, and our estimation error smaller. We thus recast the memory complexity challenge into a tradeoff between memory budget, proposal accuracy, and estimation error. When applied to linear-chain CRFs, we show that RDP scales the model by two orders of magnitude with memory complexity as small as one percent of the full DP. Beyond linear-chains, RDP is applicable to any structured model with DP-style exact inference such as trees (Kim et al., 2019) and semi-Markov models (Li & Rush, 2020), and could also be extended to more general message passing algorithms (Wainwright & Jordan, 2008). Our analytical contribution is a geometric study of the representation manifold of PLMs, using the proposed RDP algorithm. We hypothesize that there exist latent anchor embeddings (or landmarks) that describe the manifold topology. We also expect these anchor states to be informative enough to generate sentences, and their connections to be linguistically meaningful. We induce latent structures using a VAE with an inference model parameterized by a scaled CRF where state-word relations are modeled by the emission potential and state-state transitions are modeled by the transition matrix. The connections of words and states together form a latent network. We use the vector product between contextualized embeddings and state embeddings to parameterize the CRF potentials, bringing together the geometry of the representation space with graphical model inference. We further show that it is possible to generate paraphrases by traversing the induced network. Our approach is fully unsupervised and the discovered latent network is intrinsic to the representation manifold, rather than imposed by external supervision, eschewing the criticism of much previous work on supervised probes (Hewitt & Liang, 2019; Chen et al., 2021). In experiments, we \ufb01rst verify the basic properties of RDP (bias-variance) and show its effectiveness for training latent variable models. We then visualize the discovered network based on BERT Devlin et al. (2019), demonstrating how states encode information pertaining to syntax, morphology, and semantics. Finally, we perform unsupervised paraphrase generation by latent network traversal. 2 RANDOMIZED DYNAMIC PROGRAMMING Preliminaries: Speeding Summation by Randomization To motivate our randomized DP, we start with a simple setting, namely estimating the sum of a sorted list. Given a sorted list of positive numbers a, naive summation S = a1 + ..., +aN requires N \u2212 1 addition operations, which is expensive when N is large. Suppose we wish to reduce the number of addition operations to K1 << N, and we already know that the list is long-tailed (similar to how words in language follow a Zip\ufb01an distribution such that there are few very high-frequency words that account for most of the tokens in text and many low-frequency words). Then, we only need to sum over the top K1 values to get an ef\ufb01cient estimate: \u02c6S1 = a1 + ... + aK1 where {ai}N i=1 sorted, large to small (1) Clearly, \u02c6S1 underestimates S. When the summands are \u201cdense\u201d, i.e., not very different from each other, the bias is large because the top K1 terms do not contribute much to the sum (Fig. 1A). To correct this bias, we add samples a\u03b41, ..., a\u03b4K2 from the remaining summands whose indices \u03b4i are sampled from proposal \u03b4i \u223c q = [qK1+1, ..., qN]: \u02c6S2 = a1 + ... + aK1 + 1 K2 ( 1 q\u03b41 a\u03b41 + ... + 1 q\u03b4K2 a\u03b4K2 ) \u03b4i \u2208 {K1 + 1, ..., N} (2) where K1 + K2 = K. Note that this is an unbiased estimator as E[ \u02c6S2] = S, irrespective of how we choose q. Without any knowledge about a, the simplest proposal would be uniform, no matter what variance it induces. The more qi correlates with ai, the less variance \u02c6S2 has. The oracle qi is proportional to ai, under which \u02c6S2 becomes exact \u02c6S2 \u2261 S as q\u03b4i = a\u03b4i/(aK+1 + ... + aN) for all i. So, the strategy is to exploit our knowledge about a to construct a correlated proposal q. Given this estimator, we can also adjust the computation budget in order to reduce variance. When the distribution is long-tailed, we may increase K1 as an instance of Rao-Blackwellization (Liu et al., 2019). When the distribution is not long-tailed (enough), and top K1 summation underestimates signi\ufb01cantly, we may increase K2 to reduce variance, provided we have a fairly accurate q, as an instance of importance sampling. This procedure is also discussed in Kool et al. (2020) for 2 Under review as a conference paper at ICLR 2021 A. Sampled Sum of a sorted list B. Sampled Forward recursion C. Model Architecture Long-tail case \u02dcat\u22121 \u02dcat BERT x0 x1 x2 x3 r0 r1 r2 r3 z0 z1 z2 z3 Decoder x0 x1 x2 x3 TopK summand Sampled summand Gap to oracle  Dropped summand TopK state Sampled state Representation Space Dense case Encoder Latent structure in representation space RDP Decoder Backprop. 1 2 3 Log Z Paraphrasing 4 D. Experiment Protocol  Figure 1: (A): Sampled summation of an array; in the dense case the proposal is important for variance reduction, while in the long-tailed case, topK summands are important; (B): core recursion step of the Randomized Forward algorithm. We get topK and sample from the proposal (black and grey bars); Errors stem from the difference (green bars) between the oracle proposal \u02dca and constructed proposal \u02dcq; (C): Inferring latent states within the BERT representation space. We parametrize the CRF factors with vector products; the relations between states and contextualized embeddings together form a latent network (Fig. 3 and 4); (D): Experimental protocol; we \ufb01rst study the basic properties of RDP (steps 1, 2) and then integrate RDP into a LVM for inferring the structure of the representation space (steps 3, 4). Best viewed in color. gradient estimation. In fact, it is the underlying basis of many Monte Carlo estimators in various settings (Mohamed et al., 2020). The Sampled Forward Algorithm Now we will show how estimator \u02c6S2 can be used to scale summation in DP. Consider a linear chain CRF which de\ufb01nes a discrete state sequence z = [z1, ..., zT ], zt \u2208 {1, ..., N} over an input sentence x = [x1, ..., xT ]. Later we will use this CRF to construct an inference model to discover latent network structures within contextualized representations. We are interested in the partition function Z which is commonly computed with the Forward algorithm, a dynamic programming algorithm that sums over the potentials of all possible state sequences. The core recursion steps are: \u03b1t+1(i) = N \ufffd j=1 \u02dcat+1(i, j) = N \ufffd j=1 \u03b1t(j)\u03a6(j, i)\u03c6(xt+1, i) Z = N \ufffd j=1 \u03b1T (j) (3) where \u03b1t(i) is the sum of all possible sequences up to step t and at state i, \u03a6(\u00b7, \u00b7) is an N \u00d7 N transition matrix, and \u03c6(xt, i) is the emission potential that models how word xt generates state i. We assume all potentials are positive for simplicity. When implemented on GPUs, space complexity is O(TN 2) (see number of edges in the DP graph in Figure 1B) and it is the squared term N 2 that causes memory over\ufb02ows under automatic differentiation (see Appendix B for engineering details). Our key insight is to recursively use the memory-ef\ufb01cient randomization of Eq. 2 to estimate Eq. 3 at every step. Given a proposal \u02dcqt for each step t that correlates with summands \u02dcat (we discuss how to construct \u02dcqt in the next section), we obtain its top K1 index and sample K2 from the rest: [\u03c3t,1, ..., \u03c3t,K1, ..., \u03c3t,N] = arg sorti{\u02dcqt(i)}N i=1 (4) [\u03b4t,1, ..., \u03b4t,K2] \u223c Categorical{\u02dcqt(\u03c3t,K1+1), ..., \u02dcqt(\u03c3t,N)} (5) where \u02dcqt(\u00b7) are normalized to construct the categorical. Compared to Eq. 3, the key recursion of our Sampled Forward uses the top K1 index \u03c3t and sampled K2 index \u03b4t to substitute the full index: \u02c6\u03b1t+1(i) = K1 \ufffd j=1 \u02c6\u03b1t(\u03c3t,j)\u03a6(\u03c3t,j, i)\u03c6(xt+1, i) + 1 K2 K2 \ufffd j=1 \u02dcZt \u02dcqt(\u03b4t,j) \u02c6\u03b1t(\u03b4t,j)\u03a6(\u03b4t,j, i)\u03c6(xt+1, i) (6) \u02dcZt = N \ufffd j=K1+1 \u02dcqt(\u03c3t,j) \u02c6Z = K1 \ufffd j=1 \u02c6\u03b1T (\u03c3T,j) + 1 K2 K2 \ufffd j=1 \u02dcZT \u02dcqT (\u03b4T,j) \u02c6\u03b1T (\u03b4T,j) (7) where the oracle proposal q\u2217 t is proportional to the actual summand \u02dcat (Eq. 3, a little bit algebra will show this is actually the backward sampling probability p(zt = i|zt+1 = j)) , which is only accessible with the full Forward. So, we use the proposal weight \u02dcqt (Eq. 4) to move the computation outside the DP. In Fig. 1B, the top K1 summed terms correspond to black nodes. The proposal \u02dcqt 3 Under review as a conference paper at ICLR 2021 corresponds to black and grey bars, and its distance from the oracle proposal \u02dcat (which is the major source of variance) is highlighted in green. Sampled indices are shown as blue nodes. Essentially, our Sampled Forward algorithm restricts the DP computation from the full graph to subgraphs with top and sampled edges, reducing complexity to O(TK2) where K = K1 + K2. By varying K, memory complexity becomes a tradeoff between memory budget and estimation error. By induction, we can show that \u02c6Z (Eq. 7) is an unbiased estimator of Z since \u2200t, E[\u02c6\u03b1t] = \u03b1t. When implemented in log space, the expected log \u02c6Z is a lower bound of the exact log Z due to Jensen\u2019s inequality, and the variance is (trivially) reduced by log(\u00b7). See Appendix for details on implementation (Section C), theoretical analysis (Section A), and extensions to general sum-product structures (Section D). 3 LATENT NETWORK TOPOLOGY IN PRETRAINED LANGUAGE MODELS Latent States within Representation Space We now use the above technique to uncover hidden geometric structures in contextualized representations. In experiments we work with BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019), however, our method can be easily applied to other pretrained language models. Given sentence x = [x1, ..., xT ], we denote its contextualized representations as [r1, ..., rT ] = PLM(x). Representations r for all sentences lie in one manifold M, namely, the representation space of the language model. We hypothesize there exists a set of latent states s1, ..., sM that function as anchors and outline the space topology. We emphasize that all parameters of the PLM are \ufb01xed (i.e., no \ufb01ne-tuning takes place), so all learned states are intrinsic to M. We focus on two topological relations: (a) state-word relations, which represent how word embeddings may be summarized by their states and how states can be explained by their corresponding words; and (b) state-state relations, which capture how states interact with each other and how their transitions denote meaningful word combinations. Taken together, these two relations form a latent network within M (visualized in Fig. 3 and 4). We adopt a minimal parametrization of the inference network so as to respect the intrinsic structure of the representation manifold without imposing strong assumptions (e.g., via regularization). Speci\ufb01cally, for state-word relations, we associate each word embedding rt with a latent state indexed by zt \u2208 {1, ..., N} (the corresponding embedding of zt is szt). For state-state relations, we assume a transition weight \u03a6(i, j). Together we have a linear-chain CRF: log \u03c6(xt, zt) = r\u22ba t szt log \u03a6(zt\u22121, zt) = s\u22ba zt\u22121szt (8) where the dot product follows the common practice of \ufb01ne-tuning contextualized representations. We use log space for numerical stability. The probability of a state sequence given a sentence is: q\u03c8(z|x) = T \ufffd t=1 \u03a6(zt\u22121, zt)\u03c6(xt, zt)/Z (9) Here, the only learnable parameters are state embeddings: \u03c8 = [s1, ..., sN] as we try to be faithful to the representation manifold. Note how this parametrization reconciles space geometry with graphical models. As N is large, we estimate Z with the proposed Sampled Forward (Eq. 7). Constructing the Proposal We now return to proposal \u02dcqt (Eq. 4) which we construct based on a common observation that linguistic phenomena are long-tailed: \u02dcqt(i) \u221d \u03a6(i)\u03c6(xt, i) \u03a6(i) = ||si||1 (10) where \u03c6(xt, i) states that only a few states are likely to generate observation xt, which is often the case in NLP (e.g., there are only a few possible POS tags for each word); and \u03a6(i) models the prior probability of state i. This choice stems from the empirical observation that larger L1 norm correlates with larger dot product, and is thus more likely to be inferred. Essentially, our proposal combines local emissions \u03c6 and global prior \u03a6 to approximate the \u02dcat variables (Eq. 3) and bypass their expensive computation. Inference and Learning We use amortized variational inference to learn s. We simply reuse the architecture from previous work Fu et al. (2020); Li & Rush (2020) and build a generative model: p\u03b8(x, z) = \ufffd t p(xt|z1:t, x1:t\u22121) \u00b7 p(zt|z1:t\u22121, x1:t\u22121) ht = Dec([szt\u22121; xt\u22121], ht\u22121) (11) p(xt|z1:t, x1:t\u22121) = softmax(FF(ht)) p(zt|z1:t\u22121, x1:t\u22121) = softmax(FF([szt; ht])) (12) 4 ", "Related Work": "RELATED WORK Ef\ufb01cient Inference for Structured Latent Variables There has been substantial interest recently in the application of deep latent variable models (LVMs) to various language related tasks (Wiseman et al., 2018; Li & Rush, 2020), which has also exposed scalability limitations. Earlier attempts to render CRF models ef\ufb01cient (Sokolovska et al., 2010; Lavergne et al., 2010) either make many stringent assumptions (e.g., sparsity), rely on handcrafted heuristics for bias correction (Jeong et al., 2009), or cannot be easily adapted to modern GPUs with tensorization and parallelization. Sun et al. (2019) are closest to our work, however they only consider top K summation and consistently underestimate the partition. Chiu & Rush (2020) scale HMMs but assume words are clustered beforehand. Our approach systematically trades computation with proposal accuracy and estimation error (rather than over-compromising for ef\ufb01ciency). Moreover, we do not impose any hard restrictions like sparsity (Correia et al., 2020), and can accommodate dense and long-tailed distributions. Our method is inspired by randomized automatic differentiation (RAD, Oktay et al., 2020), and can be viewed as RAD applied to the DP computation graph. Advantageously, our proposal is compatible with existing ef\ufb01cient implementations (like Rush, 2020) since it does not change the computation graph. Interpretability of Contextualized Representations There has been a good deal of interest recently in analyzing contextualized representations and the information they encode. This line of research, collectively known as \u201cBertology\u201d (Rogers et al., 2020; Hewitt & Manning, 2019), focuses mainly on supervised probing of linguistic properties (Tenney et al., 2019), while the geometric properties of the representations have been less studied (Cai et al., 2021). A major dilemma facing this work is whether supervised linguistic probes reveal properties intrinsic to the embeddings or imposed by the supervision signal itself (Hewitt & Liang, 2019; Hall Maudslay et al., 2020; Chen et al., 2021). In this work, we do not use any supervision to ensure that the discovered network is intrinsic to the representation space. 5 ", "Experiments": "EXPERIMENTS In this section, we present our experimental results aimed at analyzing RDP and showcasing its practical utility (see Fig. 1). Speci\ufb01cally, we (1) verify the basic properties of RDP by estimating the partition function and (2) using it to train the structured latent variable model introduced in Section 3; (3) we then turn our attention to pretrained language models and examine the network induced with our approach and whether it is meaningful; and (4) we generate sentence paraphrases by traversing this network. For experiments (1, 2, 4), we use (a). pretrained GPT2 as the encoder since they are more about autoregressive language modeling and generation; (b). the MSCOCO dataset, a common benchmark for paraphrasing (Fu et al., 2019). For experiment (2), we use (a). BERT since it has been the main focus of most previous analytical work (Rogers et al., 2020); (b). the 20News dataset, a popular benchmark for training latent variable models (Grisel et al.). Across all experiments, we 5 Under review as a conference paper at ICLR 2021 Dense Distribution. Entropy = 34.66 Intermediate. Entropy = 18.08 Long-tail Distribution. Entropy = 6.49 A B C Figure 2: Sampled Forward vs. TopK summation (Sun et al., 2019) in different unit cases during training. Red line: target log partition. Grey line: estimates from TopK. Our method effectively corrects the bias in TopK summation with signi\ufb01cantly less memory, and is consistent with dense and long-tailed distributions. Model-#States Dev NLL Dev PPL Test NLL Test PPL FULL-100 (Fu et al., 2020) 39.64\u00b10.06 22.07\u00b10.12 39.71\u00b10.07 22.32\u00b10.12 TOPK-100 (Sun et al., 2019) 39.72\u00b10.13 22.22\u00b10.23 39.76\u00b10.11 22.41\u00b10.20 RDP-100 (ours) 39.59\u00b10.10 21.99\u00b10.18 39.59\u00b10.08 22.12\u00b10.13 TOPK-2K (Sun et al., 2019) 39.81\u00b10.30 22.43\u00b10.44 39.84\u00b10.31 22.52\u00b10.59 RDP-2K (ours) 39.47\u00b10.11 21.94\u00b10.46 39.48\u00b10.14 21.93\u00b10.24 Table 1: Results on training LVMs on MSCOCO dataset. Models are run 6 times with different random seeds. use an LSTM decoder with states identical to the encoder (762 for BERT base and GPT2 as in Wolf et al., 2020). More details on experiments and model settings can be found in Appendix E. 5.1 BASIC PROPERTIES We examine the estimation of the partition function for three unit cases, namely dense, intermediate, and long-tailed distributions. Instead of simulating these unit cases, to make our experiments more realistic, we extract CRFs on-the-\ufb02y from different LVM training stages. We also study the effects of memory budget by setting K to 20, 200, and 400 (corresponding to 1, 10, and 20 percent of the full memory). We use TopK summation (Sun et al., 2019) as our main baseline. This method can be viewed as setting K1 = K and K2 = 0 in our framework, i.e., it does not use the random sample. For training LVMs, We consider 100 and 2,000 latent states. With 100 states we are able to perform the summation exhaustively which is the same as Fu et al. (2020). Full summation with 2,000 states is intractable, so we only compare with TopK summation and use K = 100. Estimating the Partition Function As shown in Figure 2, TopK summation always underestimates the partition. The gap is quite large in the dense case (large entropy), which happens at the initial stages of training when the model is not con\ufb01dent enough. The long-tailed case represents later training epochs when the model has converged and is more concentrated. Our method effectively corrects the bias, and works well in all unit cases with signi\ufb01cantly less memory. Training Latent Variable Models We compare different LVMs in Table 1. Following common practice, we report negative log likelihood (NLL) and perplexity (PPL). We perform an extensive search over multiple hyperparameters (e.g., \u03b2, learning rate, word dropout) across multiple random seeds (3\u20136) and report the average performance of the best con\ufb01guration for each method. Our model performs best in both 100 and 2,000 state settings. The advantage is modest (as there are no architecture changes, only different training methods) but consistent. RDP trades off exploitation (i.e., increasing K1) and exploration (i.e., increasing K2) while TopK summation always focuses on the local solutions by passing gradients through top states. Intuitively, we have the chance of discovering better latent states (i.e., larger likelihood) by randomly searching the unexplored space. 5.2 DISCOVERING LATENT NETWORKS FROM PRETRAINED EMBEDDINGS We now discuss how latent structures induced with RDP reveal linguistic properties of contextualized representations. We focus on BERT Devlin et al. (2019) and set the number of latent states to 6 Under review as a conference paper at ICLR 2021 State id Interpretation Corresponding Words - Occurrence Morphology / Syntax Semantics 1634 Month april 169 | may 75 | apr 53 | march 40 | august 33 | june 28 | version 26 | february 22 | september 18 Religion 865 faith 383 | religion 377 | atheist 205 | islam 159 | religious 145 | morality 137 |  christianity 87 | muslim 40 Country 214 germany 31 | turkish 26 | qur 26 | american 25 | greek 21 | turkey 20 | muslim 19 | london 17 | islam 16 1291 Literature lines 1848 | read 701 | writes 502 | line 376 | book 319 | books 244 | write 203 | written 177 | text 171 1874 Computer apple 405 | chip 386 | disk 373 | fbi 289 | encryption 197 | ##eg 171 | hardware 166 | nsa 154 1214 Aerospace space 182 | nasa 170 | orbit 169 | motif 136 | moon 103 | planet 95 | prism 94 | lunar 92 | venus 86 Medicine 371 drug 212  | food 145 | health 130 | medical 121 | disease 117 | diet 115 | cancer 113 | aids 98 | sex 83 Log frequency Word distribution colored by  number of corresponding states Word distribution colored by type  of corresponding states State distribution colored by type  of corresponding words 1417 [er]-su\ufb03x - comparative better 784 | less 530 | faster 149 | higher 126 | greater 120 | worse 105 | larger 93 | ##er 88 | longer 80 [er]-su\ufb03x - role 127 ##er 473 | everyone 390 | user 270 | host 180 | server 136 | manager 103 | player 93 | doctor 82 Past tense 476 ##ed 609 | ##d 437 | ##ted 156 | based 144 | caused 95 | ##ized 75 | made 61 | lost 61 | built 60 1556 Present continuous ##ing 1282 | running 188 | ##ting 149 | ##ng 104 | ##ling 92 | processing 87 | killing 83 | calling 70 [s]-su\ufb03x 3rd singular 665 ##s 35 | isn 11 | comes 7 | runs 6 | remains 6 | ##ly 5 | exists 5 | contains 3 | includes 3 | becomes 3 1972 [s]-su\ufb03x plural ##s 649 | turks 222 | armenians 206 | jews 186 | keys 171 | muslims 151 | arabs 123 | christians 93 [ly]-su\ufb03x adverb 243 ##ly 929 | probably 311 | clearly 254 | completely 231 | obviously 229 | certainly 222 | directly 186 \u201cGive\u201d 890 give 854 | given 445 | provide 224 | gives 217 | gave 193 | giving 162 | show 128 | o\ufb00er 89 | cause 88 see 1721 | look 858 | seen 618 | read 302 | saw 274 | display 205 | image 199 | looks 197 | looking 196 \u201cSee\u201d 1756 A1 A2 A3 B1 B2 C Example States Corresponding words Figure 3: (A1): Frequent words partake in more latent states than rare words (presumably because they are polysemous); (A2 and A3): The distribution of states is also Zip\ufb01an, as most frequent states generate most words (the orange portion in A2 is almost indistinguishable); (B): t-SNE (Van der Maaten & Hinton, 2008) visualization of latent network induced from BERT; (B1): Words and their corresponding latent states. For states, the size of circle indicates frequency (\u2248 aggregated posterior probability) and color thickness means level of contextualization; a state with deeper blue color tends to generate content words (whose meaning is less dependent on context); lighter blue corresponds to stopwords (which are more contextualized); words are also colored by number of states (\u2248 number of linguistic roles); red color densities mean a word is generated by several states; (B2) and (C): sample from p\u2217(x)q\u03c6(z|x). Our method discovers a spectrum of meaningful states which exhibit both morpholigical, syntactic and semantic functionalities. 2,000. As BERT\u2019s vocabulary size is 32K, one state would approximately handle 15 words in the uniform case, functioning as a type of \u201cmeta\u201d word. After convergence, we use q\u03c8 to sample z for each x in the training set (recall we use the 20News dataset). These z can be viewed as samples from the aggregated posterior \ufffd x q\u03c8(z|x)p\u22c6(x) where p\u22c6(x) denotes the empirical data distribution. To get a descriptive summary of BERT\u2019s latent topology, we compute the following statistics on z samples: state frequency (Fig. 3, A3); number words corresponding to each state (Fig. 3, A2); number of states corresponding to each word (Fig. 3, A1); and state bigrams (Fig. 4). We further differentiate stopwords (e.g., in, of, am, is) from content words. State-word Relations Figure 3 gives a \ufb01rst impression of how latent states spread over the representation space. Overall, we observe that the joint space is Zip\ufb01an, and this property characterizes 7 Under review as a conference paper at ICLR 2021 Joint Visualization of All States Top 500 States, Connected by Transition Matrix Top 500 States, Connected by Aggregated Posterior Transition Interpretation Corresponding Bigrams - Occurrence W/o. Stopwords W. Stopwords In prepositional phrase 904-296 in-fact 155 | in-reality 6 | in-particular 5 | in-short 5 | in-itself 4 | in-essence 4 | in-general 4 698-145 to-believe 30 | to-prove 10 | to-assume 7 | to-check 6 | to-test 5 | to-claim 4 | to-argue 4 To + verb, in\ufb01nitive 1712-698 want-to 126 | like-to 24 | wanted-to 18 | wants-to 17 | wish-to 10 | wishes-to 9 | designed-to 6 Verb + to Passive voice (is + v.ed) 665-476 is-de\ufb01ned 3 | is-supported 3 | is-produced 3 | is-created 2 | is-available 2 | is-caused 2 Passive voice: by 476-1654 written-by 13 | ##d-by 11 | caused-by 8 | ##ed-by 8 | produced-by 6 | followed-by 6 | de\ufb01ned-by 4 1895-1966 In somewhere in-bosnia 14 | in-soviet 9 | in-seattle 9 | in-texas 8 | in-washington 6 | in-los 6 | in-azerbaijan 5 Verb (past) + in 476-904 built-in 3 | washed-in 3 | represented-in 2 | ##led-in 2 | resulted-in 2 | involved-in 2 | sealed-in 2 476-243 reacted-badly 1 | organized-electronically 1 | implemented-slightly 1 | tied-directly 1 Verb (past) + adverb 243-476 intentionally-started 3 | possibly-followed 2 | basically-threw 2 | self-proclaimed 2 | heavily-armed 2 Adverb + verb (past) 192-1417 much-less 14 | much-better 14 | much-greater 5 | much-worse 4 | much-bigger 3 | lot-better 3 Adverb + comparative 1417-1683 greater-risk 2 | less-money 2 | less-costly 2 | less-expensive 2 | bigger-budgets 1 | lower-costs 1 Comparative + noun 1064-476 people-married 2 | revolutionaries-armed 1 | people-burned 1 | people-got 1 | people-showed 1 People did 1572-476 jordan-implemented 1 | taylor-visited 1 | bullock-received 1 | ryan-walked 1 | cooper-ripped 1 Person did Highlighted Transitions a young man riding on the skate board at top of a park a young man riding a skate board at the top of a park young man riding on top of skate board in a park Input: a young man riding a skate board on top of a park a young riding man 1755 0 117 0 1755 a 0 969 on the 103 skate board 0 1061 1061 1061 at top of 103 959 a park at the 1061 959 0 top 1755 in 0 top 1061 of A1 A2 A3 A4 B C Figure 4: (A1): Geometrical differences between top and tail states; most lexical variations are encoded by the top 500 states while remaining states represent the long tail; (A3 and A4): Network topology within top 500 states; in (A3) nodes are connected to their top 1 neighbor according to the transition matrix \u03a6 (as a proxy of the empirical prior) and in (A4) according to the most frequent bigram (as a proxy of the aggregated posterior), note how the two are correlated; (A2 and B): Highlighted bigrams and their linguistic interpretation; transitions with stopwords are more about syntax (e.g., to with in\ufb01nitives or transitive verbs); transitions without stopwords are more about speci\ufb01c meanings. (C): paraphrasing as latent network traversal. the distribution of words (A1), states (A3), and word occurrence within each state (C). We also see that the top 500 states account for most word occurrence (A2) while the remaining states model tail phenomena (A3). We conjecture this number is related to the intrinsic dimension of the data manifold (see Aghajanyan et al. 2021). The induced states encode multiple linguistic properties (Fig. 3, C). Some states are similar to a lexicon entry encoding speci\ufb01c words and their morphological variants; other states exhibit clustering based on morphological features (-s, -er, -ly suf\ufb01x). We believe this is closely related to the fact that BERT learns embeddings over subwords. Note that the past tense cluster contains words exhibiting both regular (-ed suf\ufb01x) and irregular morphology (e.g., lost and built). Finally, we also see that some states are largely semantic, similar to a conventional topic model (e.g., Computer and Medicine clusters). See Appendix E.6 for more state-word examples. State-State Relations As shown in Fig. 4, we observe a clear geometric difference between top and tail states. Most linguistic constructions seem to be captured by the top 500 states (A1). The connections of top states are visualized in (A2\u2013A4). From a statistical perspective, the similarity of (A3) and (A4) clearly shows how the empirical prior (encoded by the transition matrix \u03a6) matches 8 ", "Conclusion": "CONCLUSION In this paper, we have developed a general method for scaling the inference of structured latent variable models with randomized dynamic programming. It is a useful tool for the visualization and inspection of the intrinsic structure of contextualized representations. Experiments with BERT reveal the topological structure of its latent space: state-word connections encapsulate syntactic and semantic roles while state-state connections correspond to phrase constructions. Moreover, traversal over a sequence of states represents underlying sentence structure. 9 ", "References": "REFERENCES Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model \ufb01ne-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 7319\u20137328, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.568. URL https: //aclanthology.org/2021.acl-long.568. Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics, 9:277\u2013293, 2021. Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth Church. Isotropy in the contextual embedding space: Clusters and manifolds. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=xYGNO86OWDH. Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, and Liping Jing. Probing {bert} in hyperbolic spaces. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=17VnwXYZyhH. Justin Chiu and Alexander M Rush. Scaling hidden markov language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1341\u20131349, 2020. Gonc\u00b8alo Correia, Vlad Niculae, Wilker Aziz, and Andr\u00b4e Martins. Ef\ufb01cient marginalization of discrete and structured latent variables via sparsity. Advances in Neural Information Processing Systems, 33, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423. Yao Fu, Yansong Feng, and John P. Cunningham. Paraphrase generation with latent bag of words. In NeurIPS, 2019. Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, and Alexander M. Rush. Latent template induction with gumbel-crf. In NeurIPS, 2020. Yao Fu, Chuanqi Tan, Mosha Chen, Songfang Huang, and Fei Huang. Nested named entity recognition with partially-observed treecrfs. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 35, pp. 12839\u201312847, 2021. 10 Under review as a conference paper at ICLR 2021 Olivier Grisel, Lars Buitinck, and Chyi-Kwei Yau. Topic extraction with non-negative matrix factorization and latent dirichlet allocation. URL https://scikit-learn.org/stable/ auto_examples/applications/plot_topics_extraction_with_nmf_lda. html. Rowan Hall Maudslay, Josef Valvoda, Tiago Pimentel, Adina Williams, and Ryan Cotterell. A tale of a probe and a parser. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7389\u20137395, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.659. URL https://aclanthology.org/ 2020.acl-main.659. John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2733\u20132743, 2019. John Hewitt and Christopher D. Manning. A structural probe for \ufb01nding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4129\u20134138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419. Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(5), 2013. Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee. Ef\ufb01cient inference of crfs for large-scale natural language data. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pp. 281\u2013284, 2009. Yoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and G\u00b4abor Melis. Unsupervised recurrent neural network grammars. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1105\u20131117, 2019. Wouter Kool, Herke van Hoof, and Max Welling. Estimating gradients for discrete random variables by sampling without replacement. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rklEj2EFvB. Thomas Lavergne, Olivier Capp\u00b4e, and Franc\u00b8ois Yvon. Practical very large scale crfs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 504\u2013513, 2010. Xiang Lisa Li and Alexander Rush. Posterior control of blackbox generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2731\u20132743, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 243. URL https://aclanthology.org/2020.acl-main.243. Runjing Liu, Jeffrey Regier, Nilesh Tripuraneni, Michael Jordan, and Jon Mcauliffe. Raoblackwellized stochastic gradients for discrete distributions. In International Conference on Machine Learning, pp. 4023\u20134031. PMLR, 2019. Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, and Sen Song. Unsupervised paraphrasing by simulated annealing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302\u2013312, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.28. URL https://aclanthology.org/2020. acl-main.28. Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1064\u20131074, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1101. URL https://aclanthology.org/ P16-1101. 11 Under review as a conference paper at ICLR 2021 Emile Mathieu, Tom Rainforth, Nana Siddharth, and Yee Whye Teh. Disentangling disentanglement in variational autoencoders. In International Conference on Machine Learning, pp. 4402\u20134412. PMLR, 2019. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh: Constrained sentence generation by metropolis-hastings sampling. In AAAI, 2018. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. CGMH: Constrained sentence generation by metropolis-hastings sampling. Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, 33 (01):6834\u20136842, Jul. 2019. doi: 10.1609/aaai.v33i01.33016834. URL https://ojs.aaai. org/index.php/AAAI/article/view/4659. Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1\u201362, 2020. Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P Adams. Randomized automatic differentiation. In International Conference on Learning Representations, 2020. Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, and Jian Tang. Rnnlogic: Learning logic rules for reasoning on knowledge graphs. In International Conference on Learning Representations, 2020. Lawerence R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257\u2013286, 1989. doi: 10.1109/5.18626. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Unpublished manuscript, 2019. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert works. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2020. Alexander M Rush. Torch-struct: Deep structured prediction library. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 335\u2013342, 2020. Nataliya Sokolovska, T. Lavergne, O. Capp\u00b4e, and Franc\u00b8ois Yvon. Ef\ufb01cient learning of sparse conditional random \ufb01elds for supervised sequence labeling. IEEE Journal of Selected Topics in Signal Processing, 4:953\u2013964, 2010. Hong Sun and Ming Zhou. Joint learning of a dual smt system for paraphrase generation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 38\u201342, 2012. Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, and Zhihong Deng. Fast structured decoding for sequence models. Advances in Neural Information Processing Systems, 32:3016\u2013 3026, 2019. Charles Sutton and Andrew McCallum. An introduction to conditional random \ufb01elds for relational learning. Introduction to statistical relational learning, 2:93\u2013128, 2006. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=SJzSgnRcKX. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and variational inference. Now Publishers Inc, 2008. 12 Under review as a conference paper at ICLR 2021 Sam Wiseman, Stuart Shieber, and Alexander Rush. Learning neural templates for text generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3174\u20133187, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1356. URL https://aclanthology.org/D18-1356. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6. 13 Under review as a conference paper at ICLR 2021 Appendix, table of contents \u2022 Section A. Theoretical analysis of the Sampled Forward algorithm. Bias and variance. \u2022 Section B. Challenges of full DP inference under Automatic Differentiation. \u2022 Section C. Implementation details of Sampled Forward. \u2022 Section D. Extensions of randomized dynamic programming. \u2022 Section E. Experimental details. \u2022 Section F. Additional experimentsl results. A THEORETICAL ANALYSIS OF SAMPLED FORWARD ALGORITHM A.1 BIAS ANALYSIS In this section, we discuss the unbiasedness and variance of the Randomized Forward algorithm. We \ufb01rst show that Randomized Forward gives an unbiased estimator of the partition function. Theorem A.1 (Unbiasedness). For all t \u2208 [1, 2, ..., T], the sampled sum \u02c6\u03b1t (Eq. 6) is an unbiased estimator of the forward variable \u03b1t (Eq. 3). The \ufb01nal sampled sum \u02c6Z (Eq. 7) is an unbiased estimator of the partition function Z (Eq. 3). Proof. By the Second Principle of Mathematical Induction. Assume initialization \u03b11(i) = \u03c6(x1, i). Firstly at t = 2, for all i, we have: Eq1[\u02c6\u03b12(i)] = K1 \ufffd j=1 \u03b11(\u03c31,j)\u03a6(\u03c31,j, i)\u03c6(x2, i) + 1 K2 K2 \ufffd j=1 Eq1 \ufffd \u02dcZ1 \u02dcq1(\u03b41,j)\u03b11(\u03b41,j)\u03a6(\u03b41,j)\u03c6(x2, i) \ufffd \ufffd\ufffd \ufffd =A \ufffd (14) where the second term can be expanded as a masked summation with the index rearranged from \u03c32,K1+1 to \u03c32,N: A = K2 \ufffd j=1 Eq1 \ufffd \u02dcZ1 \u02dcq1(\u03b41,j)\u03b11(\u03b41,j)\u03a6(\u03b41,j)\u03c6(x2, i) \ufffd (15) = K2 \ufffd k=1 N \ufffd j=K1+1 Eq1 \ufffd 1 q1(\u03b41,k)1(\u03b41,k = j)\u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) \ufffd (16) = K2 \ufffd k=1 N \ufffd j=K1+1 Eq1 \ufffd 1 q1(\u03b41,k)1(\u03b41,k = j) \ufffd \ufffd \ufffd\ufffd \ufffd =1 \u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) (17) = K2 N \ufffd j=K1+1 \u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) (18) Notice how we re-index the sum. Now put it back to Eq. 14 to get: Eq1[\u02c6\u03b12(i)] = K1 \ufffd j=1 \u03b11(\u03c31,j)\u03a6(\u03c31,j, i)\u03c6(x2, i) + 1 K2 \u00b7 K2 N \ufffd j=K1+1 \u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) (19) = N \ufffd j=1 \u03b11(\u03c31,j)\u03a6(\u03c3,j, i)\u03c6(x2, i) (20) = N \ufffd j=1 \u03b11(j)\u03a6(j, i)\u03c6(x2, i) (21) = \u03b12(i) (22) 14 Under review as a conference paper at ICLR 2021 This veri\ufb01es the induction foundation that Eq1[\u02c6\u03b12(i)] = \u03b12(i) for all i. Now assume for time index t we have \u2200i, Eq1:t\u22121[\u02c6\u03b1t(i)] = \u03b12(i). Consider t + 1, we have: E1:qt[\u02c6\u03b1t+1(i)] = K1 \ufffd j=1 Eq1:t\u22121 \ufffd \u02c6\u03b1t(\u03c3t,j) \ufffd \u03a6(\u03c3t,j, i)\u03c6(xt+1, i) (23) + 1 K2 K2 \ufffd j=1 Eq1:t\u22121 \ufffd \u02c6\u03b1t(\u03b4t,j) \ufffd \u00b7 Eqt \ufffd \u02dcZt \u02dcqt(\u03b4t,j)\u03a6(\u03b4t,j)\u03c6(xt+1, i)] \ufffd (24) = K1 \ufffd j=1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j, i)\u03c6(xt+1, i) (25) + 1 K2 K2 \ufffd j=1 \u03b1t(\u03c3t,j) \u00b7 Eqt \ufffd \u02dcZt \u02dcqt(\u03b4t,j)\u03a6(\u03b4t,j)\u03c6(xt+1, i)] \ufffd \ufffd \ufffd\ufffd \ufffd =A\u2032 (26) Note how we decompose the expectation by using the independence: q1:t = q1:t\u22121\u00b7qt With a similar masked summation trick as A, we have: A\u2032 = K2 N \ufffd j=K1+1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j, i)\u03c6(xt+1, i) (27) This gives us: E1:qt[\u02c6\u03b1t+1(i)] = K1 \ufffd j=1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j, i)\u03c6(xt+1, i) + 1 K2 \u00b7 K2 N \ufffd j=K1+1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j, i)\u03c6(xt+1, i) (28) = N \ufffd j=1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j, i)\u03c6(xt+1, i) (29) = N \ufffd j=1 \u03b1t(j)\u03a6(j, i)\u03c6(xt+1, i) (30) = \u03b1t+1(i) (31) Thus showing \u02c6\u03b1t is an unbiased estimator for \u03b1t at each step t. Setting t = T, the last step, gives us E[ \u02c6Z] = Z (details similar to the above). Corollary A.1.1. When we change the (sum, product) semiring to the (log-sum-exp, sum) semiring, the expectation of the estimator will become a lower bound of log \u03b1t and log Z. Proof. Denote lt(i) = log \u03b1t(i) and \u2126 the set of sampled indices where |\u2126| = K2. For simplicity, we omit the top K1 summation and only show the summation of the sample. Cases where t > 2 can be derived similarly as following: \u02c6l2(i) = log \ufffd j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i)) \u2212 log K2 (32) E[\u02c6l2(i)] = E \ufffd log \ufffd j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i)) \ufffd \u2212 log K2 (33) \u2264 log E \ufffd \ufffd j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i)) \ufffd \u2212 log K2 (34) = log K2 \u2212 log K2 + log \ufffd j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i)) (35) = l2(i) (36) 15 Under review as a conference paper at ICLR 2021 where Eq. 34 comes from Jensen\u2019s inequality. Then by induction one can show at everystep, we have E[\u02c6lt(i)] \u2264 lt(i). Although implementation in the log space makes the estimate biased, it reduces the variance exponentially in a rather trivial way. It also provides numerical stability. So, in practice we use it for training. A.2 VARIANCE ANALYSIS Now we analyze variance. We start with the estimator a\u03b4/q\u03b4, \u03b4 \u223c Categorical{qK1+1, ...qKN } in Eq. 2. Firstly, this is an unbiased estimator of the tail sum: E[a\u03b4/q\u03b4] = N \ufffd i=K1+1 ai (37) We have the folling variance arguments: Theorem A.2 (Variance of the tail estimator). V[a\u03b4/q\u03b4] = N \ufffd i=K1+1 a2 i /qi \u2212 ( N \ufffd i=K1+1 ai)2 (38) = S2 K1( N \ufffd i=K1+1 \u03f52 i /qi + 2\u03f5i) (39) where SK1 = N \ufffd i=K1+1 ai \u03f5i = ai/SK1 \u2212 qi (40) which says the variance is: \u2022 quadratic to the tail sum SK1, which will can be reduced by increasing K1 as the effect of Rao-Blackwellization. \u2022 approximately quadratic to the gap \u03f5i, as the differences between the proposal qi and the oracle ai/SK1, which can be reduced by choosing a correlated proposal as the effect of importance sampling. Optimal zero variance is achieved on q\u2217 i = ai/SK1 (41) Proof. The variance is then: V[a\u03b4/q\u03b4] = E[(a\u03b4/q\u03b4)2] \u2212 E[a\u03b4/q\u03b4]2 (42) where the \ufb01rst term is the second moment: (a\u03b4/q\u03b4)2 = ( N \ufffd i=K1+1 ai1[\u03b4 = i]/qi)2 (43) = N \ufffd i=K1+1 a2 i 1[\u03b4 = i]/q2 i + 2 N \ufffd i=K1+1 N \ufffd j=K1+1 aiaj1[\u03b4 = i]1[\u03b4 = j] qiqj \ufffd \ufffd\ufffd \ufffd =0 (44) = N \ufffd i=K1+1 a2 i 1[\u03b4 = i]/q2 i (45) 16 Under review as a conference paper at ICLR 2021 taking the expection of it we get: E[(a\u03b4/q\u03b4)2] = E[ N \ufffd i=K1+1 a2 i 1[\u03b4 = i]/q2 i ] (46) = N \ufffd i=K1+1 a2 i /qi (47) Plug this back, variance is: V[a\u03b4/q\u03b4] = N \ufffd i=K1+1 a2 i /qi \u2212 ( N \ufffd i=K1+1 ai)2 (48) To get the optimal proposal, we solve the constrained optimization problem: min qi N \ufffd i=K1+1 a2 i /qi \u2212 ( N \ufffd i=K1+1 ai)2 (49) s.t. N \ufffd i=K1+1 qi = 1 (50) By solving the corresponding Lagrangian equation system (omitted here), we get the optimal value achieved at: q\u2217 i = ai \ufffdN i=K1+1 ai N \ufffd i=K1+1 a2 i /q\u2217 i \u2212 ( N \ufffd i=K1+1 ai)2 = 0 (51) This says zero variance is achieved by a proposal equal to the normalized summands. Then de\ufb01ne the gap between the proposal and the normalized summands as: \u03f5i = ai SK1 \u2212 qi (52) Plug this to the variance expression we get: V[a\u03b4/q\u03b4] = S2 K1( N \ufffd i=K1+1 \u03f52 i /qi + 2\u03f5i) (53) Corollary A.2.1. When increasing the sample size to K2, the variance will reduce to V[ 1 K2 N \ufffd j=1 a\u03b4j q\u03b4j ] = 1 K2 S2 K1( N \ufffd i=K1+1 \u03f52 i /qi + 2\u03f5i) (54) Now we consider the variance of the Sampled Forward algorithm. An exact computation would give complicated results. For simpli\ufb01cation, we give an asymptotic argument with regard to RaoBlackwellization and importance sampling: Theorem A.3 (Single Step Asymptotic Variance of Sampled Forward). At each step, the alpha varible estimator has the following asymptotic variance: V[\u02c6\u03b1t(i)] = O \ufffd 1 K2 \u03b12 t,K1(i) \u00b7 \u03f52 t(i) \ufffd (55) where: \u2022 \u03b1t+1,K1(i) = \ufffdN j=K1+1 \u02dc\u03b1t(j, i) = \ufffdN j=K1+1 \ufffd Eq1:t\u22121[\u02c6\u03b12 t (j)]\u03a6(j, i)\u03c6(xt, i) is a tail sum after the top K1 summands. This term will reduce if we increase K1, as an instance of RaoBlackwellization. 17 Under review as a conference paper at ICLR 2021 \u2022 \u03f52 t(i) = \ufffdN j=K1+1 \u03f52 t\u22121(j, i)/qt\u22121(j) and \u03f5t\u22121(j, i) is the difference between the proposal qt\u22121(j) and the oracle proposal in Eq. 3. This term will reduce if the proposal is more correlated to the oracle, as an instance of Importance Sampling. Proof. We start with a simple setting where K2 = 1. At step t + 1 we have the following variance recursion: Eq1:t[\u02c6\u03b12 t+1(i)] = N \ufffd j=K1+1 \u03a62(j, i)\u03c62(xt+1, i) qt(j) \u00b7 Eq1:t\u22121[\u02c6\u03b12 t (j)] (56) This is derived by plugging estimator 6 to the variance Eq. 38 we have just derived. Denote: \u03b1t+1,K1(i) = N \ufffd j=K1+1 \u02dc\u03b1t(j, i) = N \ufffd j=K1+1 \ufffd Eq1:t\u22121[\u02c6\u03b12 t (j)]\u03a6(j, i)\u03c6(xt+1, i) (57) Then we have Vq1:t[\u02c6\u03b1t+1(i)] = \u03b12 t+1,K1(i) \ufffd N \ufffd j=K1+1 \u03f52 t(j, i) qt(i) + 2\u03f5t(j, i) \ufffd (58) where \u03f5t(j, i) is the differences between the proposal and the normalized exact summands at step t state i: \u03f5t(j, i) = \u02dc\u03b1t(j, i) \ufffdN j=K1=1 \u02dc\u03b1t(j, i) \u2212 qt(j) (59) Dropping out the \ufb01rst order errors and increasing the number of sample to K2, we have the asymptotics: V[\u02c6\u03b1t(i)] = O \ufffd 1 K2 \u03b12 t,K1(i) \u00b7 \u03f52 t(i) \ufffd (60) Theorem A.4 (Asymptotic Variance of Sampled Forward Partition Estimation). The alpha variable estimators has the following asymptotic variance recurrsion: V[\u02c6\u03b1t+1(i)] = O( 1 K2 \u00b7 \u03c62 t,K1 \u00b7 \u03f52 t,K1 \u00b7 V[\u02c6\u03b1t]) (61) Compared with Eq. 55, this expression: \u2022 Uses the product of the factors \u03c6t,K1 (a function of the sum-prod of the factor at step t) and the previous step variance V[\u02c6\u03b1t] to substitute the \u03b1t,K1 in equation 55. Again, this term will decrease with a larger K1 (Rao-Blackwellization). \u2022 \u03f52 t,K1(i) = \ufffdN j=K1+1 \u03f52 t\u22121(j, i)/qt\u22121(j) and \u03f5t\u22121(j, i) is the difference between the proposal qt\u22121(j) and the oracle proposal in Eq. 3. This term will reduce if the proposal is more correlated to the oracle, as an instance of Importance Sampling (same as Eq. 55). Consequently, the partition function has the following asymptotic variance: V[ \u02c6Z] = O( T \ufffd t=1 1 K2 \u00b7 \u03c62 t,K1 \u00b7 \u03f52 t,K1) (62) When implemented in the log space, the variance is trivially reduced exponentially: V[log \u02c6Z] = O( T \ufffd t=1 log 1 K2 + 2 log \u03c6t,K1 + 2 log \u03f5t,K1) (63) 18 Under review as a conference paper at ICLR 2021 Proof. Firstly for simplcity we assume K1 = 0 and K2 = 1. The the estimator variance is: V[\u02c6\u03b1t+1(i)] = Eq1:t[\u02c6\u03b12 t+1(i)] \u2212 \u03b12 t+1(i) (64) = Eq1:t\u22121 \ufffd N \ufffd j=1 \u02c6\u03b12 t (j)\u03a62(j, i)\u03c62(xt, i) qt(j) \u00b7 \u03b12 t (j) \u03b12 t (j) \ufffd \u2212 \u03b12 t+1(i) (65) Recall: \u03b1t+1(i) = N \ufffd j=1 \u03b1t(j)\u03c6(j, i)\u03c6(xt, i) (66) Let: \u03f5t(j, i) = \u03b1t(j)\u03c6(j, i)\u03c6(xt, i) \u03b1t+1(i) \ufffd \ufffd\ufffd \ufffd =q(zt=j|zt+1=i) \u2212qt(j) (67) Then: V[\u02c6\u03b1t+1(i)] = Eq1:t\u22121 \ufffd N \ufffd j=1 \u03b12 t+1(i) \ufffd\u03f52 t(j, i) qt(j) + 2\u03f5t(j, i) + qt(j) \ufffd \u02c6\u03b12 t (j) \u03b12 t (j) \ufffd \u2212 \u03b12 t+1(i) (68) = \u03b12 t+1(i) \ufffd N \ufffd j=1 \ufffdEq1:t\u22121[\u02c6\u03b12 t (j)] \u03b12 t (j) qt(j) \ufffd + N \ufffd j=1 \ufffd\u03f52 t(j, i) qt(j) + 2\u03f5t(j, i) \ufffdEq1:t\u22121[\u02c6\u03b12 t (j)] \u03b12 t (j) \ufffd (69) \u2212 \u03b12 t+1(i) (70) Note that there exist Jt such that: Eq1:t\u22121[\u02c6\u03b12 t (j)] \u03b12 t (j) <= Jt (71) This is because of the bounded gap of the Jensen\u2019s inequality. Also recall: N \ufffd j=1 qt(j) = 1 (72) So we get: V[\u02c6\u03b1t+1(i)] <= \u03b12 t+1(i) \ufffd Jt \u2212 1 + N \ufffd j=1 \ufffd\u03f52 t(j, i) qt(j) + 2\u03f5t(j, i) \ufffd \u00b7 \ufffdV[\u02c6\u03b1t(j)] \u03b12 t (j) \u2212 1 \ufffd\ufffd (73) = \u03b12 t+1(i) \ufffd Jt \u2212 1 \u2212 N \ufffd j=1 \ufffd\u03f52 t(j, i) qt(j) + 2\u03f5t(j, i) \ufffd\ufffd (74) + \u03b12 t+1(i) \ufffd N \ufffd j=1 \ufffd\u03f52 t(j, i) qt(j) + 2\u03f5t(j, i) \ufffd \u00b7 \ufffdV[\u02c6\u03b1t(j)] \u03b12 t (j) \ufffd\ufffd (75) empirically Jt is not the dominate source of variance (but could be in the worst case, depending on the tightness of Jensen\u2019s inequality). We focus on the second term: V[\u02c6\u03b1t+1(i)] = O \ufffd \u03b12 t+1(i) \ufffd N \ufffd j=1 \ufffd\u03f52 t(j, i) qt(j) + 2\u03f5t(j, i) \ufffd \u00b7 \ufffdV[\u02c6\u03b1t(j)] \u03b12 t (j) \ufffd\ufffd (76) = O \ufffd\ufffd N \ufffd j=1 \u03b1t(j) \u03a6(j, i)\u03c6(xt, j) \ufffd \ufffd\ufffd \ufffd O(\u03c62 t ) \ufffd2 \u00b7 \ufffd N \ufffd j=1 \ufffd\u03f52 t(j, i) qt(j) + 2\u03f5t(j, i) \ufffd \ufffd \ufffd\ufffd \ufffd O(\u03f52 t ) \u00b7 1 \u03b12 t (j) \u00b7 V[\u02c6\u03b1t(j)] \ufffd \ufffd\ufffd \ufffd O(V[\u03b1t]) \ufffd\ufffd (77) = O(\u03c62 t \u00b7 \u03f52 t \u00b7 V[\u03b1t]) (78) 19 Under review as a conference paper at ICLR 2021 Note that a lot of higher-order sum-products are simpli\ufb01ed here. Adding top K1 summation and increasing the sample size to K2 leads to variance reduction as: V[\u02c6\u03b1t+1(i)] = O( 1 K2 \u00b7 \u03c62 t,K1 \u00b7 \u03f52 t,K1 \u00b7 V[\u03b1t]) (79) Recursively expand this equation we get: V[ \u02c6Z] = O( T \ufffd t=1 1 K2 \u00b7 \u03c62 t,K1 \u00b7 \u03f52 t,K1) (80) Chaning the implementation to the log space we reduce the variance exponentially: V[log \u02c6Z] = O( T \ufffd t=1 log 1 K2 + 2 log \u03c6t,K1 + 2 log \u03f5t,K1) (81) B CHALLENGES OF FULL DP UNDER AUTOMATIC DIFFERENTIATION Now we analyze in detail why direct full summation causes memory over\ufb02ow. Firstly, we note that in a bachi\ufb01ed stochastic gradient descent setting, memory complexity for the forward algorithm is O(BTN 2) (82) Where B is the batch size, T is the maximum sequence length after padding, and N is the number of states. Consider a typical setting where B = 100, T = 100, N = 1000, then the complexity is: C = c \u00d7 100 \u00d7 100 \u00d7 1000 \u00d7 1000 = c \u00d7 1010 (83) where c is a constant depending on speci\ufb01c hardware and libraries. At \ufb01rst sight this may seem reasonable with a large GPU memory (e.g., 16G). Also, one can come up with improvements, such as not storing the intermediate \u03b1t variables. If we only need one single forward pass, such engineering tricks are indeed effective. In our experiments, when setting B = 50, T = 15, and N = 2000, the actual memory consumption for the forward computation, together with other model components, is about 4G when implemented with PyTorch 1.8.0. However, this is not the case when working under an automatic differentiation (AD) setting. If we want to compute the gradients of the partition function (e.g., to optimize the likelihood), we inevitably need to store all the intermediate steps to keep the full computation graph. Then, the adjacent graph, in principle, has the same complexity as the forward graph. But in practice, it will be more complicated due to the actual implementation of AD engines. Following the above example where B = 50, T = 15, N = 2000, we get a memory over\ufb02ow on a 16G memory GPU when calling the PyTorch backward function. This situation also immediately invalidates many engineering tricks (e.g., we cannot drop the intermediate \u03b1t variables anymore), and substantially increases the dif\ufb01culty of coming up with new ones, since we would also be working with the internal mechanism of automatic differentiation engines. Note that the internal mechanism of these engines is complicated and opaque to general practitioners (e.g., they may change the underlying computation graph for better speed), and many of these engines (like PyTorch) are not optimized for dynamic programming. In fact, it would be unwise to overwrite the backward computation, even if the AD engine allowed us to do so, as it would signi\ufb01cantly increase engineering dif\ufb01culty, as we not only need to overwrite the \ufb01rst order gradients (like the gradients for the likelihood), but the second order gradients too (e.g., gradients for marginals or reparameterized samples). In fact, a brute force implementation would not only forego all the advantages of AD engines (like operator-level optimization), but would also require separate implementations for every graph (e.g., chains, trees, semi-Markovs, and IsingPotts), every inference algorithm (e.g., partition, marginal, sampling, reparameterization, entropy), and higher order gradients. In summary, this is an extremely dif\ufb01cult path that requires clever tricks to ef\ufb01ciently improve a large table of already complicated DP algorithms. This is why we do not interfere with the AD framework and work on more general and ef\ufb01cient algorithms, rather than tailored implementation tricks. With the randomized DP, we do not re-implement 20 Under review as a conference paper at ICLR 2021 anything. We only choose the index by the pre-computed proposal, and introduce the re-indexed potentials to existing ef\ufb01cient libraries (like Torch-struct in Rush, 2020). This is least dif\ufb01cult from an implementation perspective and best exploits the power of ef\ufb01cient structured prediction libraries and AD engines. C IMPLEMENTATION OF SAMPLED FORWARD ALGORITHM Now we discuss how to implement the sample forward in detail. Our implementation aims to set all computation outside the actual DP, and reuse existing optimized DP libraries. Speci\ufb01cally, we: 1. normalize the log potentials; 2. construct the proposal according to the local potential and a global prior; 3. obtain the top K1 index and sample K2 from the rest, retrieve the potentials according to these indices; 4. correct the bias before the DP, then insert the corrected potentials to an existing DP implementation. Now we explain each step in detail. Potential Normalization is a technique useful for numerical stability (see Fu et al., 2021). Specifically, in practice, the log-sum-exp function will saturate (it returns the max input value) when the maximum difference between inputs is larger than 10. So, we normalize all log potentials to be within range [1, m], m < 10. Given the emission potential \u03c6 and transition potential \u03a6 from Eq. 8, we normalize as: \u02dc\u03c6(xt, i) = m \u2217 log \u03c6(xt, i) \u2212 mini log \u03c6(xt, i) maxi log \u03c6(xt, i) \u2212 mini log \u03c6(xt, i) (84) \u02dc\u03a6(i, j) = m \u2217 log \u03a6(i, j) \u2212 mini,j log \u03a6(i, j) maxi,j log \u03a6(i, j) \u2212 mini log \u03a6(i, j) (85) Then the actual CRF is constructed based on the normalized potentials. Proposal Construction Our proposal is constructed based on the normalized local emission and a prior distribution. Speci\ufb01cally: qt(i) = 1 2 \u00b7 ( exp \u02dc\u03c6(xt, i) \ufffdN i=1 exp \u02dc\u03c6(xt, i) + exp ||si||1 \ufffdN i=1 exp ||si||1 ) (86) Index Retrieval For each step t, we retrieve the top K1 index and get a K2 sized sample from the rest, as is discussed in the main paper: [\u03c3t,1, ..., \u03c3t,K1, ..., \u03c3t,N] = arg sorti{qt(i)}N i=1 (87) [\u03b4t,1, ..., \u03b4t,K2] \u223c Categorical{qt(\u03c3t,K1 + 1), ..., qt(\u03c3t,N)} (88) Note that for simplicity, we use sampling with replacement. For sampling without replacement, see the procedure in Kool et al. 2020 Conventional Forward Before calling an existing implementation of Forward, we need to correct the bias of sampled terms. We do this by multiplying the probability of the sample to its emission potential (which becomes addition in log space). Note that this will be equivalent to multiplying them inside the DP: \u02dc\u03c6\u2032(\u03c3t,i) = \u02dc\u03c6(\u03c3t,i) (89) \u02dc\u03c6\u2032(\u03b4t,i) = \u02dc\u03c6(\u03b4t, i) + log qt(\u03b4t,i) (90) We treat any duplicate indices as if they are different, and view the resulting potentials as if they are a new CRF. We also retrieve the transition potentials at each step according to the chosen index. So the new CRF has different transitions across steps. Finally, we run an existing ef\ufb01cient implementation of the Forward algorithm on the new CRF to get the estimated \u02c6\u03b1 and \u02c6Z. 21 Under review as a conference paper at ICLR 2021 D EXTENSION OF RDP In this section, we extend the RDP to more graph structures and more inference operations. We \ufb01srt discuss a randomized inside algorithm for the partition function estimation for tree-structured hypergraphs, which includes dependency trees and PCFGs. Then we discuss a randomized entropy DP estimation on chain-structured graphs. This estimation will call the randomized forward as its subroutine. Finally we discuss howto generalize RDP to the general sum-product algorithm for any graph structure. D.1 EXTENSIONS TO TREE-STRUCTURED HYPERGRAPHS: RANDOMIZED INSIDE This section discusses how to extend RDP to Tree-structured hypergraphs, including Dependency TreeCRFs and PCFGs. We focus on randomizing the Inside algorithm for partition function estimation. Speci\ufb01cally, the core recursion of the inside is: \u03b2[i, j, k] = sijk j\u22121 \ufffd l=i \ufffd k1\u2208\u2126,k2\u2208\u2126 \u03b2[i, l, k1]\u03b2[l, j, k2] (91) where \u03b2[i, j, k] is the summation of all subtrees spanning from location i to location j with label k and sjik is the local score and \u2126 is the full state space. Suppose \u2126 is large so we want to sample its subset \u2126ij to reduce the summation computation. Suppose the proposal is qijk where \ufffd k\u2208\u2126 qijk = 1. Then a randomized inside recursion with K2 sample is (we omit the top K1 summation for simplicity): \u03b2[i, j, k] = sijk j\u22121 \ufffd l=i \ufffd \u03b41\u2208\u2126il,\u03b42\u2208\u2126lj 1 K2qil\u03b41 \u03b2[i, l, \u03b41] \u00b7 1 K2qlj\u03b42 \u03b2[l, j, \u03b42] (92) The analysis about bias and variance is similar to the previous analysis (Sec. A) on the linear-chain case. D.2 EXTENSIONS TO RANDOMIZED ENTROPY DP ESTIMATION This section describes a randomized DP for estimating the entropy of a chain-structured model (HMMs and linear-chain CRFs). Recall that the core recursion of conventional entropy DP is: p(zt = i|zt+1 = j) = \u03a6(i, j)\u03c6(xt+1, j)\u03b1t(i) \u03b1t+1(j) (93) Ht+1(j) = N \ufffd i=1 p(zt = i|zt+1 = j)[Ht(i) \u2212 log p(zt = i|zt+1 = j)] (94) where Ht(i) is the intermediate conditional entropy end at state i step t. In our sampled DP, we \ufb01rst call the sampled forward to estimate the alpha variables. Then we re-use the sampled indices for the entropy DP graph, and the core recursion becomes: \u02c6p(zt = i|zt+1 = j) = \u03a6(i, j)\u03c6(xt+1, j)\u02c6\u03b1t(i) \u02c6\u03b1t+1(j) (95) Ht+1(j) = K2 \ufffd \u03b4t=1 1 K2 \u00b7 qt(\u03b4t) \u02c6p(zt = \u03b4t|zt+1 = j)[Ht(i) \u2212 log \u02c6p(zt = \u03b4t|zt+1 = j)] (96) where qt is the proposal at step t. Note how we re-use the estimated alpha variables \u02c6\u03b1 for our entropy DP and correct the bias of each step. D.3 EXTENSIONS TO GENERAL SUM-PRODUCT This section discusses the extension of sampled DP to general graph structures and message-passing algorithms, following the Bethe variational principle (Wainwright & Jordan, 2008). Recall the general message-passing algorithm computes pseudo marginals by recursively updating the message at 22 Under review as a conference paper at ICLR 2021 each edge: Mts(xs) \u2190 \u03ba \ufffd x\u2032 t \ufffd \u03c6st(xs, x\u2032 t)\u03c6t(x\u2032 t) \ufffd u\u2208N(t)/s Mut(x\u2032 t) \ufffd (97) where Mts(xs) denotes the message from node t to node s evaluated at Xs = xs, \u03c6st is the edge potential, \u03c6t is the node potential, N(t)/s denotes the set of neighbor nodes of t except s, and \u03ba the normalization constant. At convergence, we have the pseudo marginals \u00b5s(xs) = \u03ba\u03c6s(xs) \ufffd t\u2208N(s) M \u2217 ts(xs) (98) and the Bethe approximation of the log partition function is given by the evaluation of the Bethe variational problem: log ZBethe = \u03c6\u22ba\u00b5 + \ufffd s Hs(\u00b5s) \u2212 \ufffd s,t Ist(\u00b5st) (99) where H denotes marginal entropy and I denotes marginal mutual information. We consider the application of randomization to the computation of pseudo marginals and the Bethe log partition. Note that the Bethe approximation will be exact if the underlying graph is a tree. We consider a local proposal \u02dcq combined with a global prior \u03c4: q(xs) = 1 2(\u02dcq(xs) + \u03c4(xs)) (100) where the prior \u03c4 depends on our knowledge of speci\ufb01c problem structures. As one can always retreat to uniform proposals, we can explore more advanced choices. For \u02dcq, one may consider: (a) local normalization as \u02dc(q)(xs) = \u03c6(xs)/ \ufffd s \u03c6(xs) and (b) pre-computed mean-\ufb01eld approximations from algorithms like SVI (Hoffman et al., 2013). For \u03c4, one can further use a frequency-based empirical prior estimated from the data. To determine which nodes to perform DP on, one may consider the following principles: \u2022 Special nodes of interest depending on their actual meaning. For example, one may be interested in some xs = 0 (e.g., if a user is under a bank fraud threat). \u2022 Nodes with large local weight \u03c6(xs). \u2022 Nodes with large global weight \u03c4(xs). \u2022 Loop elimination. If two nodes have similarly small local and global weight, we could drop those which eliminate loops in the resulting graph. Also note that we would prefer removing small-weighted nodes for loop elimination. With the above principles, one may construct three subsets to perform the sum-product: \u2022 \u21261 including nodes of special interest, where we perform exact computation. \u2022 \u21262 from the top items of the proposal, where we also perform exact computation. \u2022 \u21263 by sampling from the remaining items of the proposal (optionally with loop-reduction). For this set of nodes, one needs to correct the estimation by dividing the proposal probabilty: \u03c6(xs) \u2190 \u03c6(xs)/q(xs). After these steps, we treat nodes in \u21261 \ufffd \u21262 \ufffd \u21263 as if they are a new model, then feed them to an existing sum-product implementation. E EXPERIMENT DETAILS E.1 MODEL ARCHITECTURE DETAILS For training LVMs, partition function estimation, and paraphrasing, we use the Huggingface checkpoint of GPT2 model1 since these experimental variables are more about autoregressive language 1https://huggingface.co/transformers/model_doc/gpt2.html 23 Under review as a conference paper at ICLR 2021 modeling and generation. For analyzing latent network topologies, we use Huggingface checkpoint of BERT base model2 since it has been the main focus of most previous analytical work. The decoder LSTM is a one-layer LSTM with hidden state size 762, the same as the size of the contextualized embeddings. It shares the embeddings of the inferred states with the encoder, and uses its own input word embeddings, whose size is also 762. This architecture suf\ufb01ces for training LVMs and inferring latent networks. For paraphrase generation, we change the decoder to be conditional by: (a) using the average word embeddings of content words of the input sentence as its initial hidden state (rather than zero hidden states); (b) letting it attend to the embeddings of content words of the input sentence, and copy from them. This effectively makes this decoder conditional on the BOW of the content words of the input sentence. Then decoding a paraphrase becomes a neural version of slot \ufb01lling: a sequence of latent states by traversing the network becomes a template of the sentence which we \ufb01ll with words. E.2 DATASET PREPROCESSING For the MSCOCO dataset, we use the GPT2 tokenizer to process sentences. As the of\ufb01cial website 3 does not release a test split (only a training and a development split), we use the of\ufb01cial development split as our test split, and re-split the of\ufb01cial training split to be a new training and a development split. For training LVMs, we only use 1/10 of the training set for quicker experiments (the full dataset takes over more than 20 hours to converge). For paraphrasing, we use the full dataset. For the 20News dataset, we use the data from the sklearn website4. We follow its of\ufb01cial split and process the sentences with the BERT tokenizer. E.3 HYPERPARAMETERS AND TRAINING STRATEGIES To get meaningful convergence of the latent space without posterior collapse, the following techniques are important: (a). set \u03b2 in the correct range. A large \u03b2 force the posterior to collapse to a uniform prior, while a small \u03b2 encourages the posterior to collapse to a Dirac distribution. (b). use word dropout in initial training epochs, otherwise the decoder may ignore the latent code. (c). use potential normalization, otherwise the logsumexp function used in the forward algorithm may saturate and only return the max of the input, this would consequently lead the posterior to collapse to few \u201cactivated\u201d states and other states will never be inferred. We further note that a full DP based entropy calculation will cause memory over\ufb02ow with automatic differentiation. So we approximate it with local emission entropy, i.e., the sum of the entropy of the normalized local emission factors. Compared with the full entropy, this approximation does not directly in\ufb02uence the transition matrix, but encourages more activated states and mitigates the posterior collapse problem. To get a full entropy regularization, one can further regularize the transition matrix towards an all-one matrix, or extend our randomized DP to entropy computation. As the current local entropy regularization is already effective for inducing a meaningful posterior, we leave the full entropy computation to future work. Our reported numbers are averaged over \u201cgood enough\u201d runs. Speci\ufb01cally, for all hyperparameter con\ufb01gurations, we \ufb01rst run three random seeds and get the mean and standard deviation of the performance metrics. If the standard deviation is small enough, we report mean performance metrics (NLL, PPL, and iBLEU) and the standard deviation. If the standard deviation is large, we run extra \ufb01ve seeds. Then we drop runs with bad performance (usually 2-3), and compute the mean and standard deviation of the performance metrics again. In total, we experiment more than 100 runs over more than 7 hyperparameters: (a). learning rate 10\u22123, 10\u22124, 5 \u00d7 10\u22124; (b). optimizer: SGD, Adam, AdamW; (c). dropout: 0.2, 0.3, 0.4; (d). \u03b2 variable: 10\u22124, 5 \u00d7 10\u22124, 10\u22123, 10\u22122; (e). Gumbel CRF v.s. Gumbel CRF straight-through (f). K1 v.s. K2; (g). word dropout schedule; and their combinations. Note that different models may achieve best performance with different hyperparameter combinations. We make sure that all models are searched over the same set of 2https://huggingface.co/transformers/model_doc/bert.html 3https://cocodataset.org/#home 4https://scikit-learn.org/stable/modules/generated/sklearn.datasets. fetch_20newsgroups.html 24 Under review as a conference paper at ICLR 2021 Table 3: Ef\ufb01ciency comparison with 100/ 500/ 2K states (Forward pass only). #States 100 500 2000 Method Mem Time Mem Time Mem Time FULL (Fu et al., 2020) 3.9G 0.24s 7.9G 1.07s TOPK (Sun et al., 2019) 4.1G* 0.23s 4.1G 0.22s 5.2G 0.27s RDP (ours) 4.1G* 0.24s 4.1G 0.22s 5.2G 0.27s hyperparameter combinations, and report average performance metrics over multiple seeds of the best hyperparameter con\ufb01guration for each model. E.4 TIME COMPLEXITY ANALYSIS Table 3 shows the actual ef\ufb01ciency comparison of our method. Time is measured by seconds per batch. For experiments with 500 and 2,000 states, we set K = 100. *When the number of states is small, our method underperforms due to the overhead of constructing the proposal. However, its advantage becomes clear as the number of states becomes large. E.5 VISUALIZATION PROCEDURE This section describes how we produce Fig. 3(B1-2) and Fig. 4(A1-4). We use the sklearn implementation 5 of tsne (Van der Maaten & Hinton, 2008). For Fig. 3(B1-2), the word embeddings are obtained by sampling 8,000 contextualized embeddings from the full word occurrences in the 20News training set. Then we put the sampled word embeddings and the 2,000 states into the tsne function. The perplexity is set to be 30. An important operation we use, for better seperating the states from each other, it to manually set the distances between states to be large, otherwise the states would be concentrated in a sub-region, rather than spread over words. Fig. 4 is produced similarly, except we do not use word embeddings as background, and change the perplexity to be 5. For Fig. 4(A3), we connect the states if their transition potential is larger than a threshold. For Fig. 4(A4), we connect the states if their bigram frequency is larger than a threshold. All our decisions of hyperparameters are for the purpose of clear visualization which includes reducing overlapping, overcrowding, and other issues. We further note that no single visualization method can reveal the full structure of high-dimensional data, and any projection to 2-D plain inevitably induces information loss. We leave the investigation of better visualization methods to future work. E.6 STATE-WORD PAIR EXAMPLES See Tables 8 and 9 for more sample. E.7 STATE-STATE TRANSITION EXAMPLES See Table 10 for transitions involving stopwords and Table 11 for transitions without stopwords. Also note their differences as transitions involving stopwords are more about syntactic constructions and transitions without stopwords are more about speci\ufb01c meaning. E.8 STATE TRAVERSAL EXAMPLES See Table 12 for more sample. 25 Under review as a conference paper at ICLR 2021 Table 4: Randomized Forward v.s. TopK Forward. Comparison of MSE. Method Dense Intermediate Long-tail TOPK 20% MEM (Sun et al., 2019) 3.8749 1.0159 0.1629 TOPK 50% MEM (Sun et al., 2019) 0.99 0.2511 0.0315 RDP 1% MEM (ours) 0.1461 0.0669 0.0766 RDP 10% MEM (ours) 0.0672 0.0333 0.0552 RDP 20% MEM (ours) 0.0469 0.0200 0.0264 RDP 50% MEM (ours) 0.0078 0.0041 0.0046 Table 5: Randomized Forward, comparison of different proposal. Same base distribution as Figure 2, all proposals use 10% memeory). Method Dense Intermediate Long-tail UNIFORM (baseline) 0.6558 11.0500 0.1600 LOCAL ONLY (ours) 2.908 9.1711 0.4817 GLOBAL ONLY (ours) 0.4531 0.0961 0.1003 LOCAL + GLOBAL (ours) 0.0284 0.0173 0.0222 F ADDITIONAL EXPERIMENT RESULTS F.1 RDP V.S. TOPK MSE COMPARISON Table 4 shows the mean square error (MSE) comparison between RDP and TopK on dense, intermediate and long-tail distributions. Again, MSE for RDP is signi\ufb01cantly smaller with less memory consumption. F.2 MSE COMPARISON OVER PROPOSALS Table 5 shows the mean square error (MSE) comparison between different proposals. Our proposed local and global proposal outperforms a baseline uniform proposal on all distributions (dense, intermediate and long-tail). F.3 CONTROLLING NUMBER OF STATES Figure 5 shows state frequency with different N. The long-tail behavior becomes clearer only when N is large enough (larger than 500 in our case). F.4 CONTROLLING K1 V.S. K2 RATIO Figure 6 show state frequency with different K1/K2 ratios at different N. We highlight that when K2 = 0, a pure topK summation approach would lead to posterior collapse where there exist inactive states that do not have any density. We also notice that an increasing K2 consistently increases the frequency of tail states. This observation can be explained by the exploitation-exploration tradeoff, where increasing K1 would encourage the exploitation of states already con\ufb01dent enough during training (consequently leading to high-frequency head states after convergence) while increasing K2 would encourage exploring states that are not con\ufb01dent enough during training, leading to a larger tail frequency after convergence. F.5 COMPARISON TO RANDOMLY INITIALIZED BERT Figure 7 shows the comparison of reconstruction NLL (\u2212 log p(xt|zt, \u00b7)) between a randomly initialized BERT and a pretrained BERT. States induced from pretrained BERT are more meaningful than random, making it easier to reconstruct words based on their corresponding states. 5https://scikit-learn.org/stable/modules/generated/sklearn.manifold. TSNE.html 26 Under review as a conference paper at ICLR 2021 A. 50 States B. 500 States C. 1000 States D. 10000 States Figure 5: State frequency with different N (number of states). When N = 50, the long-tail behavior is not visible. The long-tail behavior emerges only when N is large enough (larger than 500 in our case). Table 6: Randomized Inside algorithm for Tree-structured Hypergraphs. Comparison of MSE. Method Dense Intermediate Long-tail TOPK INSIDE 20% MEM 36.1275 27.4351 21.7788 TOPK INSIDE 50% MEM 2.8422 2.4043 2.0479 RANDOMIZED INSIDE 1% MEM (ours) 26.3312 37.6698 48.8638 RANDOMIZED INSIDE 10% MEM (ours) 1.1937 1.5307 1.3843 RANDOMIZED INSIDE 20% MEM (ours) 0.4455 0.5449 0.5997 F.6 RANDOMIZED INSIDE ALGORITHM FOR TREE-STRUCTURED HYPERGRAPH We simulate three unit cases of treecrfs (dense, intermediate and long-tail) by controlling the entropy. We set the number of latent states to 2000. We use a uniform proposal for comparison, and set K1 (top K sum size) = K2 = 50. We use RDP with 20, 200, 400 states, which correspond to 1%, 10%, and 20% of the full number of states. We use topK summation as our baseline with 500 and 1000 states (25% and 50% of the full number of states). Figure 8 shows performance of RDP to tree structured hypergraph. Our method still outperforms topK summation (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). Table 6 shows the MSE of our method compared to topK summation, and the results are consistent with Figure 8. F.7 RANDOMIZED ENTROPY DP ALGORITHM Figure 9 shows the application of RDP to entropy estimation of linear-chain CRFs. Our method consistently outperforms topK summation (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). Table 7 shows the MSE of our method compared to topK summation, and the results are consistent with Figure 9. 27 ", "Method": "", "Results": "RESULTS F.1 RDP V.S. TOPK MSE COMPARISON Table 4 shows the mean square error (MSE) comparison between RDP and TopK on dense, intermediate and long-tail distributions. Again, MSE for RDP is signi\ufb01cantly smaller with less memory consumption. F.2 MSE COMPARISON OVER PROPOSALS Table 5 shows the mean square error (MSE) comparison between different proposals. Our proposed local and global proposal outperforms a baseline uniform proposal on all distributions (dense, intermediate and long-tail). F.3 CONTROLLING NUMBER OF STATES Figure 5 shows state frequency with different N. The long-tail behavior becomes clearer only when N is large enough (larger than 500 in our case). F.4 CONTROLLING K1 V.S. K2 RATIO Figure 6 show state frequency with different K1/K2 ratios at different N. We highlight that when K2 = 0, a pure topK summation approach would lead to posterior collapse where there exist inactive states that do not have any density. We also notice that an increasing K2 consistently increases the frequency of tail states. This observation can be explained by the exploitation-exploration tradeoff, where increasing K1 would encourage the exploitation of states already con\ufb01dent enough during training (consequently leading to high-frequency head states after convergence) while increasing K2 would encourage exploring states that are not con\ufb01dent enough during training, leading to a larger tail frequency after convergence. F.5 COMPARISON TO RANDOMLY INITIALIZED BERT Figure 7 shows the comparison of reconstruction NLL (\u2212 log p(xt|zt, \u00b7)) between a randomly initialized BERT and a pretrained BERT. States induced from pretrained BERT are more meaningful than random, making it easier to reconstruct words based on their corresponding states. 5https://scikit-learn.org/stable/modules/generated/sklearn.manifold. TSNE.html 26 Under review as a conference paper at ICLR 2021 A. 50 States B. 500 States C. 1000 States D. 10000 States Figure 5: State frequency with different N (number of states). When N = 50, the long-tail behavior is not visible. The long-tail behavior emerges only when N is large enough (larger than 500 in our case). Table 6: Randomized Inside algorithm for Tree-structured Hypergraphs. Comparison of MSE. Method Dense Intermediate Long-tail TOPK INSIDE 20% MEM 36.1275 27.4351 21.7788 TOPK INSIDE 50% MEM 2.8422 2.4043 2.0479 RANDOMIZED INSIDE 1% MEM (ours) 26.3312 37.6698 48.8638 RANDOMIZED INSIDE 10% MEM (ours) 1.1937 1.5307 1.3843 RANDOMIZED INSIDE 20% MEM (ours) 0.4455 0.5449 0.5997 F.6 RANDOMIZED INSIDE ALGORITHM FOR TREE-STRUCTURED HYPERGRAPH We simulate three unit cases of treecrfs (dense, intermediate and long-tail) by controlling the entropy. We set the number of latent states to 2000. We use a uniform proposal for comparison, and set K1 (top K sum size) = K2 = 50. We use RDP with 20, 200, 400 states, which correspond to 1%, 10%, and 20% of the full number of states. We use topK summation as our baseline with 500 and 1000 states (25% and 50% of the full number of states). Figure 8 shows performance of RDP to tree structured hypergraph. Our method still outperforms topK summation (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). Table 6 shows the MSE of our method compared to topK summation, and the results are consistent with Figure 8. F.7 RANDOMIZED ENTROPY DP ALGORITHM Figure 9 shows the application of RDP to entropy estimation of linear-chain CRFs. Our method consistently outperforms topK summation (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). Table 7 shows the MSE of our method compared to topK summation, and the results are consistent with Figure 9. 27 Under review as a conference paper at ICLR 2021 A. N = 500, K1 = 100, K2 = 0 B. N = 500, K1 = 99, K2 = 1 C. N = 2000, K1 = 100, K2 = 0 D. N = 2000, K1 = 99, K2 = 1 E. N = 2000, K1 = 50, K2 = 50 F. N = 2000, K1 = 10, K2 = 90 Figure 6: State frequency by controlling K1 (sum size) and K2 (sample size). We highlight that when K2 = 0, a pure topK summation approach would lead to posterior collapse where there exist inactive states that do not have any density. We also notice that an increasing K2 consistently increases the frequency of tail states. Table 7: Randomized entropy DP v.s. TopK entropy DP on linear-chain CRFs. Comparison of MSE. Method Dense Intermediate Long-tail ENTROPY TOPK 20% MEM 443.7 84.35 8.0115 ENTROPY TOPK 50% MEM 131.8 22.1 1.8162 ENTROPY RDP 1% MEM (ours) 5.9256 1.9895 0.6914 ENTROPY RDP 10% MEM (ours) 2.1168 1.2989 0.3167 ENTROPY RDP 20% MEM (ours) 1.3267 0.7305 0.2071 ENTROPY RDP 50% MEM (ours) 0.3017 0.1461 0.0631 28 Under review as a conference paper at ICLR 2021 Figure 7: Comparison of reconstruction likelihood (log p(xt|zt, \u00b7)) between a randomly initialized BERT and a pretrained BERT. States induced from pretrained BERT are more meaningful than random, making it easier to reconstruct the words based on their corresponding states. A. Dense Tree Distribution B. Tree Distribution, Intermediate C. Long-tail Tree Distribution Figure 8: Application of RDP to tree structured hypergraph. Our method consistently outperforms topK summantion (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). A. Dense distribution, entropy = 34.66 B. Intermediate, entropy = 18.08 C. Long-tail distribution, entropy = 6.49 Figure 9: Application of RDP to entropy estimation of linear-chain CRFs. Our method consistently outperforms topK summantion (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true entropy). 29 Under review as a conference paper at ICLR 2021 State id Word - Occurrence 1724 ##s 5092 ; ##es 239 ; ##os 153 ; ##is 140 ; ##as 76 ; ##z 70 ; ##rs 54 ; ##gs 48 ; states 46 ; ##t 46 ; ##p 46 ; ##ps 43 ; ##ms 40 ; ##ns 35 ; ##e 34 ; ##r 31 ; ##i 29 ; ##ss 27 476 ##ed 609 ; ##d 437 ; ##ted 156 ; based 144 ; caused 95 ; ##ized 75 ; made 61 ; lost 61 ; built 60 ; ##able 58 ; supported 57 ; expressed 56 ; occupied 54 ; de\ufb01ned 54 ; created 54 254 david 521 ; john 404 ; mike 249 ; steve 245 ; dave 234 ; michael 223 ; robert 214 ; jim 204 ; mark 194 ; bob 173 ; paul 143 ; james 142 ; bill 133 ; tom 127 ; andrew 122 ; peter 121 710 problem 599 ; killed 278 ; death 243 ; kill 235 ; problems 211 ; error 187 ; crime 181 ; murder 176 ; sin 175 ; genocide 172 ; evil 140 ; bad 129 ; hate 112 ; pain 110 ; massacre 97 1972 ##s 649 ; turks 222 ; armenians 206 ; jews 186 ; keys 171 ; muslims 151 ; arabs 123 ; ##ms 122 ; christians 93 ; ##es 73 ; countries 71 ; villages 63 ; children 61 ; colors 61 ; guns 61 403 god 1917 ; ##a 751 ; world 730 ; ca 347 ; country 292 ; usa 289 ; ##o 284 ; group 280 ; ##u 277 ; uk 244 ; groups 133 ; ##ga 126 ; ##g 125 ; europe 124 ; heaven 120 ; hell 119 1494 problem 257 ; problems 87 ; agree 67 ; discussion 41 ; issue 39 ; argument 34 ; deal 34 ; issues 31 ; disagree 20 ; con\ufb02ict 16 ; arguments 16 ; case 15 ; relationship 14 ; agreement 14 1419 real 512 ; general 231 ; major 203 ; speci\ufb01c 157 ; main 156 ; actual 133 ; important 116 ; full 109 ; total 106 ; serious 99 ; absolute 88 ; basic 87 ; great 84 ; personal 83 ; true 76 305 ##s 1336 ; opinions 340 ; problems 154 ; rules 140 ; views 113 ; things 104 ; laws 99 ; events 97 ; actions 94 ; issues 94 ; values 93 ; arguments 90 ; cases 82 ; rates 78 ; comments 78 555 subject 2349 ; key 596 ; name 551 ; number 472 ; address 300 ; size 242 ; numbers 167 ; value 149 ; color 141 ; important 134 ; speed 122 ; names 110 ; level 106 ; count 84 1556 ##ing 1282 ; running 188 ; ##ting 149 ; ##ng 104 ; ##ling 92 ; processing 87 ; killing 83 ; ##ding 71 ; calling 70 ; ##ring 69 ; getting 66 ; reading 61 ; ##izing 61 ; ##ping 60 472 re 3084 ; com 2685 ; e 1034 ; ##e 892 ; internet 271 ; ##o 185 ; ##te 140 ; de 117 ; ##re 101 ; ##com 98 ; ##ne 71 ; ##r 71 ; ##er 70 ; org 70 ; ##me 70 ; net 68 ; se 62 1488 human 344 ; turkish 255 ; political 246 ; moral 217 ; religious 186 ; legal 161 ; jewish 159 ; israeli 143 ; sexual 128 ; social 121 ; physical 119 ; personal 114 ; civil 104 ; international 103 1410 simple 292 ; standard 213 ; normal 210 ; reasonable 208 ; \ufb01ne 192 ; nice 191 ; interesting 138 ; good 128 ; correct 127 ; perfect 121 ; stupid 117 ; ##able 106 ; proper 105 ; true 91 243 ##ly 929 ; probably 311 ; clearly 254 ; completely 231 ; obviously 229 ; certainly 222 ; directly 186 ; easily 179 ; apparently 176 ; generally 162 ; simply 154 ; necessarily 153 ; unfortunately 143 1418 never 919 ; little 48 ; cannot 46 ; none 37 ; nobody 35 ; ever 31 ; nothing 27 ; without 25 ; neither 19 ; didn 14 ; ve 11 ; hardly 9 ; always 8 ; zero 8 ; m 7 ; non 6 ; doesn 6 1246 ##t 1242 ; ##p 853 ; net 568 ; phone 395 ; call 381 ; bit 348 ; ##net 323 ; ##l 225 ; bat 135 ; line 123 ; ##g 111 ; ##d 101 ; network 94 ; ##it 93 ; tel 87 ; telephone 83 ; ##m 79 935 long 682 ; hard 600 ; big 570 ; bad 518 ; good 483 ; great 404 ; fast 216 ; large 185 ; ##y 160 ; short 158 ; quick 154 ; little 150 ; strong 145 ; high 114 ; nice 113 ; hot 107 659 cs 474 ; jp 468 ; cc 315 ; ms 291 ; cd 217 ; cm 194 ; dc 181 ; ##eg 158 ; bt 142 ; ps 125 ; ds 123 ; mc 120 ; pc 110 ; nc 110 ; gt 107 ; gs 107 ; rs 103 ; ss 103 ; bc 83 ; cb 81 1214 space 182 ; nasa 170 ; orbit 169 ; motif 136 ; moon 103 ; planet 95 ; prism 94 ; lunar 92 ; venus 86 ; saturn 85 ; spacecraft 85 ; earth 80 ; shuttle 76 ; satellite 73 ; mars 70 654 around 351 ; within 234 ; behind 137 ; across 102 ; regarding 81 ; among 73 ; throughout 70 ; beyond 57 ; near 53 ; along 53 ; inside 47 ; outside 44 ; past 40 ; concerning 38 ; following 37 127 ##er 473 ; everyone 390 ; ##r 319 ; user 270 ; anyone 259 ; host 180 ; friend 161 ; one 150 ; server 136 ; everybody 109 ; fan 104 ; manager 103 ; nobody 101 ; player 93 ; guy 84 145 believe 798 ; hope 421 ; evidence 379 ; claim 308 ; test 219 ; assume 201 ; proof 148 ; argument 145 ; prove 133 ; check 127 ; claims 106 ; suspect 104 ; doubt 95 ; guess 93 ; assuming 85 Table 8: More state-word examples 30 Under review as a conference paper at ICLR 2021 State id Word - Occurrence 1572 president 378 ; clinton 293 ; ##resh 268 ; myers 165 ; attorney 84 ; general 79 ; morris 78 ; smith 76 ; paul 75 ; bush 74 ; manager 64 ; hitler 56 ; ##ey 52 ; ##i 48 ; ##man 45 964 cut 132 ; plug 121 ; break 73 ; thread 73 ; cable 63 ; hole 59 ; holes 54 ; chip 49 ; \ufb01x 48 ; clutch 48 ; stick 46 ; connector 42 ; blow 42 ; box 41 ; screw 40 ; pin 40 ; hit 40 1756 see 1721 ; look 858 ; seen 618 ; read 302 ; saw 274 ; display 205 ; image 199 ; looks 197 ; looking 196 ; looked 188 ; screen 177 ; watch 161 ; view 153 ; monitor 149 ; images 132 585 day 779 ; sun 686 ; ##n 556 ; today 310 ; night 276 ; week 269 ; days 264 ; city 161 ; morning 145 ; sunday 125 ; ##en 117 ; year 105 ; ##net 96 ; n 92 ; ##on 89 ; weeks 87 ; month 73 66 power 433 ; control 399 ; state 347 ; virginia 202 ; mode 182 ; process 162 ; effect 139 ; period 118 ; action 117 ; authority 91 ; function 87 ; position 84 ; ##v 82 ; force 78 1240 \ufb01rst 1443 ; always 602 ; ever 559 ; never 361 ; ago 321 ; often 319 ; sometimes 284 ; usually 203 ; early 195 ; last 192 ; every 175 ; later 171 ; soon 155 ; recently 146 ; past 145 865 faith 383 ; religion 377 ; believe 211 ; atheist 205 ; ##ism 164 ; islam 159 ; ##sm 158 ; religious 145 ; morality 137 ; belief 126 ; font 115 ; language 114 ; truth 92 ; logic 90 467 ##u 1203 ; point 784 ; happen 234 ; place 201 ; happened 183 ; colorado 141 ; happens 139 ; points 132 ; wait 126 ; ground 94 ; site 94 ; center 86 ; position 78 ; situation 78 ; 1993 76 203 ca 273 ; pub 177 ; ##u 143 ; dod 143 ; au 141 ; mit 138 ; ma 132 ; ##si 129 ; sera 121 ; des 113 ; \ufb01 75 ; isa 70 ; il 58 ; ny 58 ; po 56 ; la 53 ; tar 48 ; lee 47 ; ti 47 371 drug 212 ; drugs 177 ; food 145 ; health 130 ; medical 121 ; disease 117 ; diet 115 ; cancer 113 ; aids 98 ; homosexuality 96 ; sex 83 ; homosexual 82 ; medicine 82 ; hiv 78 ; treatment 77 1683 money 479 ; cost 307 ; pay 274 ; issue 212 ; problem 186 ; matter 175 ; worth 175 ; care 153 ; costs 146 ; tax 108 ; expensive 102 ; responsible 96 ; risk 96 ; spend 95 ; insurance 94 1856 whole 379 ; entire 196 ; full 58 ; every 49 ; everything 42 ; together 37 ; everyone 25 ; ##u 17 ; rest 14 ; ##up 13 ; ##ed 13 ; away 10 ; always 10 ; top 10 ; open 10 ; ##s 9 1584 university 1064 ; government 886 ; law 769 ; science 482 ; ##u 412 ; research 312 ; history 290 ; laws 165 ; study 125 ; policy 105 ; court 103 ; scienti\ufb01c 98 ; physics 97 ; constitution 93 1514 life 663 ; live 363 ; security 192 ; exist 188 ; peace 180 ; dead 175 ; living 164 ; existence 157 ; body 157 ; lives 153 ; exists 137 ; privacy 128 ; death 126 ; die 121 ; safety 112 1208 atheist 165 ; font 144 ; bio 119 ; bb 111 ; homosexual 109 ; ##group 99 ; sy 94 ; mormon 75 ; fed 72 ; manual 56 ; posting 52 ; spec 52 ; ##s 50 ; ##eri 49 ; auto 42 ; pointer 41 ; handgun 37 837 another 974 ; last 774 ; next 581 ; else 578 ; second 498 ; others 472 ; third 124 ; \ufb01rst 82 ; \ufb01nal 69 ; later 60 ; rest 49 ; future 47 ; 2nd 44 ; latter 41 ; previous 40 ; elsewhere 33 1291 lines 1848 ; read 701 ; writes 502 ; line 376 ; book 319 ; books 244 ; write 203 ; written 177 ; text 171 ; reading 157 ; wrote 144 ; article 107 ; quote 92 ; writing 86 ; paper 84 1656 agree 182 ; solution 165 ; advice 128 ; opinion 110 ; interface 104 ; response 88 ; suggestions 80 ; recommend 75 ; alternative 75 ; discussion 71 ; offer 71 ; argument 70 ; application 69 1874 apple 405 ; chip 386 ; disk 373 ; fbi 289 ; encryption 197 ; ##eg 171 ; hardware 166 ; nsa 154 ; ram 154 ; algorithm 134 ; tape 129 ; nasa 119 ; chips 111 ; ibm 100 ; \ufb02oppy 98 1966 stanford 269 ; washington 177 ; russian 156 ; cleveland 141 ; berkeley 137 ; california 131 ; chicago 105 ; ##co 96 ; turkey 95 ; york 83 ; boston 74 ; bosnia 73 ; soviet 71 ; russia 71 603 \ufb01le 682 ; list 526 ; article 501 ; card 424 ; bill 237 ; board 196 ; book 191 ; box 180 ; package 140 ; page 139 ; directory 119 ; section 118 ; group 114 ; library 90 ; \ufb01les 83 1401 done 644 ; didn 41 ; perform 35 ; performed 32 ; accomplish 25 ; accomplished 16 ; could 14 ; ##d 11 ; conduct 10 ; happen 10 ; say 10 ; committed 9 ; \ufb01nish 9 ; completed 9 ; conducted 8 460 clip 186 ; ##op 175 ; com 162 ; news 162 ; posts 109 ; works 106 ; micro 68 ; sim 66 ; share 66 ; ##yp 58 ; net 58 ; wire 54 ; ##os 48 ; power 43 ; es 40 ; \ufb02op 39 ; mac 39 ; tool 39 Table 9: More state-word examples, continued. 31 Under review as a conference paper at ICLR 2021 Transition Bigram - Occurrence 1843-990 is-that 68 ; fact-that 51 ; so-that 50 ; think-that 46 ; note-that 41 ; say-that 39 ; sure-that 38 ; believe-that 35 ; out-that 33 ; know-that 32 ; seems-that 28 ; mean-that 26 1010-1016 instead-of 40 ; amount-of 33 ; lot-of 26 ; form-of 23 ; lack-of 19 ; institute-of 16 ; case-of 15 ; capable-of 14 ; amounts-of 13 ; out-of 12 ; years-of 12 ; department-of 11 ; terms-of 11 960-458 up-to 56 ; down-to 29 ; access-to 25 ; according-to 24 ; due-to 22 ; go-to 17 ; response-to 14 ; subject-to 13 ; related-to 13 ; reference-to 13 ; as-to 12 ; lead-to 12 ; reply-to 12 441-698 have-to 139 ; going-to 116 ; seem-to 114 ; seems-to 68 ; supposed-to 40 ; need-to 30 ; had-to 26 ; used-to 22 ; want-to 20 ; seemed-to 15 ; tend-to 14 ; appears-to 13 ; likely-to 13 ; appear-to 12 1712-698 trying-to 67 ; try-to 46 ; able-to 43 ; like-to 41 ; hard-to 32 ; seem-to 22 ; seems-to 22 ; want-to 21 ; tend-to 21 ; willing-to 18 ; tried-to 16 ; enough-to 14 ; attempt-to 13 ; continue-to 12 1814-1666 about-it 91 ; of-it 71 ; with-it 42 ; to-it 29 ; for-it 27 ; do-it 26 ; on-it 24 ; have-it 12 ; understand-it 11 ; doing-it 9 ; know-it 9 ; see-it 8 ; call-it 8 ; believe-it 8 ; ##ing-it 7 ; \ufb01x-it 6 1295-523 problem-with 32 ; deal-with 32 ; do-with 28 ; up-with 16 ; problems-with 15 ; came-with 13 ; comeswith 13 ; along-with 12 ; work-with 12 ; contact-with 11 ; wrong-with 10 ; agree-with 10 ; disagree-with 9 628-150 based-on 65 ; depending-on 23 ; is-on 13 ; ##s-on 11 ; down-on 9 ; effect-on 9 ; are-on 8 ; working-on 8 ; effects-on 7 ; activities-on 7 ; depend-on 7 ; be-on 6 ; run-on 6 ; depends-on 6 477-1414 have-to 117 ; going-to 45 ; is-to 37 ; had-to 32 ; decided-to 12 ; need-to 11 ; has-to 11 ; having-to 9 ; required-to 9 ; willing-to 8 ; how-to 8 ; ,-to 7 ; reason-to 7 ; forced-to 7 477-1277 is-to 74 ; have-to 43 ; had-to 20 ; used-to 17 ; required-to 14 ; going-to 14 ; ,-to 13 ; need-to 13 ; as-to 12 ; order-to 11 ; needed-to 11 ; ##s-to 10 ; be-to 10 ; decided-to 10 145-461 believe-that 70 ; claim-that 24 ; evidence-that 18 ; assume-that 17 ; hope-that 15 ; belief-that 11 ; sure-that 9 ; prove-that 9 ; assuming-that 8 ; argue-that 8 ; likely-that 7 ; claims-that 7 278-217 know-of 22 ; end-of 16 ; out-of 14 ; think-of 13 ; ##s-of 10 ; accuracy-of 8 ; top-of 7 ; friend-of 6 ; copy-of 6 ; heard-of 6 ; one-of 4 ; middle-of 4 ; version-of 4 ; beginning-of 4 ; aware-of 4 1820-276 come-out 30 ; came-out 17 ; coming-out 14 ; put-out 12 ; get-out 11 ; \ufb01nd-out 10 ; check-out 9 ; turns-out 7 ; found-out 7 ; turn-out 7 ; turned-out 7 ; comes-out 7 ; go-out 6 ; ##ed-out 6 1142-461 is-that 17 ; fact-that 15 ; understand-that 12 ; see-that 11 ; realize-that 11 ; noted-that 8 ; says-that 8 ; note-that 7 ; read-that 7 ; forget-that 6 ; out-that 6 ; shows-that 6 1010-1998 lot-of 34 ; set-of 26 ; bunch-of 24 ; lots-of 22 ; series-of 13 ; number-of 10 ; thousands-of 10 ; hundredsof 10 ; plenty-of 10 ; full-of 7 ; pack-of 7 ; list-of 6 ; think-of 5 1125-843 of-a 124 ; is-a 86 ; for-a 84 ; to-a 50 ; s-a 16 ; be-a 14 ; ,-a 11 ; as-a 7 ; was-a 5 ; on-a 5 ; with-a 4 ; am-a 3 ; about-a 3 ; in-a 2 ; into-a 2 ; were-a 2 ; its-a 1 ; surrounding-a 1 476-1654 written-by 13 ; ##d-by 11 ; caused-by 8 ; ##ed-by 8 ; produced-by 6 ; followed-by 6 ; de\ufb01ned-by 4 ; committed-by 4 ; hit-by 4 ; supported-by 4 ; led-by 4 ; explained-by 4 ; run-by 4 1812-837 the-other 86 ; the-next 77 ; the-last 62 ; the-second 48 ; the-\ufb01rst 14 ; the-latter 10 ; the-third 9 ; the-latest 7 ; the-rest 6 ; the-previous 6 ; the-\ufb01nal 5 ; the-fourth 3 ; the-nearest 3 1938-145 i-believe 128 ; i-hope 66 ; i-suspect 28 ; i-assume 24 ; i-doubt 18 ; i-suppose 11 ; i-guess 11 ; i-expect 8 ; i-think 7 ; i-imagine 6 ; i-feel 5 ; i-trust 4 ; i-gather 3 ; i-bet 2 1820-1856 pick-up 14 ; come-up 12 ; came-up 11 ; stand-up 11 ; set-up 11 ; bring-up 8 ; show-up 8 ; comes-up 7 ; screwed-up 7 ; give-up 6 ; wake-up 6 ; speak-up 5 ; look-up 5 ; back-up 5 1417-979 more-than 163 ; better-than 33 ; less-than 13 ; faster-than 12 ; greater-than 11 ; longer-than 8 ; ##er-than 7 ; larger-than 6 ; worse-than 6 ; higher-than 6 ; slower-than 6 ; easier-than 4 111-111 of-the 75 ; to-the 34 ; for-the 23 ; on-the 14 ; with-the 12 ; about-the 7 ; part-of 7 ; in-the 5 ; into-the 5 ; like-the 4 ; out-of 4 ; at-the 4 ; by-the 3 ; \u2019-s 3 ; as-the 2 1579-654 talking-about 45 ; talk-about 25 ; concerned-about 14 ; worried-about 9 ; know-about 8 ; stories-about 7 ; worry-about 7 ; talked-about 6 ; rumours-about 5 ; news-about 5 ; feel-about 5 ; care-about 4 Table 10: State transition examples, with stopwords 32 Under review as a conference paper at ICLR 2021 Transition Bigram - Occurrence 371-371 health-care 14 ; side-effects 8 ; im-##mun 4 ; infectious-diseases 4 ; yeast-infections 4 ; ##thic-medicine 3 ; treat-cancer 3 ; health-insurance 3 ; barbecue-##d 3 ; hiv-infection 3 ; yeast-syndrome 3 1214-1214 orbit-##er 14 ; astro-##physics 7 ; lunar-orbit 7 ; space-shuttle 7 ; earth-orbit 5 ; pioneer-venus 5 ; space-station 5 ; space-##lab 4 ; lunar-colony 4 ; orbit-around 3 ; space-tug 3 ; space-##\ufb02ight 3 716-1556 mail-##ing 15 ; fra-##ering 12 ; ##mina-##tion 9 ; bash-##ing 7 ; ##dal-##izing 6 ; ##ras-##ing 5 ; ##band-##ing 4 ; ##ress-##ing 4 ; cab-##ling 4 ; adapt-##er 4 ; cluster-##ing 4 ; sha-##ding 4 931-931 gamma-ray 17 ; lead-acid 9 ; wild-corn 4 ; mile-long 3 ; smoke-##less 3 ; drip-##py 2 ; diamond-stealth 2 ; cold-fusion 2 ; 3d-wire 2 ; acid-batteries 2 ; schneider-stealth 2 ; quantum-black 2 1488-1488 law-enforcement 17 ; national-security 5 ; cold-blooded 4 ; health-care 4 ; human-rights 4 ; im-##moral 4 ; prophet-##ic 4 ; social-science 3 ; ethnic-##al 3 ; turkish-historical 3 1246-1246 bit-##net 35 ; tel-##net 12 ; use-##net 7 ; phone-number 7 ; dial-##og 6 ; ##p-site 5 ; phone-calls 5 ; bit-block 5 ; net-##com 4 ; bat-##f 4 ; ##t-##net 4 ; phone-call 4 ; arc-##net 3 1556-1556 abu-##sing 5 ; ##dal-##izing 4 ; obey-##ing 4 ; robb-##ing 3 ; ##ov-##ing 3 ; dial-##ing 3 ; contend##ing 3 ; ##upt-##ing 3 ; rough-##ing 3 ; contact-##ing 3 ; bash-##ing 3 ; favor-##ing 2 202-202 western-reserve 21 ; case-western 20 ; ohio-state 19 ; united-states 10 ; penn-state 5 ; african-american 5 ; north-american 5 ; middle-eastern 5 ; polytechnic-state 4 ; north-carolina 4 1912-1912 world-series 9 ; home-plate 7 ; division-winner 4 ; runs-scored 4 ; batting-average 4 ; game-winner 3 ; sports-##channel 3 ; plate-umpire 3 ; baseball-players 3 ; league-baseball 3 1461-1461 ##l-bus 5 ; bit-color 5 ; 3d-graphics 4 ; ##p-posting 3 ; computer-graphics 3 ; wire-##frame 3 ; bitgraphics 2 ; ##eg-\ufb01le 2 ; access-encryption 2 ; ##frame-graphics 2 ; \ufb01le-format 2 123-123 health-care 10 ; high-school 6 ; es-##crow 6 ; key-es 5 ; high-power 4 ; local-bus 4 ; low-level 4 ; high-speed 3 ; minor-league 2 ; health-service 2 ; regular-season 2 ; mother-##board 2 1702-1702 mile-##age 8 ; engine-compartment 5 ; semi-auto 5 ; manual-transmission 5 ; drive-power 4 ; door-car 3 ; passenger-cars 3 ; sports-car 3 ; shaft-drive 3 ; mini-##van 3 ; speed-manual 3 1874-1874 \ufb02oppy-disk 11 ; jp-##eg 11 ; encryption-algorithm 8 ; ##per-chip 7 ; ##mb-ram 7 ; ##ga-card 6 ; encryption-devices 5 ; silicon-graphics 4 ; disk-drive 4 ; \ufb02oppy-drive 4 1208-1064 atheist-##s 43 ; homosexual-##s 12 ; fed-##s 9 ; libertarian-##s 8 ; ##eri-##s 7 ; ##tile-##s 7 ; azerbaijani-##s 6 ; ##tar-##s 6 ; mormon-##s 5 ; sniper-##s 5 ; physicist-##s 4 1710-1710 power-supply 5 ; atomic-energy 4 ; water-ice 4 ; power-cord 4 ; ##com-telecom 3 ; light-pollution 3 ; light-bulb 3 ; radio-station 3 ; radio-##us 3 ; air-conditioning 3 ; light-##wave 2 1080-1080 public-access 19 ; via-anonymous 5 ; private-sector 5 ; available-via 4 ; general-public 4 ; communityoutreach 4 ; public-domain 3 ; personal-freedom 3 ; private-property 3 ; private-activities 3 254-1572 jimmy-carter 9 ; george-bush 9 ; bill-clinton 8 ; bryan-murray 4 ; joe-carter 4 ; henry-spencer 4 ; bill-james 4 ; janet-reno 4 ; craig-holland 4 ; clayton-cramer 4 ; ##zie-smith 4 1571-1571 ms-windows 24 ; windows-nt 12 ; ibm-pc 10 ; ms-##dos 7 ; unix-machine 6 ; microsoft-windows 5 ; windows-applications 4 ; run-windows 3 ; apple-monitor 3 ; mac-##s 3 ; desktop-machine 3 66-66 ##ian-1919 3 ; energy-signature 2 ; charlotte-##sville 2 ; environment-variables 2 ; duty-cycle 2 ; second-period 2 ; spin-state 2 ; power-consumption 2 ; inter-##mission 2 ; power-play 2 1683-1683 worth-##while 4 ; nominal-fee 4 ; get-paid 3 ; risk-factors 3 ; scholarship-fund 2 ; cost-$ 2 ; tax-dollars 2 ; bene\ufb01cial-item 2 ; bank-account 2 ; take-responsibility 2 1579-1579 m-sorry 5 ; news-reports 4 ; heard-anything 4 ; ran-##ting 3 ; short-story 3 ; news-reporters 3 ; pressconference 3 ; heard-something 3 ; tv-coverage 2 ; horror-stories 2 ; heard-horror 2 1656-1656 urbana-champaign 3 ; peace-talks 3 ; acceptable-solutions 2 ; marriage-partner 2 ; intercontinentalmeetings 2 ; interested-parties 2 ; conference-calls 2 ; handle-conference 2 ; cooperative-behaviour 2 Table 11: State transition examples, without stopwords 33 Under review as a conference paper at ICLR 2021 State Sequences Sentence 1755 0 117 1755 1755 0 117 1138 1755 103 117 1138 1755 117 a white bathroom with a yellow toilet and a bath tub and a sink 1755 0 117 1755 1755 959 117 117 1138 0 103 117 a white bathroom with a yellow toilet sink and painted bath tub 1755 117 1755 1755 0 117 1138 0 117 1138 103 117 a bathroom with a yellow toilet and white sink and bath tub 1755 117 0 1755 117 441 959 0 1061 1755 117 a man reading a newspaper on the side of a road 1755 117 0 1755 1755 441 959 117 1061 1755 117 a man reading a newspaper on the side of a road 1755 117 441 959 117 1061 1755 117 0 959 103 959 a man on the side of a road reading the newspaper 1755 117 0 1061 1755 117 1138 0 0 117 a man posing for a camera while holding \ufb01lthy bananas 1755 117 0 1061 959 103 0 117 a man posing for the camera holding bananas 1755 117 0 0 1755 117 1138 0 959 103 103 959 a man posing for a camera while holding his \ufb01lthy bananas 1755 0 103 117 1061 1755 0 117 1061 1755 103 a silver clock tower under a black intersection near a tree 1755 0 103 117 441 1755 0 117 1061 1755 103 a silver clock tower on a black intersection near a tree 103 117 1755 959 103 1061 0 103 103 103 clock tower in the intersection of black near silver tree 1755 103 103 117 1755 1755 117 1755 1755 0 117 1061 1755 117 a motor cycl ist in a photograph with a blur motion of a freeway 1755 117 1061 1755 0 103 103 117 959 1755 1755 1755 1755 1061 117 117 a photograph of a motor cycl ist motion , with a blur in the freeway 103 103 103 1755 0 0 959 117 1061 103 103 117 motor cycl ist in motion blur the freeway at motor cycl ist 1755 117 1061 103 103 103 1755 117 0 1061 1755 117 1755 0 117 a piece of chocolate bread on a cart spread to a plate with dark bananas 1755 117 1061 0 103 103 1755 117 1138 1017 103 103 0 527 1755 1755 117 a piece of dark chocolate bread with bananas and a on cart spread in a plate 103 103 1755 117 117 441 0 117 441 117 chocolate cart with bananas spread on dark cart on plate 1755 117 0 1755 117 1061 117 1755 1755 0 117 a lot \ufb01lled with lots of \ufb02owers in a v ases 1755 117 0 1755 117 1061 117 959 1138 903 117 1061 1687 a lot \ufb01lled with lots of \ufb02owers , and v ases of it 1755 117 1061 1082 117 0 1755 1138 1061 1595 959 a lot of v ases \ufb01lled with lots of \ufb02owers 0 117 0 1755 117 1755 959 103 441 1755 103 1755 117 1755 959 117 two people walking with dogs in the ocean on a beach with people in the background 0 117 441 103 1755 117 1755 117 959 two people on beach with dogs in background 0 117 117 1755 117 441 959 1755 1755 959 117 two people walking with dogs on the beach in the background 1755 117 1061 117 0 1952 0 1061 1755 0 103 a couple of women kneeling down next to a blow cat 1755 103 0 1952 1061 1061 1755 0 0 117 1755 1755 0 103 a cat kneeling down next to a couple blow women in a dry er 1755 117 1061 117 0 1952 0 1061 1755 103 0 103 a couple of women kneeling down next to a blow dry er 1755 117 0 1755 0 103 441 1755 117 a hand holds a hot dog on a table 1755 117 117 1755 0 103 103 959 103 a hand holds a hot dog in the table 1755 117 0 1755 0 103 1061 1755 117 959 a hand holds a hot dog at a table 1755 117 0 1755 103 1755 1755 117 1061 117 117 a woman eating a pastry in a piece of market area 1755 117 1017 150 1113 0 1061 103 1061 1755 117 117 a woman is eating a piece of pastry at a market area 117 0 1755 103 1061 1755 0 1061 117 117 woman eating a pastry at a piece of market area Table 12: Paraphrase Examples. 34 ", "title": "", "paper_info": "Under review as a conference paper at ICLR 2021\nTable 4: Randomized Forward v.s. TopK Forward. Comparison of MSE.\nMethod\nDense\nIntermediate\nLong-tail\nTOPK 20% MEM (Sun et al., 2019)\n3.8749\n1.0159\n0.1629\nTOPK 50% MEM (Sun et al., 2019)\n0.99\n0.2511\n0.0315\nRDP 1% MEM (ours)\n0.1461\n0.0669\n0.0766\nRDP 10% MEM (ours)\n0.0672\n0.0333\n0.0552\nRDP 20% MEM (ours)\n0.0469\n0.0200\n0.0264\nRDP 50% MEM (ours)\n0.0078\n0.0041\n0.0046\nTable 5: Randomized Forward, comparison of different proposal. Same base distribution as Figure 2,\nall proposals use 10% memeory).\nMethod\nDense\nIntermediate\nLong-tail\nUNIFORM (baseline)\n0.6558\n11.0500\n0.1600\nLOCAL ONLY (ours)\n2.908\n9.1711\n0.4817\nGLOBAL ONLY (ours)\n0.4531\n0.0961\n0.1003\nLOCAL + GLOBAL (ours)\n0.0284\n0.0173\n0.0222\nF\nADDITIONAL EXPERIMENT RESULTS\nF.1\nRDP V.S. TOPK MSE COMPARISON\nTable 4 shows the mean square error (MSE) comparison between RDP and TopK on dense, inter-\nmediate and long-tail distributions. Again, MSE for RDP is signi\ufb01cantly smaller with less memory\nconsumption.\nF.2\nMSE COMPARISON OVER PROPOSALS\nTable 5 shows the mean square error (MSE) comparison between different proposals. Our pro-\nposed local and global proposal outperforms a baseline uniform proposal on all distributions (dense,\nintermediate and long-tail).\nF.3\nCONTROLLING NUMBER OF STATES\nFigure 5 shows state frequency with different N. The long-tail behavior becomes clearer only when\nN is large enough (larger than 500 in our case).\nF.4\nCONTROLLING K1 V.S. K2 RATIO\nFigure 6 show state frequency with different K1/K2 ratios at different N. We highlight that when\nK2 = 0, a pure topK summation approach would lead to posterior collapse where there exist inactive\nstates that do not have any density. We also notice that an increasing K2 consistently increases the\nfrequency of tail states. This observation can be explained by the exploitation-exploration tradeoff,\nwhere increasing K1 would encourage the exploitation of states already con\ufb01dent enough during\ntraining (consequently leading to high-frequency head states after convergence) while increasing\nK2 would encourage exploring states that are not con\ufb01dent enough during training, leading to a\nlarger tail frequency after convergence.\nF.5\nCOMPARISON TO RANDOMLY INITIALIZED BERT\nFigure 7 shows the comparison of reconstruction NLL (\u2212 log p(xt|zt, \u00b7)) between a randomly ini-\ntialized BERT and a pretrained BERT. States induced from pretrained BERT are more meaningful\nthan random, making it easier to reconstruct words based on their corresponding states.\n5https://scikit-learn.org/stable/modules/generated/sklearn.manifold.\nTSNE.html\n26\n"}
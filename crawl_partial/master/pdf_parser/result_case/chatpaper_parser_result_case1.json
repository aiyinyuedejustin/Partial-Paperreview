{"Abstract": "Abstract\u2014Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable ef\ufb01ciency. The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Ef\ufb01ciency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20 k actions, depending on the task. Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% ef\ufb01ciency and real rows in 100% of trialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained model on the real robot with no additional real-world \ufb01ne-tuning. Toourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhulcsr/good_robot. Index Terms\u2014Computer vision for other robotic applications, deep learning in grasping and manipulation, reinforcement learning. I. ", "Introduction": "INTRODUCTION M ULTI-STEP robotic tasks in real-world settings are notoriously challenging to learn. They intertwine learning the immediate physical consequences of actions with the need to understand how these consequences affect progress towards the overall goal. Furthermore, in contrast to traditional motion planning, which assumes perfect information and known action models, learning only has access to the spatially and temporally limited information from sensing the environment. Manuscript received February 24, 2020; accepted July 20, 2020. Date of publication August 11, 2020; date of current version August 27, 2020. This letter was recommended for publication by Associate Editor J. Kober and Editor T. Asfour upon evaluation of the reviewers\u2019. comments. This work was supported by the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce of Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew Hundt.) Andrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon Kwon, and Gregory D. Hager are with The Johns Hopkins University, Baltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu; ngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu). Chris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail: cpaxton@nvidia.com). This article has supplementary downloadable material available at https:// ieeexplore.ieee.org, provided by the authors. Digital Object Identi\ufb01er 10.1109/LRA.2020.3015448 Fig. 1. Robot-created stacks and rows of cubes with sim to real transfer. Our Schedule for Positive Task (SPOT) framework allows us to ef\ufb01ciently \ufb01nd policies which can complete multi-step tasks. Video overview: https://youtu.be/MbCuEZadkIw Our key observation is that reinforcement learning wastes signi\ufb01cant time exploring actions which are unproductive at best. For example, in a block stacking task (Fig. 1), the knowledge that grasping at empty air will never snag an object is \u201ccommon sense\u201d for humans, but may take some time for a vanilla algorithm to discover. To address this, we propose the Schedule for Positive Task (SPOT) framework, which incorporates common sense constraints in a way that signi\ufb01cantly accelerates both learning and \ufb01nal task ef\ufb01ciency. While these types of constraints are intuitive, incorporating them into Deep RL (DRL) in a manner that leads to reliable and ef\ufb01cient learning is nontrivial [1], [2]. Our methods (Section III) takeinspirationfromahumaneandeffectiveapproachtotraining pets sometimes called \u201cPositive Conditioning.\u201d Consider the goal of training a dog \u201cSpot\u201d to ignore an object or event she \ufb01nds particularly interesting on command. Spot is rewarded with treats whenever partial compliance with the desired end behavior is shown, and simply removed from regressive situations with zero treats (reward). One way to achieve this is to start with multiple treats in hand, place one treat in view of Spot, and, if she eagerly jumps at the treat (a negative action), the human snatches and hides the treat immediately for zero reward on that action. With repetition, Spot will eventually hesitate, and so she is immediately praised with \u201cGood Spot!\u201d and gets a treat 2377-3766 \u00a9 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  \u4ee3\u7801\u5f00\u6e90\u4e86\uff0c\u5f3a\u5316 \u7528\u7684\u662f\u57fa\u7840\u63a7\u5236\u7b56\u7565\uff08pick place push\uff09\uff0c\u4ee5\u53ca\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u3002 \u51e0\u5343\u6b21\u5c31\u53ef\u4ee5\u5b66\u4f1a\uff01 HUNDT et al.: \u201cGOOD ROBOT!\u201d: EFFICIENT REINFORCEMENT LEARNING FOR MULTI-STEP VISUAL TASKS WITH SIM TO REAL TRANSFER 6725 Fig. 2. Our model architecture. Images are pre-rotated to 16 orientations \u03b8 before being passed to the network. Every coordinate a = (\u03c6, x, y, \u03b8) in the output pixel-wise Q-Values corresponds to a \ufb01nal gripper position, orientation, and open loop action type, respectively. Purple circles highlight the highest likelihood action arg maxa(Q(s, M(a))) (8) with an arrow to the corresponding height map coordinate, showing how these values are transformed to a gripper pose. The rotated overhead views overlay the Q value at each pixel from dark blue values near 0 to red for high probabilities. If you take a moment to compare the Q values of a single object across all actions (green arrows identify the same object across two oriented views) you will see each object is scored in a way which leads to a successful stack in accordance with its surrounding context. For example, the grasp model learns to give a high score to the lone unstacked red block for grasp actions and a low score to the yellow top of the stack, while the place model does the reverse. Here the model chooses to grasp the red block and place on the yellow, blue, and green stack. Experiment details are in Sections IV and V. separate from the one she should ignore. This approach can be expanded to new situations and behaviors, and it encourages explorationandrapidimprovementonceaninitialpartialsuccess is achieved. As we describe in Section III, our reward functions and SPOT-Q Learning are likewise designed to provide neither reward nor punishment for actions that reverse progress. Instances of progress reversal are associated with varying complexity. On the one hand, failing to stack the \ufb01rst block on top of another leaves the robot in a similar situation, so recovery takes \u03a9(1) actions. However, once a stack of n blocks exists, even a successful grasp might knock the whole stack down, reversing the entire history of actions for a given trial ( Fig. 3), so recovery is \u03a9(n). The latter, more dramatic instance of progress reversal is a challenging problem for reinforcement learning of multi-step tasks in robotics; our work provides a method for ef\ufb01ciently solving such cases. In summary, our contributions in this article are: 1) The overall SPOT framework for reinforcement learning of multi-step tasks, which improves on state of the art in simulation and can train ef\ufb01ciently on real-world situations. 2) SPOT-Q Learning, a method for safe and ef\ufb01cient training in which a mask focuses exploration at runtime and generates extra on-the-\ufb02y training examples from past experience during replay. 3) State of the art zero-shot domain transfer from simulated stacking and row building tasks to their real world counterparts, as well as robustness with respect to a change in hardware and scene positions. Fig. 3. Red arrows show how individual successful actions can fail on the larger stacking task, forcing eventual progress reversal where a partial stack topples or the top must be removed. Ideally algorithms should ef\ufb01ciently learn to prevent this situation and succeed as indicated by the green arrows. Thus, temporal and workspace dependencies must be considered. Events at a current time ti \u2208 T, i \u2208 [1...n] can in\ufb02uence the likelihood of successful outcomes for past actions th|h < i and future actions tj|j > i. A successful choice of action atanygiventi willensurebothpastandfutureactionsareproductivecontributors to the larger task at hand. In our experiments a partial stack or row is itself a scene obstacle. The gray wall pictured here is for illustrative purposes only. 4) Anablationstudyshowingthat SituationRemoval dramatically decreases progress reversal; that a progress metric increases ef\ufb01ciency; and that trial rewards improve on discounting, but involve a trade-off between ef\ufb01ciency and support for sparse rewards. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  ", "Related Work": "RELATED WORK Deep Neural Networks (DNNs) have enabled the use of raw sensor data in robotic manipulation [1]\u2013[5]. In some approaches, a DNN\u2019s output directly corresponds to motor commands, e.g., [3], [4]. Higher-level methods, on the other hand, assume a simple model for robotic control and focus on bounding box or pose detection for downstream grasp planning [1], [6]\u2013[11]. RGB-D sensors can be bene\ufb01cial [1], [11], [12], as they capture physical information about the workspace. Object-centric skill learning can be effective and generalize well, e.g., [13]\u2013[16] focus on block stacking by classifying simulated stacks as stable orlikelytofall.Similarly,[17],[18]developphysicalintuitionby predictingpushactionoutcomes.Ourworkdiffersbydeveloping visual understanding and physical intuition in concert with the progress of multi-step tasks. Grasping is a particularly active area of research. DexNet [19], [20] learns from a large number of depth images of top-down grasps, and gets extremely good performance on grasping novel objects but does not look at long-horizon tasks. 6-DOF GraspNet [21] uses simulated grasp data to generalize to new objects and has been extended to handle reliable grasping of novel objects in clutter [12]. DRL has proven effective at increasingly complex tasks in robotic manipulation [1], [5], [22], [23]. QT-Opt [5] learns manipulation skills from hundreds of thousands of real-world grasp attempts on real robots. Domain Adaptation, such as applying random textures in simulation, can also enhance sim to real transfer [24], [25]. Other methods focus on transferring visuomotor skills from simulated to real robots [22], [26]. Our work directs a low-level controller to perform actions rather than regressing torque vectors directly, following prior work [1], [23] by learning a pixel-wise success likelihood map. Multi-step tasks with sparse rewards present a particular challenge in reinforcement learning because solutions are less likely to be discovered through random exploration. When available, demonstration can be an effective method for guiding exploration [27]\u2013[29]. Multi-step tasks can be split into modular sub-tasks comprising a sketch [30], while [31] has robot-speci\ufb01c and task-speci\ufb01c learning modules. Safety is crucial for reinforcement learning in many realworld settings [32]\u2013[34]. The preliminary experiments in SectionIV-DshowthatSPOT-Qprovidesawaytoincorporatesafety into general Q-Learning based algorithms [35]. We compare the SPOT framework to VPG [1], a method for RL-based table clearing tasks which can be trained from images within hours on a single robot, in Sections IV and V. VPG is frequently able to complete adversarial scenarios like \ufb01rst pushing a tightly packed group of blocks apart and then grasping the now-separated objects. Some of the most closely related recent work involves tasks with multiple actions: [36] includes placing one block on another, [37] places one towel on a bar, and [38] clears a bin, but the \ufb01rst two are not long-horizon tasks and the possibility of progress reversal (Fig. 3) is never considered. III. ", "Approach": "APPROACH We investigate multi-step tasks for which there is a sparse and approximate notion of task progress. It is possible to improve the ef\ufb01ciency of learning by taking these four measures: structuring such problems to capture invariant properties of the data, deploying traditional algorithms where they are most effective, ensuring rewards do not propagate through failed actions, and introducing an algorithm which removes unnecessary exploration. We will later demonstrate our approach in the context of the general problem of assembly through vision-based robotic manipulation. We frame the problem as a Markov Decision Process (S, A, P, R), with state space S, action space A, transition probability function P : S \u00d7 S \u00d7 A \u2192 R, and reward function R : S \u00d7 A \u2192 R. This includes a simplifying assumption equating sensor observations and state. At time step t, the agent observes state st and chooses an action at according to its policy \u03c0 : S \u2192 A. The action results in a new state st+1 with probability P(st+1|st, at). As in VPG [1], we use Q-learning to produce a deterministic policy for choosing actions. The function Q : S \u00d7 A \u2192 R estimates the expected reward R of an action from a given state, i.e. the \u201cquality\u201d of an action. Our policy \u03c0 selects an action at as follows: \u03c0(st) = arg maxa\u2208AQ(st, a) (1) Thus, the goal of training is to learn a Q that maximizes R over time. This is accomplished by iteratively minimizing |Q(st, at) \u2212 yt|, where the target value yt is: yt = R(st+1, at) + \u03b3Q(st+1, \u03c0(st+1)) (2) Q-learning is a fundamental algorithm in RL, but there are key limitations in its most general form for applications like robotics where the space and cost of actions and new trials is extremely large, and ef\ufb01cient exploration can be essential or even safety critical. It is also highly dependent on R, whose de\ufb01nition can cause learning ef\ufb01ciency to vary by orders of magnitude, as we show in Section IV-C, and so we begin with our approach to reward shaping. A. Reward Shaping Reward shaping is an effective technique for optimizing a reward R, to train policies [39] and their neural networks ef\ufb01ciently. Here, we present several reward functions for later comparison (Section IV-C), which build towards a general formulation for reward shaping conducive to ef\ufb01cient learning on a broad range of novel tasks, thus reducing the ad hoc nature of successful reward schedules. Suppose each action a is associated with a sub-task \u03c6 \u2208 \u03a6 and that we have an indicator function 1a[st+1, at] which equals 1 if an action at succeeds at \u03c6 and 0 otherwise.1 As in VPG [1], our baseline rewards follow this principle and include a sub-task weighting function W : \u03a6 \u2192 R, according to their subjective dif\ufb01culty and importance:2 Rbase(st+1, at) = W(\u03c6t)1a[st+1, at]. (3) Next, we de\ufb01ne a sparse and approximate task progress function P : S \u2192 R \u2208 [0, 1],indicatingproportionalprogresstowardsan overall goal, where P(st) = 1 means the task is complete.3 As in our story of Spot the dog (Section I), a progress reversal leads us to perform Situation Removal (SR) on the agent and 1Examples of action indicator sources include the grasp detector in our Robotiq 2F85 gripper, human supervision, or another detection algorithm. 2In our experiments we assign simple values for each successful action type: W\u03c6t \u2208 {Wpush =0.1, Wgrasp =1, Wplace =1}. 3In our block tasks P is the height of the stack or length of the row vs the goal size, in table clearing either the number of objects or occupied pixels vs the total, and in navigation the remaining vs initial distance. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  HUNDT et al.: \u201cGOOD ROBOT!\u201d: EFFICIENT REINFORCEMENT LEARNING FOR MULTI-STEP VISUAL TASKS WITH SIM TO REAL TRANSFER 6727 Fig. 4. Example of SPOT Trial Reward Rtrial (6), and the SPOT Progress Reward RP (5) with images of key action steps. Actions 1\u20133: a1 is an initial grasp, followed by a successful place where a slightly off balance stack of height 2 is formed. Actions 4-5: Progress reversal occurs when a grasp then place knocks the stack over, so the reward values go to zero. Action 7: While not pictured, the scene is similar to a3 but with a better balanced top block. Intuitively, since a9 doesn\u2019t topple like a5 a better reward at a7 would be appropriate, which is one advantage of Rtrial over RP, because RP(s4, a3) = RP(s8, a7) and Rtrial(s4, a3) < Rtrial(s8, a7) because a7 leads directly to a successful stack. Actions 11\u201314: Grasp and place actions lead to a full stack of 4 completing the trial. The \ufb01nal Rtrial at a14 is 2 \u00d7 RP. Here W\u03c6t \u2208 {Wpush =.5, Wgrasp = 1, Wplace =1.25} for chart visibility. physically reset the environment during training (Fig. 3). We de\ufb01ne an associated indicator 1SR[st, st+1], which equals 1 if P(st+1) \u2265 P(st) and 0 otherwise. These lead to new reward functions: RSR(st+1, at) = 1SR[st, st+1]Rbase(st+1, at) (4) RP(st+1, at) = P(st+1)RSR(st+1, at) (5) One advantage of Rbase, RSR, and RP is that each is available \u201cinstantaneously\u201dinthemidstofatrialaftertwostatetransitions. However, they do not consider the possibility that an early mistake might lead to failure many steps down the line (Fig. 3, 4), and so we will develop a reward which propagates across whole trials. B. Situation Removal: SPOT Trial Reward Is it possible for a reward function to account for actions which lead to failures at a later time step while still training more ef\ufb01ciently than a standard discounted reward RD where RD(st+1, at)=\u03b3 RD(st+2, at+1)? Our approach is to block reward propagation through failed actions via the Situation Removal concept: Rtrial(st+1, at) = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 0, if R\u2217(st+1, at) = 0 2R\u2217(st+1, at), if t = N R\u2217(st+1, at) + \u03b3Rtrial(st+2, at+1), otherwise (6) Algorithm 1: SPOT-Q with Prioritized Experience Replay. 1: Input Replay Memory HT =(ST , AT , RT , PredictedT ) 2: while AGENT_IS_ACTING( ) do 3: t = PRIORITIZED_EXPERIENCE_SAMPLE(T, HT ) 4: yt = R(st+1, at) + \u03b3Q(st+1, \u03c0(st+1)) 5: \u03b4t = HUBER_LOSS(Q(st, at); yt) 6: a\u03c0,t = \u03c0(st) 7: if M(st, a\u03c0,t) = 0 then \u25b7 The action would fail. 8: y\u2032 t = \u03b3Q(st+1, a\u03c0,t) \u25b7 New 0 reward sample. 9: \u03b4t = \u03b4t+ HUBER_LOSS(Q(st, at); y\u2032 t) 10: end if 11: BACKPROP(\ufffd \u03b4t); step optimizer; update weights. 12: end while where R\u2217 can be an arbitrary instant reward function such as RSR or RP from Section III-A, N marks the end of the trial, and \u03b3 is the usual discount factor which is set to \u03b3 = 0.65. The effect of using Rtrial is that future rewards only propagate across time steps where subtasks are completed successfully. As illustrated in Fig. 4 and described in the caption, the zero reward from situation removal cuts the propagation of future rewards back through time steps containing failed actions. This focuses learning on short and successful sequences that complete a task. C. SPOT-Q Learning and Dynamic Action Spaces In this section, we go a step further and leverage a priori knowledge about the environment to make simple but powerful assumptions which both reduce unproductive attempts and accelerate training. Speci\ufb01cally, there are many occasions when certain action failures are easily predicted from the same sensor signal used for Q learning. To this end, we assume the existence of an oracle, M(st, a) \u2192 {0, 1}, which takes the current state st and an action a and returns 0 if an action is certain to fail, and 1 otherwise. This is subtly different from the success indicator 1a[st+1, at], which requires the outcome st+1 of an action at to determine success or failure.4 Using M, we de\ufb01ne the dynamic action space Mt(A): Mt(A) = {a \u2208 A|M(st, a) = 1}. (7) In short, Mt(A) does not tell us whether a \u2208 A is an action worth taking, but rather whether it is worth exploring. Given a state st, the question becomes how to most effectively utilize Mt in training. If \u03c0(st) \u0338\u2208 Mt(A), then \u03c0(st) can be treated as a failure for the purposes of learning and we can explore the next best action not guaranteed to fail. To formalize this, we introduce SPOT-Q Learning which is a new target value function replacing (2): yM,t = \u23a7 \u23a8 \u23a9 yt, if \u03c0(st+1) \u2208 Mt(A) yt + \u03b3Q(st+1, \u03c0M(st+1)) otherwise + R(st+1, at), (8) 4For example, grasping an object can only succeed if there is depth data in the neighborhood of a predicted action, so attempts to grasp in free space can be easily predicted to fail, as we demonstrate in Section IV Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  6728 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 5, NO. 4, OCTOBER 2020 TABLE I SIMULATED ROBOT TASK RESULTS (SECTION IV) WITH THE SPOT FRAMEWORK Multi-Step Task Test Success Rates Measured Out of 100% for Simulated Tasks Involving Push, Grasp and Place Actions Trained for 20 K Actions (Section IV-C). Bold Entries Highlight Our Key Algorithm Improvements Over the Baseline. \u201cTrials\u201d Indicates the Overall Rate at Which Stacks or Rows Are Successfully Completed. the Algorithm Components Are Described in Section III, Except for \u201cMask But No SPOT-Q\u201d Which Is a Special Case Described in the SPOT-Q Section of Our Ablation Study (Section IV-C). Values Are the Min and Max of Two Runs. where \u03c0M(st) = arg maxa\u2208Mt(A)Q(st, a). Crucially, we perform backpropagation on both the masked action, which has 0 reward, and the unmasked action \u03c0M(st), which the robot actually performs. Algorithm 1 describes how we continuously train from past examples with SPOT-Q and Prioritized Experienced Replay (PER) [40] as the current policy is rolled out. In Section IV, we discuss how SPOT-Q allows us to surpass prior work, wherein similar heuristics [1], [41] neither match SPOT-Q nor account for the safety considerations we discuss later. IV. SIMULATION EXPERIMENTS Our method improves performance and action ef\ufb01ciency over the state of the art on the table clearing task from VPG [1], as well as on two challenging multi-step tasks of our design: creating a stack of four blocks and creating a horizontal row of four blocks. Our best results can achieve 100% trial success on the simulated stacking and row tasks, models which successfully transfer to the real world as we show in Section V. We detail a series of simulation experiments to understand the contribution of each element of our approach to this overall performance. To do so, we evaluate each reward function, the effect of SPOT-Q on heuristic exploration, other possible SPOTQ implementations, the reward weighting term W, and then we describe our best results with SPOT-Q + RP and SPOT-Q + Rtrial. In brief, we \ufb01nd that Situation Removal RSR is the largest contributortoourimprovedperformance,RP improvesaccuracy and ef\ufb01ciency, and Rtrial trains more ef\ufb01ciently than discounted rewards while accounting for a time delay between actions and consequences. SPOT-Q improves results over no masking, and over basic masking on its own. Finally, we test a grid world navigation task [42] to show how the SPOT framework applies to safe reinforcement learning. Tables I and III summarize these results. TABLE II REAL ROBOT TASK RESULTS (SECTION V) WITH THE SPOT FRAMEWORK Bold Entries Highlight Sim to Real Transfer With SPOT-Q. In This Table No SPOT-Q Also Means No Masking. TABLE III SAFETY GRID WORLD (FIG. 5) COMPARISON OF ALGORITHM CHANGES ON TOP OF RAINBOW [35] Cases without RP Use the Built-In Reward. \u201cTrials Complete\u201d Is the Percentage of 1000 Test Trials Successfully Completed By Reaching the Green Square in Fewer Than 100 Actions Without Entering Lava. \u201cEf\ufb01ciency\u201d Is the Best Test Ideal/Actual Actions Per Trial After 500 K Training Actions. the Ideal Action Count for Each Trial Is Found Via a Wavefront Planner. \u201cActions\u201d Reports How Many Training Steps Were Taken Until the First Case Where 100% of 30 Validation Trials Succeed. Values Are the Min and Max of Two Runs. A. Robot Implementation Details We consider a robot capable of being commanded to a speci\ufb01ed arm pose and gripper state in its workspace. Our action space consists of three components: action types \u03a6, locations X \u00d7 Y , and angles \u0398. The agent observes the environment via a \ufb01xed RGB-D camera, which we project so that z is aligned with the direction of gravity, as shown in Fig. 2. We discretize the spatial action space into a square height map with 0.448m on a side and 224 \u00d7 224 bins with coordinates (x, y), so each pixel represents roughly 4mm2 as per VPG [1]. The angle space \u0398 = { 2\u03c0i k |i \u2208 [0, k \u2212 1]} is similarly discretized into k = 16 bins. The set of action types consists of three high-level motion primitives \u03a6 = {grasp, push, place}. In our experiments action success is determined by our gripper\u2019s sensor for grasp, object perturbations for push, and an increase in stack height or row length for place. A traditional trajectory planner executes each action a = (\u03c6, x, y, \u03b8) \u2208 A on the robot. For grasping and placing, each action moves to (x, y) with gripper angle \u03b8 \u2208 \u0398 and closes or opens the gripper, respectively. A push starts with the gripper closed at (x, y) and moves horizontally a \ufb01xed distance along angle \u03b8. Fig. 2 visualizes our overall algorithm, including the action space and corresponding Q-values. B. Evaluation Metrics We evaluate our algorithms in randomized test cases in accordance with the metrics found in VPG [1]. Ideal Action Ef\ufb01ciency is 100% and calculated as Ideal/Actual action count; de\ufb01ned Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  HUNDT et al.: \u201cGOOD ROBOT!\u201d: EFFICIENT REINFORCEMENT LEARNING FOR MULTI-STEP VISUAL TASKS WITH SIM TO REAL TRANSFER 6729 as 1 action per object for grasping tasks; and 2 actions per object for tasks which involve placement. This means 6 total actions for a stack of height 4 since only 3 objects must move, and 4 total actions for rows by placing two blocks between two endpoints. We validate simulated results twice with 100 trials of novel random object positions. C. Algorithm Ablation We compare the contribution from each component of the underlying algorithm and against baseline approaches in Table I, except for clearing tasks which are provided in the text. Unless otherwise stated we summarize rows and stacks together as a combined average below. Clear 20 Toys: We establish a baseline via the primary simulated experiment found in VPG [1], where 20 toys with varied shapes must be grasped to clear the robot workspace. The SPOT framework matches VPG [1] with 100% task completion and improves both the rate of grasp successes from 68% to 84% and action ef\ufb01ciency from 64% to 74%. Clear Toys Adversarial: The second baseline scenario is the 11 cases of challenging adversarial arrangements from VPG [1], where toys are placed in tightly packed con\ufb01gurations. Each case is run 10 times and the SPOT framework completely clears 7/11 cases compared to 5/11 in VPG [1]; the clearance rate across all 110 runs improves to 95% from 84%. Ef\ufb01ciency in this case drops from 60% to 38%, which is accounted for by the increase in the number of dif\ufb01cult cases solved, as separating the blocks can take several attempts. Reward Functions: Rbase, RSR, RP, and Rtrial incrementally extend one another (Section III-A, III-B). All masking is disabled for this study unless otherwise indicated. RD s.t. RD(st+1, at) = \u03b3 RD(st+2, at+1) is discounting, the most conventional approach to trial rewards. When evaluated with RP at the \ufb01nal time step and \u03b3 = 0.9, grasp and place actions succeed at a rate of 5% and 45%, respectively. Stacks of height 2-3 are created and performance improves with masking (32%, 48%). However, this approach is incredibly inef\ufb01cient with no stacks of 4 within 20 k actions. That said, we would expect convergence if orders of magnitude more training were feasible [43]. Rbase is effective for pushing and grasping [1], but it is not suf\ufb01cient for multi-step tasks, only completing 13% of rows and stacks with about 200+ actions per trial in the best case. In another case, it repeatedly reverses progress by often looping grasping then placing of the same object at one spot, leading to 99% successful grasps but 0 successful trials overall, even after manual scene resets. We do not expect Rbase to converge on these tasks as there is no progress signal to indicate, for example, that grasping from the top of an existing stack is a poor choice. RSR resolves the progress reversal problem immediately since suchactionsget0reward;andthusweseeanastoundingincrease in trial successes from 13% to 94%, and an order of magnitude ef\ufb01ciency increase to 23% across both tasks, or about 22 actions per trial. RP leads to a rise in combined trial successes to 97%, and ef\ufb01ciency to 45%, or about 20 actions per trial. This improves upon pure situation removal by incorporating the quantitative amount of progress. Rtrial utilizes RP as the instant reward function in this test, and has an average trial success rate of 96% for stacks and ef\ufb01ciency of 31%, or about 19 actions per trial. However, performance degrades signi\ufb01cantly for rows, declining to an 80% trial success rate and just 16% action ef\ufb01ciency, or about 25 actions per trial. These values indicate Rtrial strikes a trade-off between the inef\ufb01ciency of RD and the need for a more instantaneous progress metric in RP, as the most recent value can be utilized to \ufb01ll actions with no progress feedback. We also note that once SPOT-Q is added this reward is the best for stacking and second best overall, as we show below. SPOT-Q: VPG [1] evaluated heuristics that specify exact locations to explore, and they found it led to worse performance. A similar approach in QT-Opt [41] is phased out as training proceeds, indicating that their methods do not contribute to improving outcomes throughout the training process. By contrast, SPOT-Q is enabled at all times and excises regions with zero likelihood of success, while other regions of interest remain open for exploration. So does this difference in heuristic design matter? The \u201cMask but no SPOT-Q\u201d test disables the if statement in Alg. 1 to simulate a typical heuristic in which exploration is directed to particular regions without zero reward guidance. \u201cMask but no SPOT-Q\u201d completes 95% of trials, compared to 88% without masking and 99% with SPOT-Q; action ef\ufb01ciency results are even more pronounced at 37%, 23%, and 50% respectively. Both these results and Section IV-D show SPOT-Q simply works throughout training and testing with little to no tuning, and so we conclude that SPOT-Q improves the ef\ufb01ciency of learning from heuristic data. SPOT-Q Alternatives: We evaluated two alternatives to SPOTQ (eq. 8, Alg. 1), where 0 reward backpropagation is performed on all masked pixels with loss applied to the (1) sum, and (2) average of the masked scores in addition to the actually executed action. In both cases, the gradients exploded and the algorithm did not converge. Only SPOT-Q is able to ef\ufb01ciently enhance convergence. Reward Weighting: SPOT-Q + RP where Wpush =0.1 succeeds in 99% of trials, but just 27% when Wpush =1.0. The weighting in Fig. 4 on Rtrial without masking or SPOT-Q achieves 97% stack success and 38% action ef\ufb01ciency, but we leave all weighting constant for consistency in Table I. This shows W (3) is important for ef\ufb01cient training. SPOT-Q + RP: This con\ufb01guration has the best overall simulation performance with a 99% trial success rate and 50% ef\ufb01ciency, or about 10 actions per trial. It is also the best simulated row model with 98% trial success in one test and 100% in the second, with a high 62\u201368% action ef\ufb01ciency. SPOT-Q + Rtrial: This has the best stacking model with 100% completion in both test cases, and 45\u201351% ef\ufb01ciency. Overall performance is the second best with 97% trial success, and 37% ef\ufb01ciency, or about 14 actions per trial. D. Safety and Domain Generalization To demonstrate the broad scope of the SPOT framework, we evaluate it on the simple but challenging Safety Grid World [42] (Fig. 5), an environment type widely used to evaluate RL algorithms [32], [39]. Here the red robot must move forward or turn as it navigates towards the green square without ever entering the lava. If we had just one real robot to learn within this world, standard DRL would be extremely unsafe, but the SPOT framework allows the robot to safely explore the space. As Table III shows, all improvements are consistent with our more realistic tasks. We start with Rainbow [35], a Q learning Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  ", "Experiments": "EXPERIMENTS Finally, we examine the performance of SPOT-Q on real robot tasks, both via training from scratch and sim to real transfer. In both cases, performance was roughly equivalent to that achieved in simulation, which shows the strength of our approach for ef\ufb01cient and effective reinforcement learning. We use the setup described in [29], [44], including a Universal Robot UR5, a Robotiq 2-\ufb01nger gripper, and a Primesense Carmine RGB-D camera; all but the arm differ from those in our simulation. Other implementation details are as described in Section IV-A, and results are in Table II. Real Pushing and Grasping: We train the baseline pushing and grasping task from scratch in the real world, test with 20 objects and see 100% test clearance, 75% grasp success rate, and 75% ef\ufb01ciency in 1 k actions; these results are comparable to the performance charted by VPG [1] over 2.5 k actions. Sim to real does not succeed in this task. Sim to Real vs Real Stacking: After training in simulation we directly load the model for execution on the real robot. Remarkably, all tested sim to real stacking models complete 100% of trials, outperforming a model trained on the real robot which is successful in 82% of trials (Fig. 6, Table II). RP and Rtrial have an equal action ef\ufb01ciency at 61%, and the version of RP without SPOT-Q or a mask exhibits slightly lower ef\ufb01ciency at 51%. This is particularly impressive considering that our scene is exposed to variable sunlight. Intuitively, these results are in part due to the depth heightmap input in stacking and row-making. Sim to Real Rows: Our RP + SPOT-Q sim to real rows model is also able to create rows in a remarkable 100% of attempts with 59% ef\ufb01ciency. Rtrial + SPOT-Q and RP with no mask 5In the grid world we only evaluate RP and the built-in reward (where all reward is delivered at the end) because there is little distinction between a failed action and failed trial. Fig. 6. Real training of the SPOT framework to Stack 4 Cubes with Rtrial and SPOT-Q. Failures include missed grasps, off-stack placements, and actions in which the stack topples. Toppling can occur during successful grasp and push actions. perform slightly worse, both with 90% of trials complete, and an ef\ufb01ciency of 83% and 58%, respectively. The high ef\ufb01ciency of RP with no mask is because we end real trials immediately when the task becomes unrecoverable, such as when a block tumbles out of the workspace. We exclusively evaluate sim to real transfer in this case because training progress is signi\ufb01cantly slower than with stacks. We expect that block based tasks are able to transfer because the network relies primarily on the depth images, which are more consistent between simulated and real data. This might reasonably explain why pushing and grasping does not transfer, a problem which could be mitigated in future work with methods like Domain Adaptation [24], [25]. VI. ", "Conclusion": "CONCLUSION We have demonstrated that the SPOT framework is effective for training long-horizon tasks. To our knowledge, this is the \ufb01rst instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and creating rows with consideration of progress reversal. The SPOT framework quanti\ufb01es an agent\u2019s progress within multi-step tasks while also providing zero-reward guidance, a masked action space, and situation removal. It is able to quickly learn policies that generalize from simulation to the real world. We \ufb01nd these methods are necessary to achieve a 100% completion rate on both the real block stacking task and the row-making task. SPOT\u2019s main limitation is that while intermediate rewards can be sparse, they are still necessary. Future research should look at ways of learning task structures that incorporate situation removal from data. In addition, the action space mask M is currently manually designed; this and the lower-level open loop actions might be learned as well. Another topic for investigation is the difference underlying successful sim to real transfer of stacking and row tasks when compared to pushing and grasping. Finally, in the future, we would like to apply our method to more challenging tasks. ACKNOWLEDGMENT We extend our thanks to Adit Murali for Safety Grid World integration; to Molly O\u2019Brien for valuable discussions, feedback, Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  ", "References": "REFERENCES [1] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser, \u201cLearning synergies between pushing and grasping with self-supervised deep reinforcement learning,\u201d in Proc. Int. Conf. Intell. Robot. Syst. 2018, pp. 4238\u20134245. [2] O. Kroemer, S. Niekum, and G. D. Konidaris, \u201cA review of robot learning for manipulation: Challenges, representations, and algorithms,\u201d2019, arXiv:1907.03146. [3] S. Levine, C. Finn, T. Darrell, and P. Abbeel, \u201cEnd-to-end training of deep visuomotor policies,\u201d The J. Mach. Learn. Res., vol. 17, no. 1, pp. 1334\u20131373, 2016. [4] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, \u201cLearning hand-eye coordination for robotic grasping with deep learning and largescale data collection,\u201d Int. J. Robot. Res., vol. 37, no. 4\u20135, pp. 421\u2013436, 2018. [5] D. Kalashnikov et al., \u201cQT-OPT: Scalable deep reinforcement learning for vision-based robotic manipulation,\u201d CoRL, 2018. [6] J. Redmon and A. Angelova, \u201cReal-time grasp detection using convolutional neural networks,\u201d in Proc. Int. Conf. Robot. Autom.. 2015, pp. 1316\u20131322. [7] B. Drost, M. Ulrich, N. Navab, and S. Ilic, \u201cModel globally, match locally: Ef\ufb01cient and robust 3D object recognition,\u201d in Proc. Comput. Vis. Patt. Recognit., vol. 1, no. 2, 2010, p. 5. [8] S.KumraandC.Kanan,\u201cRoboticgraspdetectionusingdeepconvolutional neural networks,\u201d Int. Conf. Intell. Robot. Syst., Sep. 2017. [9] E. Jang, S. Vijayanarasimhan, P. Pastor, J. Ibarz, and S. Levine, \u201cEnd-toend learning of semantic grasping,\u201d in CoRL, 2017, pp. 119\u2013132. [10] I. Lenz, H. Lee, and A. Saxena, \u201cDeep learning for detecting robotic grasps,\u201d Int. J. Robot. Res., vol. 34, no. 4-5, pp. 705\u2013724, 2015. [11] D. Morrison, J. Leitner, and P. Corke, \u201cClosing the loop for robotic grasping: A real-time, generative grasp synthesis approach,\u201d RSS, 2018. [12] A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox, \u201c6-DoF grasping for target-driven object manipulation in clutter,\u201d Int. Conf. Robot. Autom., 2020. [13] A. Gupta, C. Eppner, S. Levine, and P. Abbeel, \u201cLearning dexterous manipulation for a soft robotic hand from human demonstrations,\u201d in Int. Conf. Intelligent Robots Syst.. IEEE, 2016, pp. 3786\u20133793. [14] C. Devin, P. Abbeel, T. Darrell, and S. Levine, \u201cDeep object-centric representations for generalizable robot learning,\u201d in Int. Conf. Robot. Autom.. 2018, pp. 7111\u20137118. [15] A. Lerer, S. Gross, and R. Fergus, \u201cLearning physical intuition of block towers by example,\u201d Int. Conf. Mach. Learn., pp. 430\u2013438, 2016. [16] O. Groth, F. B. Fuchs, I. Posner, and A. Vedaldi, \u201cShapeStacks: Learning vision-based physical intuition for generalised object stacking,\u201d in Proc. Europ. Conf. Comput. Vis., 2018, pp. 702\u2013717. [17] C. Finn, I. J. Goodfellow, and S. Levine, \u201cUnsupervised learning for physical interaction through video prediction,\u201d in Proc. Adv. Neural Inform. Process. Syst., 2016, pp. 64\u201372. [18] A. Byravan and D. Fox, \u201cSE3-Nets: Learning rigid body motion using deep neural networks,\u201d in Proc. Int. Conf. Robot. Autom., 2017, pp. 173\u2013180. [19] J. Mahler et al. \u201cDEX-Net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics,\u201d in Proc. Robot.: Sci. Syst. (RSS), 2017. [20] J. Mahler et al., \u201cLearning ambidextrous robot grasping policies,\u201d Sci. Robot., vol. 4, no. 26, 2019. [21] A. Mousavian, C. Eppner, and D. Fox, \u201c6-DoF GraspNet: Variational grasp generation for object manipulation,\u201d in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 2901\u20132910. [22] F. Zhang, J. Leitner, M. Milford, and P. Corke, \u201cModular deep Q networks for sim-to-real transfer of visuo-motor policies,\u201d ACRA, 2017. [Online]. Available: http://arxiv.org/abs/1610.06781 [23] A. Zeng, S. Song, J. Lee, A. Rodriguez, and T. A. Funkhouser, \u201cTossingBot: Learning to throw arbitrary objects with residual physics,\u201d in Proc. Robot.: Sci. Syst. XV, vol. 15, 2019. [24] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, \u201cDomain randomization for transferring deep neural networks from simulation to the real world,\u201d in Proc. Int. Conf. Intell. Robot. Syst. 2017, pp. 23\u201330. [25] K. Bousmalis et al., \u201cUsing simulation and domain adaptation to improve ef\ufb01ciency of deep robotic grasping,\u201d in Int. Conf. Robot. Autom.. 2018, pp. 4243\u20134250. [26] Y.Zhuetal.,\u201cReinforcementandimitationlearningfordiversevisuomotor skills,\u201d in Proc. Robot.: Sci. Syst. XIV, vol. 14, 2018. [Online]. Available: http://arxiv.org/abs/1802.09564 [27] D. Xu, S. Nair, Y. Zhu, J. Gao, A. Garg, L. Fei-Fei, and S. Savarese, \u201cNeural task programming: Learning to generalize across hierarchical tasks,\u201d in Proc. Int. Conf. Robot. Autom. 2018, pp. 1\u20138. [28] Y.Aytar,T.Pfaff,D.Budden,T.Paine,Z.Wang,and N.de Freitas,\u201cPlaying hardexplorationgamesbywatchingyoutube,\u201dinProc.Adv.NeuralInform. Process. Syst., 2018, pp. 2935\u20132945. [29] A. Hundt, V. Jain, C.-H. Lin, C. Paxton, and G. D. Hager, \u201cThe costar block stacking dataset: Learning with workspace constraints,\u201d Intell. Robots Syst. (IROS), 2019 IEEE Int. Conf., 2019. [Online]. Available: https://arxiv.org/ abs/1810.11714 [30] J. Andreas, D. Klein, and S. Levine, \u201cModular multitask reinforcement learning with policy sketches,\u201d in ICML\u201917 Proc. 34th Int. Conf. Mach. Learning - Volume 70, 2017, pp. 166\u2013175. [31] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, \u201cLearning modular neural network policies for multi-task and multi-robot transfer,\u201d in Proc. Int. Conf. Robot. Autom. 2017, pp. 2169\u20132176. [32] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman, and D. Man, \u201cConcrete problems in ai safety,\u201d 2016, arXiv:1606.06565. [33] J. Leike, M. Martic, V. Krakovna, P. A. Ortega, T. Everitt, A. Lefrancq, L. Orseau, and S. Legg, \u201cAi safety gridworlds,\u201d arXiv:1711.09883, 2017. [34] T. Everitt, G. Lea, and M. Hutter, \u201cAGI safety literature review,\u201d in Proc. 27th Int. Joint Conf. Artif. Intell., 2018, pp. 5441\u20135449. [35] M. Hessel et al., \u201cRainBow: Combining improvements in deep reinforcement learning,\u201d in Proc. AAAI, 2018, pp. 3215\u20133222. [36] Y.Zhuetal.,\u201cReinforcementandimitationlearningfordiversevisuomotor skills,\u201d in Proc. Robot.: Sci. Syst. XIV, vol. 14, 2018. [37] J. Matas, S. James, and A. J. Davison, \u201cSim-to-real reinforcement learning for deformable object manipulation,\u201d Conf. Robot Learn., pp. 734\u2013743, 2018. [38] J. Mahler and K. Goldberg, \u201cLearning deep policies for robot bin picking by simulating robust grasping sequences,\u201d Conf. Robot Learn., pp. 515\u2013524, 2017. [39] A. Y. Ng, D. Harada, and S. J. Russell, \u201cPolicy invariance under reward transformations: Theory and application to reward shaping,\u201d in Proc. Int. Conf. Mach. Learn., 1999, pp. 278\u2013287. [40] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience replay,\u201d in ICLR 2016 : Int. Conf. Learning Representations 2016, 2016. [41] D. Kalashnikov et al., \u201cScalable deep reinforcement learning for visionbased robotic manipulation,\u201d in Proc. Conf. Robot. Learn., 2018. [42] M. Chevalier-Boisvert, L. Willems, and S. Pal, \u201cMinimalistic gridworld environment for openai gym,\u201d [Online]. Available https://github.com/ maximecb/gym-minigrid, 2018. [43] M. Andrychowicz et al., \u201cHindsight experience replay,\u201d in Proc. Adv. Neural Inform. Process. Syst., 2017, pp. 5048\u20135058. [44] C. Paxton, F. Jonathan, A. Hundt, B. Mutlu, and G. D. Hager, \u201cEvaluating methods for end-user creation of robot task plans,\u201d in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS). 2018, pp. 6086\u20136092. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  ", "title": "\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer", "paper_info": "6724\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 5, NO. 4, OCTOBER 2020\n\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for\nMulti-Step Visual Tasks with Sim to Real Transfer\nAndrew Hundt\n, Benjamin Killeen\n, Nicholas Greene, Hongtao Wu\n, Heeyeon Kwon,\nChris Paxton\n, and Gregory D. Hager\nAbstract\u2014Current Reinforcement Learning (RL) algorithms\nstruggle with long-horizon tasks where time can be wasted explor-\ning dead ends and task progress may be easily reversed. We develop\nthe SPOT framework, which explores within action safety zones,\nlearns about unsafe regions without exploring them, and prioritizes\nexperiences that reverse earlier progress to learn with remarkable\nef\ufb01ciency. The SPOT framework successfully completes simulated\ntrials of a variety of tasks, improving a baseline trial success rate\nfrom 13% to 100% when stacking 4 cubes, from 13% to 99% when\ncreating rows of 4 cubes, and from 84% to 95% when clearing toys\narranged in adversarial patterns. Ef\ufb01ciency with respect to actions\nper trial typically improves by 30% or more, while training takes\njust 1-20 k actions, depending on the task. Furthermore, we demon-\nstrate direct sim to real transfer. We are able to create real stacks\nin 100% of trials with 61% ef\ufb01ciency and real rows in 100% of\ntrialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained\nmodel on the real robot with no additional real-world \ufb01ne-tuning.\nToourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning\nwith successful sim to real transfer applied to long term multi-step\ntasks such as block-stacking and row-making with consideration\nof progress reversal. Code is available at https://github.com/jhu-\nlcsr/good_robot.\nIndex Terms\u2014Computer vision for other robotic applications,\ndeep learning in grasping and manipulation, reinforcement\nlearning.\nI. INTRODUCTION\nM\nULTI-STEP robotic tasks in real-world settings are no-\ntoriously challenging to learn. They intertwine learning\nthe immediate physical consequences of actions with the need\nto understand how these consequences affect progress towards\nthe overall goal. Furthermore, in contrast to traditional motion\nplanning, which assumes perfect information and known action\nmodels, learning only has access to the spatially and temporally\nlimited information from sensing the environment.\nManuscript received February 24, 2020; accepted July 20, 2020. Date of\npublication August 11, 2020; date of current version August 27, 2020. This letter\nwas recommended for publication by Associate Editor J. Kober and Editor T.\nAsfour upon evaluation of the reviewers\u2019. comments. This work was supported\nby the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce\nof Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew\nHundt.)\nAndrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon\nKwon, and Gregory D. Hager are with The Johns Hopkins University,\nBaltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu;\nngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu).\nChris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail:\ncpaxton@nvidia.com).\nThis article has supplementary downloadable material available at https://\nieeexplore.ieee.org, provided by the authors.\nDigital Object Identi\ufb01er 10.1109/LRA.2020.3015448\nFig. 1.\nRobot-created stacks and rows of cubes with sim to real transfer. Our\nSchedule for Positive Task (SPOT) framework allows us to ef\ufb01ciently\n\ufb01nd\npolicies\nwhich\ncan\ncomplete\nmulti-step\ntasks.\nVideo\noverview:\nhttps://youtu.be/MbCuEZadkIw\nOur key observation is that reinforcement learning wastes sig-\nni\ufb01cant time exploring actions which are unproductive at best.\nFor example, in a block stacking task (Fig. 1), the knowledge\nthat grasping at empty air will never snag an object is \u201ccommon\nsense\u201d for humans, but may take some time for a vanilla algo-\nrithm to discover. To address this, we propose the Schedule for\nPositive Task (SPOT) framework, which incorporates common\nsense constraints in a way that signi\ufb01cantly accelerates both\nlearning and \ufb01nal task ef\ufb01ciency.\nWhile these types of constraints are intuitive, incorporating\nthem into Deep RL (DRL) in a manner that leads to reliable and\nef\ufb01cient learning is nontrivial [1], [2]. Our methods (Section III)\ntakeinspirationfromahumaneandeffectiveapproachtotraining\npets sometimes called \u201cPositive Conditioning.\u201d Consider the\ngoal of training a dog \u201cSpot\u201d to ignore an object or event she\n\ufb01nds particularly interesting on command. Spot is rewarded with\ntreats whenever partial compliance with the desired end behavior\nis shown, and simply removed from regressive situations with\nzero treats (reward). One way to achieve this is to start with\nmultiple treats in hand, place one treat in view of Spot, and, if\nshe eagerly jumps at the treat (a negative action), the human\nsnatches and hides the treat immediately for zero reward on\nthat action. With repetition, Spot will eventually hesitate, and so\nshe is immediately praised with \u201cGood Spot!\u201d and gets a treat\n2377-3766 \u00a9 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply. \n\u4ee3\u7801\u5f00\u6e90\u4e86\uff0c\u5f3a\u5316\n\u7528\u7684\u662f\u57fa\u7840\u63a7\u5236\u7b56\u7565\uff08pick place push\uff09\uff0c\u4ee5\u53ca\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u3002\n\u51e0\u5343\u6b21\u5c31\u53ef\u4ee5\u5b66\u4f1a\uff01\n"}
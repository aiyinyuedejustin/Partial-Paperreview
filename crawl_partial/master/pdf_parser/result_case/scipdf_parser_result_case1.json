{"title": "", "authors": "Andrew Hundt; Benjamin Killeen; Nicholas Greene; Hongtao Wu; Heeyeon Kwon; Chris Paxton; Gregory D Hager", "pub_date": "", "abstract": "Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable efficiency. The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Efficiency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20 k actions, depending on the task. Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% efficiency and real rows in 100% of trials with 59% efficiency by directly loading the simulation-trained model on the real robot with no additional real-world fine-tuning. To our knowledge, this is the first instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhulcsr/good_robot. Index Terms-Computer vision for other robotic applications, deep learning in grasping and manipulation, reinforcement learning.M ULTI-STEP robotic tasks in real-world settings are notoriously challenging to learn. They intertwine learning the immediate physical consequences of actions with the need to understand how these consequences affect progress towards the overall goal. Furthermore, in contrast to traditional motion planning, which assumes perfect information and known action models, learning only has access to the spatially and temporally limited information from sensing the environment.", "sections": [{"heading": "", "text": "\"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer Fig. 1. Robot-created stacks and rows of cubes with sim to real transfer. Our Schedule for Positive Task (SPOT) framework allows us to efficiently find policies which can complete multi-step tasks. Video overview: https://youtu.be/MbCuEZadkIw\nOur key observation is that reinforcement learning wastes significant time exploring actions which are unproductive at best. For example, in a block stacking task (Fig. 1), the knowledge that grasping at empty air will never snag an object is \"common sense\" for humans, but may take some time for a vanilla algorithm to discover. To address this, we propose the Schedule for Positive Task (SPOT) framework, which incorporates common sense constraints in a way that significantly accelerates both learning and final task efficiency.\nWhile these types of constraints are intuitive, incorporating them into Deep RL (DRL) in a manner that leads to reliable and efficient learning is nontrivial [1], [2]. Our methods (Section III) take inspiration from a humane and effective approach to training pets sometimes called \"Positive Conditioning.\" Consider the goal of training a dog \"Spot\" to ignore an object or event she finds particularly interesting on command. Spot is rewarded with treats whenever partial compliance with the desired end behavior is shown, and simply removed from regressive situations with zero treats (reward). One way to achieve this is to start with multiple treats in hand, place one treat in view of Spot, and, if she eagerly jumps at the treat (a negative action), the human snatches and hides the treat immediately for zero reward on that action. With repetition, Spot will eventually hesitate, and so she is immediately praised with \"Good Spot!\" and gets a treat  8) with an arrow to the corresponding height map coordinate, showing how these values are transformed to a gripper pose. The rotated overhead views overlay the Q value at each pixel from dark blue values near 0 to red for high probabilities. If you take a moment to compare the Q values of a single object across all actions (green arrows identify the same object across two oriented views) you will see each object is scored in a way which leads to a successful stack in accordance with its surrounding context. For example, the grasp model learns to give a high score to the lone unstacked red block for grasp actions and a low score to the yellow top of the stack, while the place model does the reverse. Here the model chooses to grasp the red block and place on the yellow, blue, and green stack. Experiment details are in Sections IV and V.\nseparate from the one she should ignore. This approach can be expanded to new situations and behaviors, and it encourages exploration and rapid improvement once an initial partial success is achieved. As we describe in Section III, our reward functions and SPOT-Q Learning are likewise designed to provide neither reward nor punishment for actions that reverse progress.\nInstances of progress reversal are associated with varying complexity. On the one hand, failing to stack the first block on top of another leaves the robot in a similar situation, so recovery takes \u03a9(1) actions. However, once a stack of n blocks exists, even a successful grasp might knock the whole stack down, reversing the entire history of actions for a given trial ( Fig. 3), so recovery is \u03a9(n). The latter, more dramatic instance of progress reversal is a challenging problem for reinforcement learning of multi-step tasks in robotics; our work provides a method for efficiently solving such cases.\nIn summary, our contributions in this article are:\n1) The overall SPOT framework for reinforcement learning of multi-step tasks, which improves on state of the art in simulation and can train efficiently on real-world situations. 2) SPOT-Q Learning, a method for safe and efficient training in which a mask focuses exploration at runtime and generates extra on-the-fly training examples from past experience during replay. 3) State of the art zero-shot domain transfer from simulated stacking and row building tasks to their real world counterparts, as well as robustness with respect to a change in hardware and scene positions. Ideally algorithms should efficiently learn to prevent this situation and succeed as indicated by the green arrows. Thus, temporal and workspace dependencies must be considered. Events at a current time t i \u2208 T, i \u2208 [1...n] can influence the likelihood of successful outcomes for past actions t h |h < i and future actions t j |j > i. A successful choice of action at any given t i will ensure both past and future actions are productive contributors to the larger task at hand. In our experiments a partial stack or row is itself a scene obstacle. The gray wall pictured here is for illustrative purposes only.\n4) An ablation study showing that Situation Removal dramatically decreases progress reversal; that a progress metric increases efficiency; and that trial rewards improve on discounting, but involve a trade-off between efficiency and support for sparse rewards.", "n_publication_ref": 2, "n_figure_ref": 3}, {"heading": "II. RELATED WORK", "text": "Deep Neural Networks (DNNs) have enabled the use of raw sensor data in robotic manipulation [1]- [5]. In some approaches, a DNN's output directly corresponds to motor commands, e.g., [3], [4]. Higher-level methods, on the other hand, assume a simple model for robotic control and focus on bounding box or pose detection for downstream grasp planning [1], [6]- [11]. RGB-D sensors can be beneficial [1], [11], [12], as they capture physical information about the workspace. Object-centric skill learning can be effective and generalize well, e.g., [13]- [16] focus on block stacking by classifying simulated stacks as stable or likely to fall. Similarly, [17], [18] develop physical intuition by predicting push action outcomes. Our work differs by developing visual understanding and physical intuition in concert with the progress of multi-step tasks.\nGrasping is a particularly active area of research. DexNet [19], [20] learns from a large number of depth images of top-down grasps, and gets extremely good performance on grasping novel objects but does not look at long-horizon tasks. 6-DOF Grasp-Net [21] uses simulated grasp data to generalize to new objects and has been extended to handle reliable grasping of novel objects in clutter [12].\nDRL has proven effective at increasingly complex tasks in robotic manipulation [1], [5], [22], [23]. QT-Opt [5] learns manipulation skills from hundreds of thousands of real-world grasp attempts on real robots. Domain Adaptation, such as applying random textures in simulation, can also enhance sim to real transfer [24], [25]. Other methods focus on transferring visuomotor skills from simulated to real robots [22], [26]. Our work directs a low-level controller to perform actions rather than regressing torque vectors directly, following prior work [1], [23] by learning a pixel-wise success likelihood map.\nMulti-step tasks with sparse rewards present a particular challenge in reinforcement learning because solutions are less likely to be discovered through random exploration. When available, demonstration can be an effective method for guiding exploration [27]- [29]. Multi-step tasks can be split into modular sub-tasks comprising a sketch [30], while [31] has robot-specific and task-specific learning modules.\nSafety is crucial for reinforcement learning in many realworld settings [32]- [34]. The preliminary experiments in Section IV-D show that SPOT-Q provides a way to incorporate safety into general Q-Learning based algorithms [35].\nWe compare the SPOT framework to VPG [1], a method for RL-based table clearing tasks which can be trained from images within hours on a single robot, in Sections IV and V. VPG is frequently able to complete adversarial scenarios like first pushing a tightly packed group of blocks apart and then grasping the now-separated objects. Some of the most closely related recent work involves tasks with multiple actions: [36] includes placing one block on another, [37] places one towel on a bar, and [38] clears a bin, but the first two are not long-horizon tasks and the possibility of progress reversal (Fig. 3) is never considered.", "n_publication_ref": 40, "n_figure_ref": 1}, {"heading": "III. APPROACH", "text": "We investigate multi-step tasks for which there is a sparse and approximate notion of task progress. It is possible to improve the efficiency of learning by taking these four measures: structuring such problems to capture invariant properties of the data, deploying traditional algorithms where they are most effective, ensuring rewards do not propagate through failed actions, and introducing an algorithm which removes unnecessary exploration. We will later demonstrate our approach in the context of the general problem of assembly through vision-based robotic manipulation.\nWe frame the problem as a Markov Decision Process (S, A, P, R), with state space S, action space A, transition probability function P : S \u00d7 S \u00d7 A \u2192 R, and reward function R : S \u00d7 A \u2192 R. This includes a simplifying assumption equating sensor observations and state. At time step t, the agent observes state s t and chooses an action a t according to its policy \u03c0 : S \u2192 A. The action results in a new state s t+1 with probability P (s t+1 |s t , a t ). As in VPG [1], we use Q-learning to produce a deterministic policy for choosing actions. The function Q : S \u00d7 A \u2192 R estimates the expected reward R of an action from a given state, i.e. the \"quality\" of an action. Our policy \u03c0 selects an action a t as follows:\n\u03c0(s t ) = arg max a\u2208A Q(s t , a) (1)\nThus, the goal of training is to learn a Q that maximizes R over time. This is accomplished by iteratively minimizing |Q(s t , a t ) \u2212 y t |, where the target value y t is:\ny t = R(s t+1 , a t ) + \u03b3Q(s t+1 , \u03c0(s t+1 ))(2)\nQ-learning is a fundamental algorithm in RL, but there are key limitations in its most general form for applications like robotics where the space and cost of actions and new trials is extremely large, and efficient exploration can be essential or even safety critical. It is also highly dependent on R, whose definition can cause learning efficiency to vary by orders of magnitude, as we show in Section IV-C, and so we begin with our approach to reward shaping.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A. Reward Shaping", "text": "Reward shaping is an effective technique for optimizing a reward R, to train policies [39] and their neural networks efficiently. Here, we present several reward functions for later comparison (Section IV-C), which build towards a general formulation for reward shaping conducive to efficient learning on a broad range of novel tasks, thus reducing the ad hoc nature of successful reward schedules.\nSuppose each action a is associated with a sub-task \u03c6 \u2208 \u03a6 and that we have an indicator function 1 a [s t+1 , a t ] which equals 1 if an action a t succeeds at \u03c6 and 0 otherwise. 1 As in VPG [1], our baseline rewards follow this principle and include a sub-task weighting function W : \u03a6 \u2192 R, according to their subjective difficulty and importance: 2\nR base (s t+1 , a t ) = W (\u03c6 t )1 a [s t+1 , a t ].(3)\nNext, we define a sparse and approximate task progress function P : S \u2192 R \u2208 [0, 1], indicating proportional progress towards an overall goal, where P(s t ) = 1 means the task is complete. 3 As in our story of Spot the dog (Section I), a progress reversal leads us to perform Situation Removal (SR) on the agent and 1 Examples of action indicator sources include the grasp detector in our Robotiq 2F85 gripper, human supervision, or another detection algorithm. 2 In our experiments we assign simple values for each successful action type: 3 In our block tasks P is the height of the stack or length of the row vs the goal size, in table clearing either the number of objects or occupied pixels vs the total, and in navigation the remaining vs initial distance.  (6), and the SPOT Progress Reward R P (5) with images of key action steps. Actions 1-3: a 1 is an initial grasp, followed by a successful place where a slightly off balance stack of height 2 is formed. Actions 4-5: Progress reversal occurs when a grasp then place knocks the stack over, so the reward values go to zero. Action 7: While not pictured, the scene is similar to a 3 but with a better balanced top block. Intuitively, since a 9 doesn't topple like a 5 a better reward at a 7 would be appropriate, which is one advantage of R trial over R P , because R P (s 4 , a 3 ) = R P (s 8 , a 7 ) and R trial (s 4 , a 3 ) < R trial (s 8 , a 7 ) because a 7 leads directly to a successful stack.\nW \u03c6 t \u2208 {W push =0.1, W grasp =1, W place =1}.\nActions 11-14: Grasp and place actions lead to a full stack of 4 completing the trial. The final R trial at a 14 is 2 \u00d7 R P . Here W \u03c6 t \u2208 {W push = .5, W grasp = 1, W place =1.25} for chart visibility.\nphysically reset the environment during training (Fig. 3). We define an associated indicator 1 SR [s t , s t+1 ], which equals 1 if P(s t+1 ) \u2265 P(s t ) and 0 otherwise. These lead to new reward functions:\nR SR (s t+1 , a t ) = 1 SR [s t , s t+1 ]R base (s t+1 , a t )(4)\nR P (s t+1 , a t ) = P(s t+1 )R SR (s t+1 , a t )(5)\nOne advantage of R base , R SR , and R P is that each is available \"instantaneously\" in the midst of a trial after two state transitions. However, they do not consider the possibility that an early mistake might lead to failure many steps down the line (Fig. 3,  4), and so we will develop a reward which propagates across whole trials.", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "B. Situation Removal: SPOT Trial Reward", "text": "Is it possible for a reward function to account for actions which lead to failures at a later time step while still training more efficiently than a standard discounted reward R D where R D (s t+1 , a t ) = \u03b3 R D (s t+2 , a t+1 )? Our approach is to block reward propagation through failed actions via the Situation Removal concept: where R * can be an arbitrary instant reward function such as R SR or R P from Section III-A, N marks the end of the trial, and \u03b3 is the usual discount factor which is set to \u03b3 = 0.65.\nR trial (s t+1 , a t ) = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 0, if R * (s t+1 , a t ) = 0 2R * (s t+1 , a t ), if t = N R * (s t+1 , a t ) + \u03b3R trial (s t+2 , a t+1 ), otherwise(\nThe effect of using R trial is that future rewards only propagate across time steps where subtasks are completed successfully. As illustrated in Fig. 4 and described in the caption, the zero reward from situation removal cuts the propagation of future rewards back through time steps containing failed actions. This focuses learning on short and successful sequences that complete a task.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "C. SPOT-Q Learning and Dynamic Action Spaces", "text": "In this section, we go a step further and leverage a priori knowledge about the environment to make simple but powerful assumptions which both reduce unproductive attempts and accelerate training. Specifically, there are many occasions when certain action failures are easily predicted from the same sensor signal used for Q learning. To this end, we assume the existence of an oracle, M (s t , a) \u2192 {0, 1}, which takes the current state s t and an action a and returns 0 if an action is certain to fail, and 1 otherwise. This is subtly different from the success indicator 1 a [s t+1 , a t ], which requires the outcome s t+1 of an action a t to determine success or failure. 4 Using M , we define the dynamic action space M t (A):\nM t (A) = {a \u2208 A|M (s t , a) = 1}.(7)\nIn short, M t (A) does not tell us whether a \u2208 A is an action worth taking, but rather whether it is worth exploring. Given a state s t , the question becomes how to most effectively utilize M t in training. If \u03c0(s t ) \u2208 M t (A), then \u03c0(s t ) can be treated as a failure for the purposes of learning and we can explore the next best action not guaranteed to fail. To formalize this, we introduce SPOT-Q Learning which is a new target value function replacing (2): where \u03c0 M (s t ) = arg max a\u2208M t (A) Q(s t , a). Crucially, we perform backpropagation on both the masked action, which has 0 reward, and the unmasked action \u03c0 M (s t ), which the robot actually performs. Algorithm 1 describes how we continuously train from past examples with SPOT-Q and Prioritized Experienced Replay (PER) [40] as the current policy is rolled out. In Section IV, we discuss how SPOT-Q allows us to surpass prior work, wherein similar heuristics [1], [41] neither match SPOT-Q nor account for the safety considerations we discuss later.\ny M,t = \u23a7 \u23a8 \u23a9 y t , if \u03c0(s t+1 ) \u2208 M t (A) y t + \u03b3Q(s t+1 , \u03c0 M (s t+1 )) otherwise + R(s t+1 , a t ),(8)", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "IV. SIMULATION EXPERIMENTS", "text": "Our method improves performance and action efficiency over the state of the art on the table clearing task from VPG [1], as well as on two challenging multi-step tasks of our design: creating a stack of four blocks and creating a horizontal row of four blocks. Our best results can achieve 100% trial success on the simulated stacking and row tasks, models which successfully transfer to the real world as we show in Section V.\nWe detail a series of simulation experiments to understand the contribution of each element of our approach to this overall performance. To do so, we evaluate each reward function, the effect of SPOT-Q on heuristic exploration, other possible SPOT-Q implementations, the reward weighting term W , and then we describe our best results with SPOT-Q + R P and SPOT-Q + R trial . In brief, we find that Situation Removal R SR is the largest contributor to our improved performance, R P improves accuracy and efficiency, and R trial trains more efficiently than discounted rewards while accounting for a time delay between actions and consequences. SPOT-Q improves results over no masking, and over basic masking on its own. Finally, we test a grid world navigation task [42] to show how the SPOT framework applies to safe reinforcement learning. Tables I and III summarize these results.  ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A. Robot Implementation Details", "text": "We consider a robot capable of being commanded to a specified arm pose and gripper state in its workspace. Our action space consists of three components: action types \u03a6, locations X \u00d7 Y , and angles \u0398. The agent observes the environment via a fixed RGB-D camera, which we project so that z is aligned with the direction of gravity, as shown in Fig. 2. We discretize the spatial action space into a square height map with 0.448m on a side and 224 \u00d7 224 bins with coordinates (x, y), so each pixel represents roughly 4mm 2 as per VPG [1]. The angle space\n\u0398 = { 2\u03c0i k |i \u2208 [0, k \u2212 1]} is similarly discretized into k = 16 bins.\nThe set of action types consists of three high-level motion primitives \u03a6 = {grasp, push, place}. In our experiments action success is determined by our gripper's sensor for grasp, object perturbations for push, and an increase in stack height or row length for place.\nA traditional trajectory planner executes each action a = (\u03c6, x, y, \u03b8) \u2208 A on the robot. For grasping and placing, each action moves to (x, y) with gripper angle \u03b8 \u2208 \u0398 and closes or opens the gripper, respectively. A push starts with the gripper closed at (x, y) and moves horizontally a fixed distance along angle \u03b8. Fig. 2 visualizes our overall algorithm, including the action space and corresponding Q-values.", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "B. Evaluation Metrics", "text": "We evaluate our algorithms in randomized test cases in accordance with the metrics found in VPG [1]. Ideal Action Efficiency is 100% and calculated as Ideal/Actual action count; defined as 1 action per object for grasping tasks; and 2 actions per object for tasks which involve placement. This means 6 total actions for a stack of height 4 since only 3 objects must move, and 4 total actions for rows by placing two blocks between two endpoints. We validate simulated results twice with 100 trials of novel random object positions.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "C. Algorithm Ablation", "text": "We compare the contribution from each component of the underlying algorithm and against baseline approaches in Table I, except for clearing tasks which are provided in the text. Unless otherwise stated we summarize rows and stacks together as a combined average below.\nClear 20 Toys: We establish a baseline via the primary simulated experiment found in VPG [1], where 20 toys with varied shapes must be grasped to clear the robot workspace. The SPOT framework matches VPG [1] with 100% task completion and improves both the rate of grasp successes from 68% to 84% and action efficiency from 64% to 74%.\nClear Toys Adversarial: The second baseline scenario is the 11 cases of challenging adversarial arrangements from VPG [1], where toys are placed in tightly packed configurations. Each case is run 10 times and the SPOT framework completely clears 7/11 cases compared to 5/11 in VPG [1]; the clearance rate across all 110 runs improves to 95% from 84%. Efficiency in this case drops from 60% to 38%, which is accounted for by the increase in the number of difficult cases solved, as separating the blocks can take several attempts.\nReward Functions: R base , R SR , R P , and R trial incrementally extend one another (Section III-A, III-B). All masking is disabled for this study unless otherwise indicated.\nR D s.t. R D (s t+1 , a t ) = \u03b3 R D (s t+2 , a t+1 ) is discounting, the most conventional approach to trial rewards. When evaluated with R P at the final time step and \u03b3 = 0.9, grasp and place actions succeed at a rate of 5% and 45%, respectively. Stacks of height 2-3 are created and performance improves with masking (32%, 48%). However, this approach is incredibly inefficient with no stacks of 4 within 20 k actions. That said, we would expect convergence if orders of magnitude more training were feasible [43].\nR base is effective for pushing and grasping [1], but it is not sufficient for multi-step tasks, only completing 13% of rows and stacks with about 200+ actions per trial in the best case. In another case, it repeatedly reverses progress by often looping grasping then placing of the same object at one spot, leading to 99% successful grasps but 0 successful trials overall, even after manual scene resets. We do not expect R base to converge on these tasks as there is no progress signal to indicate, for example, that grasping from the top of an existing stack is a poor choice.\nR SR resolves the progress reversal problem immediately since such actions get 0 reward; and thus we see an astounding increase in trial successes from 13% to 94%, and an order of magnitude efficiency increase to 23% across both tasks, or about 22 actions per trial.\nR P leads to a rise in combined trial successes to 97%, and efficiency to 45%, or about 20 actions per trial. This improves upon pure situation removal by incorporating the quantitative amount of progress.\nR trial utilizes R P as the instant reward function in this test, and has an average trial success rate of 96% for stacks and efficiency of 31%, or about 19 actions per trial. However, performance degrades significantly for rows, declining to an 80% trial success rate and just 16% action efficiency, or about 25 actions per trial. These values indicate R trial strikes a trade-off between the inefficiency of R D and the need for a more instantaneous progress metric in R P , as the most recent value can be utilized to fill actions with no progress feedback. We also note that once SPOT-Q is added this reward is the best for stacking and second best overall, as we show below.\nSPOT-Q: VPG [1] evaluated heuristics that specify exact locations to explore, and they found it led to worse performance. A similar approach in QT-Opt [41] is phased out as training proceeds, indicating that their methods do not contribute to improving outcomes throughout the training process. By contrast, SPOT-Q is enabled at all times and excises regions with zero likelihood of success, while other regions of interest remain open for exploration. So does this difference in heuristic design matter?\nThe \"Mask but no SPOT-Q\" test disables the if statement in Alg. 1 to simulate a typical heuristic in which exploration is directed to particular regions without zero reward guidance. \"Mask but no SPOT-Q\" completes 95% of trials, compared to 88% without masking and 99% with SPOT-Q; action efficiency results are even more pronounced at 37%, 23%, and 50% respectively. Both these results and Section IV-D show SPOT-Q simply works throughout training and testing with little to no tuning, and so we conclude that SPOT-Q improves the efficiency of learning from heuristic data.\nSPOT-Q Alternatives: We evaluated two alternatives to SPOT-Q (eq. 8, Alg. 1), where 0 reward backpropagation is performed on all masked pixels with loss applied to the (1) sum, and (2) average of the masked scores in addition to the actually executed action. In both cases, the gradients exploded and the algorithm did not converge. Only SPOT-Q is able to efficiently enhance convergence.\nReward Weighting: SPOT-Q + R P where W push = 0.1 succeeds in 99% of trials, but just 27% when W push = 1.0. The weighting in Fig. 4 on R trial without masking or SPOT-Q achieves 97% stack success and 38% action efficiency, but we leave all weighting constant for consistency in Table I. This shows W (3) is important for efficient training.\nSPOT-Q + R P : This configuration has the best overall simulation performance with a 99% trial success rate and 50% efficiency, or about 10 actions per trial. It is also the best simulated row model with 98% trial success in one test and 100% in the second, with a high 62-68% action efficiency.\nSPOT-Q + R trial : This has the best stacking model with 100% completion in both test cases, and 45-51% efficiency. Overall performance is the second best with 97% trial success, and 37% efficiency, or about 14 actions per trial.", "n_publication_ref": 8, "n_figure_ref": 1}, {"heading": "D. Safety and Domain Generalization", "text": "To demonstrate the broad scope of the SPOT framework, we evaluate it on the simple but challenging Safety Grid World [42] (Fig. 5), an environment type widely used to evaluate RL algorithms [32], [39]. Here the red robot must move forward or turn as it navigates towards the green square without ever entering the lava. If we had just one real robot to learn within this world, standard DRL would be extremely unsafe, but the SPOT framework allows the robot to safely explore the space.\nAs Table III shows, all improvements are consistent with our more realistic tasks. We start with Rainbow [35], a Q learning based DRL method, which only completes at most 12% of trials within 500 k actions with a 12% efficiency. We then perform a small ablation study, successively adding Masking, SPOT-Q, and R P to Rainbow; 96.9%, 95.5%, and 99.9% of 1000 test trials are completed, respectively; average efficiency is 75%, 73%, and 62%, respectively; and the average number of actions to complete 100% of 30 validation trials is 123 k, 113 k, and 70 k, respectively. 5 All failures with a mask did not enter the lava, they hit a 100 action limit.\nThese results are consistent with our more realistic experiments, demonstrate how the SPOT framework generalizes across completely different scenarios, and illustrate the application of the SPOT framework to safe exploration. Next, we demonstrate how the SPOT framework leverages knowledge acquired in simulation directly on a real robot task.", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "V. REAL WORLD EXPERIMENTS", "text": "Finally, we examine the performance of SPOT-Q on real robot tasks, both via training from scratch and sim to real transfer. In both cases, performance was roughly equivalent to that achieved in simulation, which shows the strength of our approach for efficient and effective reinforcement learning. We use the setup described in [29], [44], including a Universal Robot UR5, a Robotiq 2-finger gripper, and a Primesense Carmine RGB-D camera; all but the arm differ from those in our simulation. Other implementation details are as described in Section IV-A, and results are in Table II.\nReal Pushing and Grasping: We train the baseline pushing and grasping task from scratch in the real world, test with 20 objects and see 100% test clearance, 75% grasp success rate, and 75% efficiency in 1 k actions; these results are comparable to the performance charted by VPG [1] over 2.5 k actions. Sim to real does not succeed in this task.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Sim to Real vs Real Stacking:", "text": "After training in simulation we directly load the model for execution on the real robot. Remarkably, all tested sim to real stacking models complete 100% of trials, outperforming a model trained on the real robot which is successful in 82% of trials (Fig. 6, Table II). R P and R trial have an equal action efficiency at 61%, and the version of R P without SPOT-Q or a mask exhibits slightly lower efficiency at 51%. This is particularly impressive considering that our scene is exposed to variable sunlight. Intuitively, these results are in part due to the depth heightmap input in stacking and row-making.\nSim to Real Rows: Our R P + SPOT-Q sim to real rows model is also able to create rows in a remarkable 100% of attempts with 59% efficiency. R trial + SPOT-Q and R P with no mask perform slightly worse, both with 90% of trials complete, and an efficiency of 83% and 58%, respectively. The high efficiency of R P with no mask is because we end real trials immediately when the task becomes unrecoverable, such as when a block tumbles out of the workspace. We exclusively evaluate sim to real transfer in this case because training progress is significantly slower than with stacks.\nWe expect that block based tasks are able to transfer because the network relies primarily on the depth images, which are more consistent between simulated and real data. This might reasonably explain why pushing and grasping does not transfer, a problem which could be mitigated in future work with methods like Domain Adaptation [24], [25].", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "VI. CONCLUSION", "text": "We have demonstrated that the SPOT framework is effective for training long-horizon tasks. To our knowledge, this is the first instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and creating rows with consideration of progress reversal. The SPOT framework quantifies an agent's progress within multi-step tasks while also providing zero-reward guidance, a masked action space, and situation removal. It is able to quickly learn policies that generalize from simulation to the real world. We find these methods are necessary to achieve a 100% completion rate on both the real block stacking task and the row-making task.\nSPOT's main limitation is that while intermediate rewards can be sparse, they are still necessary. Future research should look at ways of learning task structures that incorporate situation removal from data. In addition, the action space mask M is currently manually designed; this and the lower-level open loop actions might be learned as well. Another topic for investigation is the difference underlying successful sim to real transfer of stacking and row tasks when compared to pushing and grasping. Finally, in the future, we would like to apply our method to more challenging tasks.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENT", "text": "We extend our thanks to Adit Murali for Safety Grid World integration; to Molly O'Brien for valuable discussions, feedback, and editing; to Corinne Hundt for the \"Good Robot!\" title copywriting; to Michelle Hundt, Thomas Hundt, and Ian Harkins for editing; to all those who gave their time for reading, reviewing, and feedback; and to the VPG [1] authors for releasing their code.", "n_publication_ref": 1, "n_figure_ref": 0}], "references": [{"title": "Learning synergies between pushing and grasping with self-supervised deep reinforcement learning", "journal": "", "year": "2018", "authors": "A Zeng; S Song; S Welker; J Lee; A Rodriguez; T Funkhouser"}, {"title": "A review of robot learning for manipulation: Challenges, representations, and algorithms", "journal": "", "year": "2019", "authors": "O Kroemer; S Niekum; G D Konidaris"}, {"title": "End-to-end training of deep visuomotor policies", "journal": "The J. Mach. Learn. Res", "year": "2016", "authors": "S Levine; C Finn; T Darrell; P Abbeel"}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and largescale data collection", "journal": "Int. J. Robot. Res", "year": "2018", "authors": "S Levine; P Pastor; A Krizhevsky; J Ibarz; D Quillen"}, {"title": "QT-OPT: Scalable deep reinforcement learning for vision-based robotic manipulation", "journal": "CoRL", "year": "2018", "authors": "D Kalashnikov"}, {"title": "Real-time grasp detection using convolutional neural networks", "journal": "", "year": "2015", "authors": "J Redmon; A Angelova"}, {"title": "Model globally, match locally: Efficient and robust 3D object recognition", "journal": "", "year": "2010", "authors": "B Drost; M Ulrich; N Navab; S Ilic"}, {"title": "Robotic grasp detection using deep convolutional neural networks", "journal": "Int. Conf. Intell. Robot. Syst", "year": "2017-09", "authors": "S Kumra; C Kanan"}, {"title": "End-toend learning of semantic grasping", "journal": "", "year": "2017", "authors": "E Jang; S Vijayanarasimhan; P Pastor; J Ibarz; S Levine"}, {"title": "Deep learning for detecting robotic grasps", "journal": "Int. J. Robot. Res", "year": "2015", "authors": "I Lenz; H Lee; A Saxena"}, {"title": "Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach", "journal": "RSS", "year": "2018", "authors": "D Morrison; J Leitner; P Corke"}, {"title": "6-DoF grasping for target-driven object manipulation in clutter", "journal": "Int. Conf. Robot. Autom", "year": "2020", "authors": "A Murali; A Mousavian; C Eppner; C Paxton; D Fox"}, {"title": "Learning dexterous manipulation for a soft robotic hand from human demonstrations", "journal": "IEEE", "year": "2016", "authors": "A Gupta; C Eppner; S Levine; P Abbeel"}, {"title": "Deep object-centric representations for generalizable robot learning", "journal": "", "year": "2018", "authors": "C Devin; P Abbeel; T Darrell; S Levine"}, {"title": "Learning physical intuition of block towers by example", "journal": "Int. Conf. Mach. Learn", "year": "2016", "authors": "A Lerer; S Gross; R Fergus"}, {"title": "ShapeStacks: Learning vision-based physical intuition for generalised object stacking", "journal": "", "year": "2018", "authors": "O Groth; F B Fuchs; I Posner; A Vedaldi"}, {"title": "Unsupervised learning for physical interaction through video prediction", "journal": "", "year": "2016", "authors": "C Finn; I J Goodfellow; S Levine"}, {"title": "SE3-Nets: Learning rigid body motion using deep neural networks", "journal": "", "year": "2017", "authors": "A Byravan; D Fox"}, {"title": "DEX-Net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics", "journal": "Proc. Robot.: Sci. Syst", "year": "2017", "authors": "J Mahler"}, {"title": "Learning ambidextrous robot grasping policies", "journal": "Sci. Robot", "year": "2019", "authors": "J Mahler"}, {"title": "6-DoF GraspNet: Variational grasp generation for object manipulation", "journal": "", "year": "2019", "authors": "A Mousavian; C Eppner; D Fox"}, {"title": "Modular deep Q networks for sim-to-real transfer of visuo-motor policies", "journal": "ACRA", "year": "2017", "authors": "F Zhang; J Leitner; M Milford; P Corke"}, {"title": "Available", "journal": "", "year": "", "authors": ""}, {"title": "Tossing-Bot: Learning to throw arbitrary objects with residual physics", "journal": "Proc. Robot.: Sci. Syst. XV", "year": "2019", "authors": "A Zeng; S Song; J Lee; A Rodriguez; T A Funkhouser"}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "journal": "", "year": "", "authors": "J Tobin; R Fong; A Ray; J Schneider; W Zaremba; P Abbeel"}, {"title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping", "journal": "", "year": "2018", "authors": "K Bousmalis"}, {"title": "Reinforcement and imitation learning for diverse visuomotor skills", "journal": "", "year": "2018", "authors": "Y Zhu"}, {"title": "Neural task programming: Learning to generalize across hierarchical tasks", "journal": "", "year": "2018", "authors": "D Xu; S Nair; Y Zhu; J Gao; A Garg; L Fei-Fei; S Savarese"}, {"title": "Playing hard exploration games by watching youtube", "journal": "", "year": "2018", "authors": "Y Aytar; T Pfaff; D Budden; T Paine; Z Wang; N De Freitas"}, {"title": "The costar block stacking dataset: Learning with workspace constraints", "journal": "Intell. Robots Syst. (IROS)", "year": "2019", "authors": "A Hundt; V Jain; C.-H Lin; C Paxton; G D Hager"}, {"title": "Modular multitask reinforcement learning with policy sketches", "journal": "", "year": "2017", "authors": "J Andreas; D Klein; S Levine"}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "journal": "", "year": "", "authors": "C Devin; A Gupta; T Darrell; P Abbeel; S Levine"}, {"title": "Concrete problems in ai safety", "journal": "", "year": "2016", "authors": "D Amodei; C Olah; J Steinhardt; P F Christiano; J Schulman; D Man"}, {"title": "Ai safety gridworlds", "journal": "", "year": "2017", "authors": "J Leike; M Martic; V Krakovna; P A Ortega; T Everitt; A Lefrancq; L Orseau; S Legg"}, {"title": "AGI safety literature review", "journal": "", "year": "2018", "authors": "T Everitt; G Lea; M Hutter"}, {"title": "RainBow: Combining improvements in deep reinforcement learning", "journal": "", "year": "2018", "authors": "M Hessel"}, {"title": "Reinforcement and imitation learning for diverse visuomotor skills", "journal": "", "year": "2018", "authors": "Y Zhu"}, {"title": "Sim-to-real reinforcement learning for deformable object manipulation", "journal": "", "year": "2018", "authors": "J Matas; S James; A J Davison"}, {"title": "Learning deep policies for robot bin picking by simulating robust grasping sequences", "journal": "", "year": "2017", "authors": "J Mahler; K Goldberg"}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "journal": "", "year": "1999", "authors": "A Y Ng; D Harada; S J Russell"}, {"title": "Prioritized experience replay", "journal": "", "year": "2016", "authors": "T Schaul; J Quan; I Antonoglou; D Silver"}, {"title": "Scalable deep reinforcement learning for visionbased robotic manipulation", "journal": "", "year": "2018", "authors": "D Kalashnikov"}, {"title": "Minimalistic gridworld environment for openai gym", "journal": "", "year": "2018", "authors": "M Chevalier-Boisvert; L Willems; S Pal"}, {"title": "Hindsight experience replay", "journal": "", "year": "2017", "authors": "M Andrychowicz"}, {"title": "Evaluating methods for end-user creation of robot task plans", "journal": "", "year": "2018", "authors": "C Paxton; F Jonathan; A Hundt; B Mutlu; G D Hager"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Fig. 2 .2Fig. 2. Our model architecture. Images are pre-rotated to 16 orientations \u03b8 before being passed to the network. Every coordinate a = (\u03c6, x, y, \u03b8) in the output pixel-wise Q-Values corresponds to a final gripper position, orientation, and open loop action type, respectively. Purple circles highlight the highest likelihood action arg max a (Q(s, M (a))) (8) with an arrow to the corresponding height map coordinate, showing how these values are transformed to a gripper pose. The rotated overhead views overlay the Q value at each pixel from dark blue values near 0 to red for high probabilities. If you take a moment to compare the Q values of a single object across all actions (green arrows identify the same object across two oriented views) you will see each object is scored in a way which leads to a successful stack in accordance with its surrounding context. For example, the grasp model learns to give a high score to the lone unstacked red block for grasp actions and a low score to the yellow top of the stack, while the place model does the reverse. Here the model chooses to grasp the red block and place on the yellow, blue, and green stack. Experiment details are in Sections IV and V.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Fig. 3 .3Fig.3. Red arrows show how individual successful actions can fail on the larger stacking task, forcing eventual progress reversal where a partial stack topples or the top must be removed. Ideally algorithms should efficiently learn to prevent this situation and succeed as indicated by the green arrows. Thus, temporal and workspace dependencies must be considered. Events at a current time t i \u2208 T, i \u2208 [1...n] can influence the likelihood of successful outcomes for past actions t h |h < i and future actions t j |j > i. A successful choice of action at any given t i will ensure both past and future actions are productive contributors to the larger task at hand. In our experiments a partial stack or row is itself a scene obstacle. The gray wall pictured here is for illustrative purposes only.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Fig. 4 .4Fig. 4. Example of SPOT Trial Reward R trial (6), and the SPOT Progress Re-", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Fig. 5 .5Fig.5. Safety Grid World where the goal is to avoid lava and get to the green square.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Fig. 6 .6Fig. 6. Real training of the SPOT framework to Stack 4 Cubes with R trial and SPOT-Q. Failures include missed grasps, off-stack placements, and actions in which the stack topples. Toppling can occur during successful grasp and push actions.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "SPOT-Q with Prioritized Experience Replay. 1: Input Replay Memory H T = (S T , A T , R T , Predicted T ) 2: while AGENT_IS_ACTING( ) do 3: t = PRIORITIZED_EXPERIENCE_SAMPLE(T, H T ) 4: y t = R(s t+1 , a t ) + \u03b3Q(s t+1 , \u03c0(s t+1 ))", "figure_data": "Algorithm 1: 5: \u03b4 t = HUBER_LOSS(Q(s t , a t ); y t )6:a \u03c0,t = \u03c0(s t )7:if M (s t , a \u03c0,t ) = 0 thenThe action wouldfail.8:y t = \u03b3Q(s t+1 , a \u03c0,t )New 0 rewardsample.9:\u03b4 t = \u03b4 t + HUBER_LOSS(Q(s t , a t ); y t )10:end if11:BACKPROP(\u03b4 t ); step optimizer; update weights.12: end while6)"}, {"figure_label": "I", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "ROBOT TASK RESULTS (SECTION IV) WITH THE SPOT FRAMEWORK", "figure_data": ""}, {"figure_label": "II", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "ROBOT TASK RESULTS (SECTION V) WITH THE SPOT FRAMEWORK Bold Entries Highlight Sim to Real Transfer With SPOT-Q. In This Table No SPOT-Q Also Means No Masking.", "figure_data": ""}, {"figure_label": "III", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "SAFETY GRID WORLD (FIG. 5) COMPARISON OF ALGORITHM CHANGES ONTOP OF RAINBOW [35]"}], "formulas": [], "doi": "10.1109/LRA.2020.3015448"}
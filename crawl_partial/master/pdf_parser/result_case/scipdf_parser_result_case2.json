{"title": "DISCOVERING LATENT NETWORK TOPOLOGY IN CONTEXTUALIZED REPRESENTATIONS WITH RANDOMIZED DYNAMIC PROGRAMMING", "authors": "", "pub_date": "", "abstract": "The discovery of large-scale discrete latent structures is crucial for understanding the fundamental generative processes of language. In this work, we use structured latent variables to study the representation space of contextualized embeddings and gain insight into the hidden topology of pretrained language models. However, existing methods are severely limited by issues of scalability and efficiency as working with large combinatorial spaces requires expensive memory consumption. We address this challenge by proposing a Randomized Dynamic Programming (RDP) algorithm for the approximate inference of structured models with DP-style exact computation (e.g., Forward-Backward). Our technique samples a subset of DP paths reducing memory complexity to as small as one percent. We use RDP to analyze the representation space of pretrained language models, discovering a large-scale latent network in a fully unsupervised way. The induced latent states not only serve as anchors marking the topology of the space (neighbors and connectivity), but also reveal linguistic properties related to syntax, morphology, and semantics. We also show that traversing this latent network yields unsupervised paraphrase generation.", "sections": [{"heading": "INTRODUCTION", "text": "The discovery of large-scale discrete latent structures is crucial for understanding the fundamental generative processes of language, and has been shown useful to various NLP tasks ranging from data-to-text generation (Li & Rush, 2020), summarization (Angelidis et al., 2021), syntactic parsing , and knowledge graph reasoning (Qu et al., 2020). In this work, we use latent structures to analyze geometric properties of representation space of pretrained language models (PLMs). Despite the large volume of recent work analyzing PLMs and proposing various improvements (Rogers et al., 2020), little is known about the topological structure of their representation manifold. Since such structure cannot be easily observed, it is only natural to resort to latent variables. Yet scaling discrete combinatorial structures is extremely difficult with multiple modeling and computational challenges (Wainwright & Jordan, 2008).\nIn this work, we address the computational challenges arising from working with combinatorial structures. We consider linear-chain CRFs, a popular structured model family (Ma & Hovy, 2016;Sutton & McCallum, 2006) that uses dynamic programming for exact inference. Specifically, we focus on the forward algorithm (Rabiner, 1989), which is widely used to compute the partition function. Space complexity for this algorithm is O(T N 2 ) where N is the number of latent states and T the length of the sequence. It is precisely the N 2 term that becomes problematic when we construct the adjacent gradient graph with automatic differentiation. DP-based inference algorithms are not optimized for modern computational devices like GPUs and typically work under small-data regimes, with N in the range [10,100] (Ma & Hovy, 2016;Wiseman et al., 2018). With larger N , inference becomes intractable since gradients do not easily fit into GPU memory (Sun et al., 2019).\nOur algorithmic contribution is a randomization technique for dynamic programming which allows us to scale N to thousands (possibly more) latent states. Specifically, to approximate the partition function, instead of summing over all possible combinations of latent states, we only sum over paths with most probable states, and sample a subset of less likely paths to correct the bias according to a reasonable proposal. Since we only calculate the sampled path, memory consumption can be reduced to a small controllable budget which is scale invariant. With a larger memory budget, our method becomes more accurate, and our estimation error smaller. We thus recast the memory complexity challenge into a tradeoff between memory budget, proposal accuracy, and estimation error. When applied to linear-chain CRFs, we show that RDP scales the model by two orders of magnitude with memory complexity as small as one percent of the full DP. Beyond linear-chains, RDP is applicable to any structured model with DP-style exact inference such as trees  and semi-Markov models (Li & Rush, 2020), and could also be extended to more general message passing algorithms (Wainwright & Jordan, 2008).\nOur analytical contribution is a geometric study of the representation manifold of PLMs, using the proposed RDP algorithm. We hypothesize that there exist latent anchor embeddings (or landmarks) that describe the manifold topology. We also expect these anchor states to be informative enough to generate sentences, and their connections to be linguistically meaningful. We induce latent structures using a VAE with an inference model parameterized by a scaled CRF where state-word relations are modeled by the emission potential and state-state transitions are modeled by the transition matrix. The connections of words and states together form a latent network. We use the vector product between contextualized embeddings and state embeddings to parameterize the CRF potentials, bringing together the geometry of the representation space with graphical model inference. We further show that it is possible to generate paraphrases by traversing the induced network.\nOur approach is fully unsupervised and the discovered latent network is intrinsic to the representation manifold, rather than imposed by external supervision, eschewing the criticism of much previous work on supervised probes (Hewitt & Liang, 2019;Chen et al., 2021). In experiments, we first verify the basic properties of RDP (bias-variance) and show its effectiveness for training latent variable models. We then visualize the discovered network based on BERT Devlin et al. (2019), demonstrating how states encode information pertaining to syntax, morphology, and semantics. Finally, we perform unsupervised paraphrase generation by latent network traversal.", "n_publication_ref": 18, "n_figure_ref": 0}, {"heading": "RANDOMIZED DYNAMIC PROGRAMMING", "text": "Preliminaries: Speeding Summation by Randomization To motivate our randomized DP, we start with a simple setting, namely estimating the sum of a sorted list. Given a sorted list of positive numbers a, naive summation S = a 1 + ..., +a N requires N \u2212 1 addition operations, which is expensive when N is large. Suppose we wish to reduce the number of addition operations to K 1 << N , and we already know that the list is long-tailed (similar to how words in language follow a Zipfian distribution such that there are few very high-frequency words that account for most of the tokens in text and many low-frequency words). Then, we only need to sum over the top K 1 values to get an efficient estimate:\u015c\n1 = a 1 + ... + a K1\nwhere {a i } N i=1 sorted, large to small (1)\nClearly,\u015c 1 underestimates S. When the summands are \"dense\", i.e., not very different from each other, the bias is large because the top K 1 terms do not contribute much to the sum (Fig. 1A). To correct this bias, we add samples a \u03b41 , ..., a \u03b4 K 2 from the remaining summands whose indices \u03b4 i are sampled from proposal \u03b4 i \u223c q = [q K1+1 , ..., q N ]:\nS 2 = a 1 + ... + a K1 + 1 K 2 ( 1 q \u03b41 a \u03b41 + ... + 1 q \u03b4 K 2 a \u03b4 K 2 ) \u03b4 i \u2208 {K 1 + 1, ..., N } (2)\nwhere K 1 + K 2 = K. Note that this is an unbiased estimator as E[\u015c 2 ] = S, irrespective of how we choose q. Without any knowledge about a, the simplest proposal would be uniform, no matter what variance it induces. The more q i correlates with a i , the less variance\u015c 2 has. The oracle q i is proportional to a i , under which\u015c 2 becomes exact\u015c 2 \u2261 S as q \u03b4i = a \u03b4i /(a K+1 + ... + a N ) for all i.\nSo, the strategy is to exploit our knowledge about a to construct a correlated proposal q. Given this estimator, we can also adjust the computation budget in order to reduce variance. When the distribution is long-tailed, we may increase K 1 as an instance of Rao-Blackwellization (Liu et al., 2019). When the distribution is not long-tailed (enough), and top K 1 summation underestimates significantly, we may increase K 2 to reduce variance, provided we have a fairly accurate q, as an instance of importance sampling. This procedure is also discussed in Kool et al. (2020) for We parametrize the CRF factors with vector products; the relations between states and contextualized embeddings together form a latent network (Fig. 3 and 4); (D): Experimental protocol; we first study the basic properties of RDP (steps 1, 2) and then integrate RDP into a LVM for inferring the structure of the representation space (steps 3, 4). Best viewed in color.\ngradient estimation. In fact, it is the underlying basis of many Monte Carlo estimators in various settings (Mohamed et al., 2020).\nThe Sampled Forward Algorithm Now we will show how estimator\u015c 2 can be used to scale summation in DP. Consider a linear chain CRF which defines a discrete state sequence\nz = [z 1 , ..., z T ], z t \u2208 {1, ..., N } over an input sentence x = [x 1 , ..., x T ]\n. Later we will use this CRF to construct an inference model to discover latent network structures within contextualized representations. We are interested in the partition function Z which is commonly computed with the Forward algorithm, a dynamic programming algorithm that sums over the potentials of all possible state sequences. The core recursion steps are:\n\u03b1 t+1 (i) = N j=1\u00e3 t+1 (i, j) = N j=1 \u03b1 t (j)\u03a6(j, i)\u03c6(x t+1 , i) Z = N j=1 \u03b1 T (j)(3)\nwhere \u03b1 t (i) is the sum of all possible sequences up to step t and at state i, \u03a6(\u2022, \u2022) is an N \u00d7 N transition matrix, and \u03c6(x t , i) is the emission potential that models how word x t generates state i. We assume all potentials are positive for simplicity. When implemented on GPUs, space complexity is O(T N 2 ) (see number of edges in the DP graph in Figure 1B) and it is the squared term N 2 that causes memory overflows under automatic differentiation (see Appendix B for engineering details).\nOur key insight is to recursively use the memory-efficient randomization of Eq. 2 to estimate Eq. 3 at every step. Given a proposalq t for each step t that correlates with summands\u00e3 t (we discuss how to constructq t in the next section), we obtain its top K 1 index and sample K 2 from the rest:\n[\u03c3 t,1 , ..., \u03c3 t,K1 , ..., \u03c3 t,N ] = arg sort i {q t (i)} N i=1 (4) [\u03b4 t,1 , ..., \u03b4 t,K2 ] \u223c Categorical{q t (\u03c3 t,K1+1 ), ...,q t (\u03c3 t,N )} (5\n)\nwhereq t (\u2022) are normalized to construct the categorical. Compared to Eq. 3, the key recursion of our Sampled Forward uses the top K 1 index \u03c3 t and sampled K 2 index \u03b4 t to substitute the full index:\n\u03b1 t+1 (i) = K1 j=1\u03b1 t (\u03c3 t,j )\u03a6(\u03c3 t,j , i)\u03c6(x t+1 , i) + 1 K 2 K2 j=1Z t q t (\u03b4 t,j )\u03b1 t (\u03b4 t,j )\u03a6(\u03b4 t,j , i)\u03c6(x t+1 , i) (6) Z t = N j=K1+1q t (\u03c3 t,j )\u1e90 = K1 j=1\u03b1 T (\u03c3 T,j ) + 1 K 2 K2 j=1Z T q T (\u03b4 T,j )\u03b1 T (\u03b4 T,j )(7)\nwhere the oracle proposal q * t is proportional to the actual summand\u00e3 t (Eq. 3, a little bit algebra will show this is actually the backward sampling probability p(z t = i|z t+1 = j)) , which is only accessible with the full Forward. So, we use the proposal weightq t (Eq. 4) to move the computation outside the DP. In Fig. 1B, the top K 1 summed terms correspond to black nodes. The proposalq t corresponds to black and grey bars, and its distance from the oracle proposal\u00e3 t (which is the major source of variance) is highlighted in green. Sampled indices are shown as blue nodes. Essentially, our Sampled Forward algorithm restricts the DP computation from the full graph to subgraphs with top and sampled edges, reducing complexity to O(T K 2 ) where K = K 1 + K 2 . By varying K, memory complexity becomes a tradeoff between memory budget and estimation error. By induction, we can show that\u1e90 (Eq. 7) is an unbiased estimator of Z since \u2200t, E[\u03b1 t ] = \u03b1 t . When implemented in log space, the expected log\u1e90 is a lower bound of the exact log Z due to Jensen's inequality, and the variance is (trivially) reduced by log(\u2022). See Appendix for details on implementation (Section C), theoretical analysis (Section A), and extensions to general sum-product structures (Section D).", "n_publication_ref": 3, "n_figure_ref": 4}, {"heading": "LATENT NETWORK TOPOLOGY IN PRETRAINED LANGUAGE MODELS", "text": "Latent States within Representation Space We now use the above technique to uncover hidden geometric structures in contextualized representations. In experiments we work with BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019), however, our method can be easily applied to other pretrained language models. Given sentence x = [x 1 , ..., x T ], we denote its contextualized representations as [r 1 , ..., r T ] = PLM(x). Representations r for all sentences lie in one manifold M, namely, the representation space of the language model. We hypothesize there exists a set of latent states s 1 , ..., s M that function as anchors and outline the space topology. We emphasize that all parameters of the PLM are fixed (i.e., no fine-tuning takes place), so all learned states are intrinsic to M. We focus on two topological relations: (a) state-word relations, which represent how word embeddings may be summarized by their states and how states can be explained by their corresponding words; and (b) state-state relations, which capture how states interact with each other and how their transitions denote meaningful word combinations. Taken together, these two relations form a latent network within M (visualized in Fig. 3 and 4).\nWe adopt a minimal parametrization of the inference network so as to respect the intrinsic structure of the representation manifold without imposing strong assumptions (e.g., via regularization). Specifically, for state-word relations, we associate each word embedding r t with a latent state indexed by z t \u2208 {1, ..., N } (the corresponding embedding of z t is s zt ). For state-state relations, we assume a transition weight \u03a6(i, j). Together we have a linear-chain CRF:\nlog \u03c6(x t , z t ) = r t s zt log \u03a6(z t\u22121 , z t ) = s zt\u22121 s zt (8)\nwhere the dot product follows the common practice of fine-tuning contextualized representations. We use log space for numerical stability. The probability of a state sequence given a sentence is:\nq \u03c8 (z|x) = T t=1 \u03a6(z t\u22121 , z t )\u03c6(x t , z t )/Z (9)\nHere, the only learnable parameters are state embeddings: \u03c8 = [s 1 , ..., s N ] as we try to be faithful to the representation manifold. Note how this parametrization reconciles space geometry with graphical models. As N is large, we estimate Z with the proposed Sampled Forward (Eq. 7).\nConstructing the Proposal We now return to proposalq t (Eq. 4) which we construct based on a common observation that linguistic phenomena are long-tailed:\nq t (i) \u221d \u03a6(i)\u03c6(x t , i) \u03a6(i) = ||s i || 1 (10\n)\nwhere \u03c6(x t , i) states that only a few states are likely to generate observation x t , which is often the case in NLP (e.g., there are only a few possible POS tags for each word); and \u03a6(i) models the prior probability of state i. This choice stems from the empirical observation that larger L1 norm correlates with larger dot product, and is thus more likely to be inferred. Essentially, our proposal combines local emissions \u03c6 and global prior \u03a6 to approximate the\u00e3 t variables (Eq. 3) and bypass their expensive computation.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Inference and Learning", "text": "We use amortized variational inference to learn s. We simply reuse the architecture from previous work Fu et al. (2020); Li & Rush (2020) and build a generative model:\np \u03b8 (x, z) = t p(x t |z 1:t , x 1:t\u22121 ) \u2022 p(z t |z 1:t\u22121 , x 1:t\u22121 ) h t = Dec([s zt\u22121 ; x t\u22121 ], h t\u22121 ) (11)\np(x t |z 1:t , x 1:t\u22121 ) = softmax(FF(h t )) p(z t |z 1:t\u22121 , x 1:t\u22121 ) = softmax(FF([s zt ; h t ])) ( 12)\nwhere \u03b8 denotes the decoder parameters, Dec(\u2022) denotes the decoder (we use an LSTM), h t denotes decoder states, and FF(\u2022) denotes a feed-forward network. This autoregressive formulation essentially encourages states to be \"generative\", i.e., to generate sentences and themselves. We will show in experiments how this formulation lends itself to paraphrasing. We use q \u03c8 directly from Eq. 9 as our variational posterior, and optimize the following \u03b2-ELBO objective:\nL ELBO = E q \u03c8 (z|x) [log p \u03b8 (x, z)] \u2212 \u03b2H(q \u03c8 (z|x))(13)\nwhere the \u03b2 parameter modulates the topology of the latent structure and prevents posterior collapse. We follow Fu et al. (2020) and use their Gumbel reparameterization to optimize q \u03c8 , which is more stable than the REINFORCE gradient estimator (Li & Rush, 2020).\nWhen integrating RDP with the Gumbel reparameterization, we noticed that the gradient will only pass through the top K 1 and sampled K 2 states, in other words, not all states receive gradients. In this case, trading K 1 against K 2 amounts to exploration versus exploitation. A large K 1 means we give gradients to high-confidence states, i.e., we exploit large local emission and global transition potentials. While increasing K 2 means we explore low-confidence states. So, by splitting the computation budget between top K 1 and sampled K 2 states, we not only reduce variance for estimating the partition, but also effectively introduce different strategies for searching over the latent space.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "RELATED WORK", "text": "Efficient Inference for Structured Latent Variables There has been substantial interest recently in the application of deep latent variable models (LVMs) to various language related tasks (Wiseman et al., 2018;Li & Rush, 2020), which has also exposed scalability limitations. Earlier attempts to render CRF models efficient (Sokolovska et al., 2010; either make many stringent assumptions (e.g., sparsity), rely on handcrafted heuristics for bias correction (Jeong et al., 2009), or cannot be easily adapted to modern GPUs with tensorization and parallelization. Sun et al. (2019) are closest to our work, however they only consider top K summation and consistently underestimate the partition. Chiu & Rush (2020) scale HMMs but assume words are clustered beforehand.\nOur approach systematically trades computation with proposal accuracy and estimation error (rather than over-compromising for efficiency). Moreover, we do not impose any hard restrictions like sparsity (Correia et al., 2020), and can accommodate dense and long-tailed distributions. Our method is inspired by randomized automatic differentiation (RAD, Oktay et al., 2020), and can be viewed as RAD applied to the DP computation graph. Advantageously, our proposal is compatible with existing efficient implementations (like Rush, 2020) since it does not change the computation graph.\nInterpretability of Contextualized Representations There has been a good deal of interest recently in analyzing contextualized representations and the information they encode. This line of research, collectively known as \"Bertology\" (Rogers et al., 2020;Hewitt & Manning, 2019), focuses mainly on supervised probing of linguistic properties (Tenney et al., 2019), while the geometric properties of the representations have been less studied (Cai et al., 2021). A major dilemma facing this work is whether supervised linguistic probes reveal properties intrinsic to the embeddings or imposed by the supervision signal itself (Hewitt & Liang, 2019;Hall Maudslay et al., 2020;Chen et al., 2021). In this work, we do not use any supervision to ensure that the discovered network is intrinsic to the representation space.", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "EXPERIMENTS", "text": "In this section, we present our experimental results aimed at analyzing RDP and showcasing its practical utility (see Fig. 1). Specifically, we (1) verify the basic properties of RDP by estimating the partition function and (2) using it to train the structured latent variable model introduced in Section 3;\n(3) we then turn our attention to pretrained language models and examine the network induced with our approach and whether it is meaningful; and (4) we generate sentence paraphrases by traversing this network. For experiments (1, 2, 4), we use (a). pretrained GPT2 as the encoder since they are more about autoregressive language modeling and generation; (b). the MSCOCO dataset, a common benchmark for paraphrasing (Fu et al., 2019). For experiment (2), we use (a). BERT since it has been the main focus of most previous analytical work (Rogers et al., 2020);(b). the 20News dataset, a popular benchmark for training latent variable models (Grisel et al.). Across all experiments, we  ", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "BASIC PROPERTIES", "text": "We examine the estimation of the partition function for three unit cases, namely dense, intermediate, and long-tailed distributions. Instead of simulating these unit cases, to make our experiments more realistic, we extract CRFs on-the-fly from different LVM training stages. We also study the effects of memory budget by setting K to 20, 200, and 400 (corresponding to 1, 10, and 20 percent of the full memory). We use TopK summation (Sun et al., 2019) as our main baseline. This method can be viewed as setting K 1 = K and K 2 = 0 in our framework, i.e., it does not use the random sample.\nFor training LVMs, We consider 100 and 2,000 latent states. With 100 states we are able to perform the summation exhaustively which is the same as Fu et al. (2020). Full summation with 2,000 states is intractable, so we only compare with TopK summation and use K = 100.\nEstimating the Partition Function As shown in Figure 2, TopK summation always underestimates the partition. The gap is quite large in the dense case (large entropy), which happens at the initial stages of training when the model is not confident enough. The long-tailed case represents later training epochs when the model has converged and is more concentrated. Our method effectively corrects the bias, and works well in all unit cases with significantly less memory.\nTraining Latent Variable Models We compare different LVMs in Table 1. Following common practice, we report negative log likelihood (NLL) and perplexity (PPL). We perform an extensive search over multiple hyperparameters (e.g., \u03b2, learning rate, word dropout) across multiple random seeds (3-6) and report the average performance of the best configuration for each method. Our model performs best in both 100 and 2,000 state settings. The advantage is modest (as there are no architecture changes, only different training methods) but consistent. RDP trades off exploitation (i.e., increasing K 1 ) and exploration (i.e., increasing K 2 ) while TopK summation always focuses on the local solutions by passing gradients through top states. Intuitively, we have the chance of discovering better latent states (i.e., larger likelihood) by randomly searching the unexplored space.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "DISCOVERING LATENT NETWORKS FROM PRETRAINED EMBEDDINGS", "text": "We now discuss how latent structures induced with RDP reveal linguistic properties of contextualized representations. We focus on BERT Devlin et al. (2019) and set the number of latent states to 2,000. As BERT's vocabulary size is 32K, one state would approximately handle 15 words in the uniform case, functioning as a type of \"meta\" word. After convergence, we use q \u03c8 to sample z for each x in the training set (recall we use the 20News dataset). These z can be viewed as samples from the aggregated posterior x q \u03c8 (z|x)p (x) where p (x) denotes the empirical data distribution. To get a descriptive summary of BERT's latent topology, we compute the following statistics on z samples: state frequency (Fig. 3, A3); number words corresponding to each state (Fig. 3, A2); number of states corresponding to each word (Fig. 3, A1); and state bigrams (Fig. 4). We further differentiate stopwords (e.g., in, of, am, is) from content words.\nState-word Relations Figure 3 gives a first impression of how latent states spread over the representation space. Overall, we observe that the joint space is Zipfian, and this property characterizes  the distribution of words (A1), states (A3), and word occurrence within each state (C). We also see that the top 500 states account for most word occurrence (A2) while the remaining states model tail phenomena (A3). We conjecture this number is related to the intrinsic dimension of the data manifold (see Aghajanyan et al. 2021). The induced states encode multiple linguistic properties (Fig. 3,  C). Some states are similar to a lexicon entry encoding specific words and their morphological variants; other states exhibit clustering based on morphological features (-s, -er, -ly suffix). We believe this is closely related to the fact that BERT learns embeddings over subwords. Note that the past tense cluster contains words exhibiting both regular (-ed suffix) and irregular morphology (e.g., lost and built). Finally, we also see that some states are largely semantic, similar to a conventional topic model (e.g.  the aggregated posterior (encoded in the bigram sample from q \u03c8 ), which is an important desideratum of generative modeling (Mathieu et al., 2019). Note that the number of edges linked to each node, again, follows a Zipfian distribution as top nodes have most of the connections. From a linguistic perspective, we see how the combination of states leads to meaningful syntactic and semantic constructions. Again, BERT encodes various syntactic configurations such as to infinitives, passive voice, and even manages to distinguish adverbials (e.g., in fact) from prepositional phrases (e.g., in Bosnia). In general, the latent network seems to have some grasp of syntax, semantic roles, and collocations. In the following section, we examine whether this inherent knowledge can be harvested for generation. See Appendix E.7 for more state transition examples.\n(is + v.ed) 665-476 is-defined 3 | is-supported 3 | is-produced 3 | is-created 2 | is-available 2 | is-caused 2", "n_publication_ref": 3, "n_figure_ref": 6}, {"heading": "PARAPHRASING THROUGH NETWORK TRAVERSAL", "text": "We now study how the latent network can be usefully employed to generate paraphrases without access to parallel training instances. Given a sentence, we generate its paraphrase by conditioning on the input which we represent as a bag-of-words Fu et al. (2020) and by sampling from latent states. This amounts to traversing the latent network then fill in the traversal path to assemble a sentence, as visualized in Fig. 4 C. We instantiate our approach with a latent network learned from GPT2 representations (Radford et al., 2019) and refer to our model collectively as GPTNET.\nWe compare against three previous unsupervised models (first block in Table 2), including CGMH (Miao et al., 2019), a general-purpose MCMC method for controllable generation; UPSA (Liu et al., 2020), a strong paraphrasing model with simulated annealing, and GUMBEL-CRF (Fu et al., 2020), a template induction model based on a continuous relaxation of the CRF sampling algorithm. We present GPTNET variants with 50 and 2,000 states, and show results with RDP and topK, and the full summation for 50 states. Following previous work, we use iBLEU (Sun & Zhou, 2012) as our main metric, which trades off fidelity to the references (BLEU) and variation from the input (self-BLEU). Table 2 shows that RDP is superior to TopK and full summation in terms of iBLUE. GPTNet models do not outperform GUMBEL-CRF or UPSA. This is expected as these methods are highly tailored to the task and more flexible (e.g., they do not fix the encoder), while we restrict the modeling within the GPT2 representation space (to infer its structure). So, our results should be viewed as a sanity check demonstrating the latent network is indeed meaningful for generation (see Appendix E.8 for more generation examples).", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "CONCLUSION", "text": "In this paper, we have developed a general method for scaling the inference of structured latent variable models with randomized dynamic programming. It is a useful tool for the visualization and inspection of the intrinsic structure of contextualized representations. Experiments with BERT reveal the topological structure of its latent space: state-word connections encapsulate syntactic and semantic roles while state-state connections correspond to phrase constructions. Moreover, traversal over a sequence of states represents underlying sentence structure.\nEthics Statement As this paper inspects the internal structure of pretrained language models, it is likely that it will reveal frequent linguistic patterns encoded in the language model. Specifically, the frequent words, phrases, and sentences associated with different gender, ethnic groups, nationality, interest groups, social status, and all other factors, are likely to be revealed by our model. When calling the generative part of our model for paraphrasing, these differences are likely to exist in the generated sentences (depending on the dataset). These facts should be considered when using this model. \u2022 Section D. Extensions of randomized dynamic programming.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Reproducibility Statement", "text": "\u2022 Section E. Experimental details.\n\u2022 Section F. Additional experimentsl results.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A THEORETICAL ANALYSIS OF SAMPLED FORWARD ALGORITHM", "text": "A.1 BIAS ANALYSIS\nIn this section, we discuss the unbiasedness and variance of the Randomized Forward algorithm. We first show that Randomized Forward gives an unbiased estimator of the partition function.\nTheorem A.1 (Unbiasedness). For all t \u2208 [1, 2, ..., T ], the sampled sum\u03b1 t (Eq. 6) is an unbiased estimator of the forward variable \u03b1 t (Eq. 3). The final sampled sum\u1e90 (Eq. 7) is an unbiased estimator of the partition function Z (Eq. 3).\nProof. By the Second Principle of Mathematical Induction. Assume initialization \u03b1 1 (i) = \u03c6(x 1 , i).\nFirstly at t = 2, for all i, we have:\nE q1 [\u03b1 2 (i)] = K1 j=1 \u03b1 1 (\u03c3 1,j )\u03a6(\u03c3 1,j , i)\u03c6(x 2 , i) + 1 K 2 K2 j=1 E q1 Z 1 q 1 (\u03b4 1,j ) \u03b1 1 (\u03b4 1,j )\u03a6(\u03b4 1,j )\u03c6(x 2 , i) =A (14\n)\nwhere the second term can be expanded as a masked summation with the index rearranged from \u03c3 2,K1+1 to \u03c3 2,N :\nA = K2 j=1 E q1 Z 1 q 1 (\u03b4 1,j ) \u03b1 1 (\u03b4 1,j )\u03a6(\u03b4 1,j )\u03c6(x 2 , i) (15) = K2 k=1 N j=K1+1 E q1 1 q 1 (\u03b4 1,k ) 1(\u03b4 1,k = j)\u03b1 1 (\u03c3 1,j )\u03a6(\u03c3 1,j )\u03c6(x 2 , i) (16) = K2 k=1 N j=K1+1 E q1 1 q 1 (\u03b4 1,k ) 1(\u03b4 1,k = j) =1 \u03b1 1 (\u03c3 1,j )\u03a6(\u03c3 1,j )\u03c6(x 2 , i) (17) = K 2 N j=K1+1 \u03b1 1 (\u03c3 1,j )\u03a6(\u03c3 1,j )\u03c6(x 2 , i)(18)\nNotice how we re-index the sum. Now put it back to Eq. 14 to get:\nE q1 [\u03b1 2 (i)] = K1 j=1 \u03b1 1 (\u03c3 1,j )\u03a6(\u03c3 1,j , i)\u03c6(x 2 , i) + 1 K 2 \u2022 K 2 N j=K1+1 \u03b1 1 (\u03c3 1,j )\u03a6(\u03c3 1,j )\u03c6(x 2 , i) (19) = N j=1 \u03b1 1 (\u03c3 1,j )\u03a6(\u03c3 ,j , i)\u03c6(x 2 , i) (20) = N j=1 \u03b1 1 (j)\u03a6(j, i)\u03c6(x 2 , i) (21) = \u03b1 2 (i)(22)\nThis verifies the induction foundation that E q1 [\u03b1 2 (i)] = \u03b1 2 (i) for all i.\nNow assume for time index t we have \u2200i, E q1:t\u22121 [\u03b1 t (i)] = \u03b1 2 (i). Consider t + 1, we have:\nE 1:qt [\u03b1 t+1 (i)] = K1 j=1 E q1:t\u22121 \u03b1 t (\u03c3 t,j ) \u03a6(\u03c3 t,j , i)\u03c6(x t+1 , i) (23) + 1 K 2 K2 j=1 E q1:t\u22121 \u03b1 t (\u03b4 t,j ) \u2022 E qt Z t q t (\u03b4 t,j ) \u03a6(\u03b4 t,j )\u03c6(x t+1 , i)] (24) = K1 j=1 \u03b1 t (\u03c3 t,j )\u03a6(\u03c3 t,j , i)\u03c6(x t+1 , i)(25)\n+ 1 K 2 K2 j=1 \u03b1 t (\u03c3 t,j ) \u2022 E qt Z t q t (\u03b4 t,j ) \u03a6(\u03b4 t,j )\u03c6(x t+1 , i)] =A (26)\nNote how we decompose the expectation by using the independence: q 1:t = q 1:t\u22121 \u2022q t With a similar masked summation trick as A, we have:\nA = K 2 N j=K1+1 \u03b1 t (\u03c3 t,j )\u03a6(\u03c3 t,j , i)\u03c6(x t+1 , i)(27)\nThis gives us:\nE 1:qt [\u03b1 t+1 (i)] = K1 j=1 \u03b1 t (\u03c3 t,j )\u03a6(\u03c3 t,j , i)\u03c6(x t+1 , i) + 1 K 2 \u2022 K 2 N j=K1+1 \u03b1 t (\u03c3 t,j )\u03a6(\u03c3 t,j , i)\u03c6(x t+1 , i)(28)\n= N j=1 \u03b1 t (\u03c3 t,j )\u03a6(\u03c3 t,j , i)\u03c6(x t+1 , i) (29) = N j=1 \u03b1 t (j)\u03a6(j, i)\u03c6(x t+1 , i) (30) = \u03b1 t+1 (i)(31)\nThus showing\u03b1 t is an unbiased estimator for \u03b1 t at each step t. Setting t = T , the last step, gives us E[\u1e90] = Z (details similar to the above).\nCorollary A.1.1. When we change the (sum, product) semiring to the (log-sum-exp, sum) semiring, the expectation of the estimator will become a lower bound of log \u03b1 t and log Z.\nProof. Denote l t (i) = log \u03b1 t (i) and \u2126 the set of sampled indices where |\u2126| = K 2 . For simplicity, we omit the top K 1 summation and only show the summation of the sample. Cases where t > 2 can be derived similarly as following:\nl 2 (i) = log j\u2208\u2126 exp(l 1 (j) + log \u03a6(j, i) + log \u03c6(x t , i)) \u2212 log K 2 (32) E[l 2 (i)] = E log j\u2208\u2126 exp(l 1 (j) + log \u03a6(j, i) + log \u03c6(x t , i)) \u2212 log K 2 (33) \u2264 log E j\u2208\u2126 exp(l 1 (j) + log \u03a6(j, i) + log \u03c6(x t , i)) \u2212 log K 2 (34) = log K 2 \u2212 log K 2 + log j\u2208\u2126 exp(l 1 (j) + log \u03a6(j, i) + log \u03c6(x t , i)) (35) = l 2 (i)(36)\nwhere Eq. 34 comes from Jensen's inequality. Then by induction one can show at everystep, we have E[l t (i)] \u2264 l t (i).\nAlthough implementation in the log space makes the estimate biased, it reduces the variance exponentially in a rather trivial way. It also provides numerical stability. So, in practice we use it for training.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 VARIANCE ANALYSIS", "text": "Now we analyze variance. We start with the estimator a \u03b4 /q \u03b4 , \u03b4 \u223c Categorical{q K1+1 , ...q K N } in Eq. 2. Firstly, this is an unbiased estimator of the tail sum:\nE[a \u03b4 /q \u03b4 ] = N i=K1+1 a i (37\n)\nWe have the folling variance arguments:\nTheorem A.2 (Variance of the tail estimator). V[a \u03b4 /q \u03b4 ] = N i=K1+1 a 2 i /q i \u2212 ( N i=K1+1 a i ) 2 (38) = S 2 K1 ( N i=K1+1 2 i /q i + 2 i )(39)\nwhere\nS K1 = N i=K1+1 a i i = a i /S K1 \u2212 q i (40\n)\nwhich says the variance is:\n\u2022 quadratic to the tail sum S K1 , which will can be reduced by increasing K 1 as the effect of Rao-Blackwellization.\n\u2022 approximately quadratic to the gap i , as the differences between the proposal q i and the oracle a i /S K1 , which can be reduced by choosing a correlated proposal as the effect of importance sampling.\nOptimal zero variance is achieved on\nq * i = a i /S K1 (41)\nProof. The variance is then:\nV[a \u03b4 /q \u03b4 ] = E[(a \u03b4 /q \u03b4 ) 2 ] \u2212 E[a \u03b4 /q \u03b4 ] 2 (42)\nwhere the first term is the second moment:\n(a \u03b4 /q \u03b4 ) 2 = ( N i=K1+1 a i 1[\u03b4 = i]/q i ) 2 (43) = N i=K1+1 a 2 i 1[\u03b4 = i]/q 2 i + 2 N i=K1+1 N j=K1+1 a i a j 1[\u03b4 = i]1[\u03b4 = j] q i q j =0(44)\n= N i=K1+1 a 2 i 1[\u03b4 = i]/q 2 i (45\n)\ntaking the expection of it we get:\nE[(a \u03b4 /q \u03b4 ) 2 ] = E[ N i=K1+1 a 2 i 1[\u03b4 = i]/q 2 i ] (46) = N i=K1+1 a 2 i /q i (47\n)\nPlug this back, variance is:\nV[a \u03b4 /q \u03b4 ] = N i=K1+1 a 2 i /q i \u2212 ( N i=K1+1 a i ) 2 (48)\nTo get the optimal proposal, we solve the constrained optimization problem:\nmin qi N i=K1+1 a 2 i /q i \u2212 ( N i=K1+1 a i ) 2 (49) s.t. N i=K1+1 q i = 1 (50)\nBy solving the corresponding Lagrangian equation system (omitted here), we get the optimal value achieved at:\nq * i = a i N i=K1+1 a i N i=K1+1 a 2 i /q * i \u2212 ( N i=K1+1 a i ) 2 = 0 (51)\nThis says zero variance is achieved by a proposal equal to the normalized summands.\nThen define the gap between the proposal and the normalized summands as:\ni = a i S K1 \u2212 q i (52\n)\nPlug this to the variance expression we get:\nV[a \u03b4 /q \u03b4 ] = S 2 K1 ( N i=K1+1 2 i /q i + 2 i )(53)\nCorollary A.2.1. When increasing the sample size to K 2 , the variance will reduce to\nV[ 1 K 2 N j=1 a \u03b4j q \u03b4j ] = 1 K 2 S 2 K1 ( N i=K1+1 2 i /q i + 2 i )(54)\nNow we consider the variance of the Sampled Forward algorithm. An exact computation would give complicated results. For simplification, we give an asymptotic argument with regard to Rao-Blackwellization and importance sampling:\nTheorem A.3 (Single\nStep Asymptotic Variance of Sampled Forward). At each step, the alpha varible estimator has the following asymptotic variance:\nV[\u03b1 t (i)] = O 1 K 2 \u03b1 2 t,K1 (i) \u2022 2 t (i)(55)\nwhere:\n\u2022 \u03b1 t+1,K1 (i) = N j=K1+1\u03b1 t (j, i) = N j=K1+1 E q1:t\u22121 [\u03b1 2 t (j)]\u03a6(j, i)\u03c6(x t , i\n) is a tail sum after the top K 1 summands. This term will reduce if we increase K 1 , as an instance of Rao-Blackwellization.\n\u2022 2 t (i) =\nN j=K1+1 2 t\u22121 (j, i)/q t\u22121 (j) and t\u22121 (j, i) is the difference between the proposal q t\u22121 (j) and the oracle proposal in Eq. 3. This term will reduce if the proposal is more correlated to the oracle, as an instance of Importance Sampling.\nProof. We start with a simple setting where K 2 = 1. At step t + 1 we have the following variance recursion:\nE q1:t [\u03b1 2 t+1 (i)] = N j=K1+1 \u03a6 2 (j, i)\u03c6 2 (x t+1 , i) q t (j) \u2022 E q1:t\u22121 [\u03b1 2 t (j)](56)\nThis is derived by plugging estimator 6 to the variance Eq. 38 we have just derived. Denote:\n\u03b1 t+1,K1 (i) = N j=K1+1\u03b1 t (j, i) = N j=K1+1 E q1:t\u22121 [\u03b1 2 t (j)]\u03a6(j, i)\u03c6(x t+1 , i)(57)\nThen we have\nV q1:t [\u03b1 t+1 (i)] = \u03b1 2 t+1,K1 (i) N j=K1+1 2 t (j, i) q t (i) + 2 t (j, i)(58)\nwhere t (j, i) is the differences between the proposal and the normalized exact summands at step t state i:\nt (j, i) =\u03b1 t (j, i) N j=K1=1\u03b1 t (j, i) \u2212 q t (j)(59)\nDropping out the first order errors and increasing the number of sample to K 2 , we have the asymptotics:\nV[\u03b1 t (i)] = O 1 K 2 \u03b1 2 t,K1 (i) \u2022 2 t (i)(60)\nTheorem A.4 (Asymptotic Variance of Sampled Forward Partition Estimation). The alpha variable estimators has the following asymptotic variance recurrsion:\nV[\u03b1 t+1 (i)] = O( 1 K 2 \u2022 \u03c6 2 t,K1 \u2022 2 t,K1 \u2022 V[\u03b1 t ])(61)\nCompared with Eq. 55, this expression:\n\u2022 Uses the product of the factors \u03c6 t,K1 (a function of the sum-prod of the factor at step t) and the previous step variance V[\u03b1 t ] to substitute the \u03b1 t,K1 in equation 55. Again, this term will decrease with a larger K 1 (Rao-Blackwellization).\n\u2022 2 t,K1 (i) = N j=K1+1 2\nt\u22121 (j, i)/q t\u22121 (j) and t\u22121 (j, i) is the difference between the proposal q t\u22121 (j) and the oracle proposal in Eq. 3. This term will reduce if the proposal is more correlated to the oracle, as an instance of Importance Sampling (same as Eq. 55).\nConsequently, the partition function has the following asymptotic variance:\nV[\u1e90] = O( T t=1 1 K 2 \u2022 \u03c6 2 t,K1 \u2022 2 t,K1 )(62)\nWhen implemented in the log space, the variance is trivially reduced exponentially:\nV[log\u1e90] = O( T t=1 log 1 K 2 + 2 log \u03c6 t,K1 + 2 log t,K1 )(63)\nProof. Firstly for simplcity we assume K 1 = 0 and K 2 = 1. The the estimator variance is:\nV[\u03b1 t+1 (i)] = E q1:t [\u03b1 2 t+1 (i)] \u2212 \u03b1 2 t+1 (i) (64) = E q1:t\u22121 N j=1\u03b1 2 t (j)\u03a6 2 (j, i)\u03c6 2 (x t , i) q t (j) \u2022 \u03b1 2 t (j) \u03b1 2 t (j) \u2212 \u03b1 2 t+1 (i)(65)\nRecall:\n\u03b1 t+1 (i) = N j=1 \u03b1 t (j)\u03c6(j, i)\u03c6(x t , i)(66)\nLet:\nt (j, i) = \u03b1 t (j)\u03c6(j, i)\u03c6(x t , i) \u03b1 t+1 (i) =q(zt=j|zt+1=i) \u2212q t (j)(67)\nThen:\nV[\u03b1 t+1 (i)] = E q1:t\u22121 N j=1 \u03b1 2 t+1 (i) 2 t (j, i) q t (j) + 2 t (j, i) + q t (j) \u03b1 2 t (j) \u03b1 2 t (j) \u2212 \u03b1 2 t+1 (i) (68) = \u03b1 2 t+1 (i) N j=1 E q1:t\u22121 [\u03b1 2 t (j)] \u03b1 2 t (j) q t (j) + N j=1 2 t (j, i) q t (j) + 2 t (j, i) E q1:t\u22121 [\u03b1 2 t (j)] \u03b1 2 t (j) (69) \u2212 \u03b1 2 t+1 (i)(70)\nNote that there exist J t such that:\nE q1:t\u22121 [\u03b1 2 t (j)] \u03b1 2 t (j) <= J t (71)\nThis is because of the bounded gap of the Jensen's inequality. Also recall:\nN j=1 q t (j) = 1 (72)\nSo we get:\nV[\u03b1 t+1 (i)] <= \u03b1 2 t+1 (i) J t \u2212 1 + N j=1 2 t (j, i) q t (j) + 2 t (j, i) \u2022 V[\u03b1 t (j)] \u03b1 2 t (j) \u2212 1 (73) = \u03b1 2 t+1 (i) J t \u2212 1 \u2212 N j=1 2 t (j, i) q t (j) + 2 t (j, i) (74) + \u03b1 2 t+1 (i) N j=1 2 t (j, i) q t (j) + 2 t (j, i) \u2022 V[\u03b1 t (j)] \u03b1 2 t (j)(75)\nempirically J t is not the dominate source of variance (but could be in the worst case, depending on the tightness of Jensen's inequality). We focus on the second term:\nV[\u03b1 t+1 (i)] = O \u03b1 2 t+1 (i) N j=1 2 t (j, i) q t (j) + 2 t (j, i) \u2022 V[\u03b1 t (j)] \u03b1 2 t (j) (76) = O N j=1 \u03b1 t (j) \u03a6(j, i)\u03c6(x t , j) O(\u03c6 2 t ) 2 \u2022 N j=1 2 t (j, i) q t (j) + 2 t (j, i) O( 2 t ) \u2022 1 \u03b1 2 t (j) \u2022 V[\u03b1 t (j)] O(V[\u03b1t])(77)\n= O(\u03c6 2 t \u2022 2 t \u2022 V[\u03b1 t ])(78)\nNote that a lot of higher-order sum-products are simplified here. Adding top K summation and increasing the sample size to K 2 leads to variance reduction as:\nV[\u03b1 t+1 (i)] = O( 1 K 2 \u2022 \u03c6 2 t,K1 \u2022 2 t,K1 \u2022 V[\u03b1 t ])(79)\nRecursively expand this equation we get:\nV[\u1e90] = O( T t=1 1 K 2 \u2022 \u03c6 2 t,K1 \u2022 2 t,K1 )(80)\nChaning the implementation to the log space we reduce the variance exponentially:\nV[log\u1e90] = O( T t=1 log 1 K 2 + 2 log \u03c6 t,K1 + 2 log t,K1 )(81)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B CHALLENGES OF FULL DP UNDER AUTOMATIC DIFFERENTIATION", "text": "Now we analyze in detail why direct full summation causes memory overflow. Firstly, we note that in a bachified stochastic gradient descent setting, memory complexity for the forward algorithm is\nO(BT N 2 ) (82\n)\nWhere B is the batch size, T is the maximum sequence length after padding, and N is the number of states. Consider a typical setting where B = 100, T = 100, N = 1000, then the complexity is:\nC = c \u00d7 100 \u00d7 100 \u00d7 1000 \u00d7 1000 = c \u00d7 10 10 (83\n)\nwhere c is a constant depending on specific hardware and libraries. At first sight this may seem reasonable with a large GPU memory (e.g., 16G). Also, one can come up with improvements, such as not storing the intermediate \u03b1 t variables. If we only need one single forward pass, such engineering tricks are indeed effective. In our experiments, when setting B = 50, T = 15, and N = 2000, the actual memory consumption for the forward computation, together with other model components, is about 4G when implemented with PyTorch 1.8.0.\nHowever, this is not the case when working under an automatic differentiation (AD) setting. If we want to compute the gradients of the partition function (e.g., to optimize the likelihood), we inevitably need to store all the intermediate steps to keep the full computation graph. Then, the adjacent graph, in principle, has the same complexity as the forward graph. But in practice, it will be more complicated due to the actual implementation of AD engines. Following the above example where B = 50, T = 15, N = 2000, we get a memory overflow on a 16G memory GPU when calling the PyTorch backward function. This situation also immediately invalidates many engineering tricks (e.g., we cannot drop the intermediate \u03b1 t variables anymore), and substantially increases the difficulty of coming up with new ones, since we would also be working with the internal mechanism of automatic differentiation engines. Note that the internal mechanism of these engines is complicated and opaque to general practitioners (e.g., they may change the underlying computation graph for better speed), and many of these engines (like PyTorch) are not optimized for dynamic programming.\nIn fact, it would be unwise to overwrite the backward computation, even if the AD engine allowed us to do so, as it would significantly increase engineering difficulty, as we not only need to overwrite the first order gradients (like the gradients for the likelihood), but the second order gradients too (e.g., gradients for marginals or reparameterized samples). In fact, a brute force implementation would not only forego all the advantages of AD engines (like operator-level optimization), but would also require separate implementations for every graph (e.g., chains, trees, semi-Markovs, and Ising-Potts), every inference algorithm (e.g., partition, marginal, sampling, reparameterization, entropy), and higher order gradients. In summary, this is an extremely difficult path that requires clever tricks to efficiently improve a large table of already complicated DP algorithms.\nThis is why we do not interfere with the AD framework and work on more general and efficient algorithms, rather than tailored implementation tricks. With the randomized DP, we do not re-implement anything. We only choose the index by the pre-computed proposal, and introduce the re-indexed potentials to existing efficient libraries (like Torch-struct in Rush, 2020). This is least difficult from an implementation perspective and best exploits the power of efficient structured prediction libraries and AD engines.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C IMPLEMENTATION OF SAMPLED FORWARD ALGORITHM", "text": "Now we discuss how to implement the sample forward in detail. Our implementation aims to set all computation outside the actual DP, and reuse existing optimized DP libraries. Specifically, we:\n1. normalize the log potentials; 2. construct the proposal according to the local potential and a global prior;\n3. obtain the top K 1 index and sample K 2 from the rest, retrieve the potentials according to these indices; 4. correct the bias before the DP, then insert the corrected potentials to an existing DP implementation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Now we explain each step in detail.", "text": "Potential Normalization is a technique useful for numerical stability (see Fu et al., 2021). Specifically, in practice, the log-sum-exp function will saturate (it returns the max input value) when the maximum difference between inputs is larger than 10. So, we normalize all log potentials to be within range [1, m], m < 10. Given the emission potential \u03c6 and transition potential \u03a6 from Eq. 8, we normalize as:\n\u03c6(x t , i) = m * log \u03c6(x t , i) \u2212 min i log \u03c6(x t , i) max i log \u03c6(x t , i) \u2212 min i log \u03c6(x t , i)(84)\n\u03a6(i, j) = m * log \u03a6(i, j) \u2212 min i,j log \u03a6(i, j) max i,j log \u03a6(i, j) \u2212 min i log \u03a6(i, j)(85)\nThen the actual CRF is constructed based on the normalized potentials. Proposal Construction Our proposal is constructed based on the normalized local emission and a prior distribution. Specifically:\nq t (i) = 1 2 \u2022 ( exp\u03c6(x t , i) N i=1 exp\u03c6(x t , i) + exp ||s i || 1 N i=1 exp ||s i || 1 )(86)\nIndex Retrieval For each step t, we retrieve the top K 1 index and get a K 2 sized sample from the rest, as is discussed in the main paper:\n[\u03c3 t,1 , ..., \u03c3 t,K1 , ..., \u03c3 t,N ] = arg sort i {q t (i)} N i=1 (87) [\u03b4 t,1 , ..., \u03b4 t,K2 ] \u223c Categorical{q t (\u03c3 t,K1 + 1), ..., q t (\u03c3 t,N )}(88)\nNote that for simplicity, we use sampling with replacement. For sampling without replacement, see the procedure in Kool et al. 2020 Conventional Forward Before calling an existing implementation of Forward, we need to correct the bias of sampled terms. We do this by multiplying the probability of the sample to its emission potential (which becomes addition in log space). Note that this will be equivalent to multiplying them inside the DP:\n\u03c6 (\u03c3 t,i ) =\u03c6(\u03c3 t,i )(89)\n\u03c6 (\u03b4 t,i ) =\u03c6(\u03b4t, i) + log q t (\u03b4 t,i )(90)\nWe treat any duplicate indices as if they are different, and view the resulting potentials as if they are a new CRF. We also retrieve the transition potentials at each step according to the chosen index. So the new CRF has different transitions across steps. Finally, we run an existing efficient implementation of the Forward algorithm on the new CRF to get the estimated\u03b1 and\u1e90.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "D EXTENSION OF RDP", "text": "In this section, we extend the RDP to more graph structures and more inference operations. We fisrt discuss a randomized inside algorithm for the partition function estimation for tree-structured hypergraphs, which includes dependency trees and PCFGs. Then we discuss a randomized entropy DP estimation on chain-structured graphs. This estimation will call the randomized forward as its subroutine. Finally we discuss howto generalize RDP to the general sum-product algorithm for any graph structure.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D.1 EXTENSIONS TO TREE-STRUCTURED HYPERGRAPHS: RANDOMIZED INSIDE", "text": "This section discusses how to extend RDP to Tree-structured hypergraphs, including Dependency TreeCRFs and PCFGs. We focus on randomizing the Inside algorithm for partition function estimation. Specifically, the core recursion of the inside is:\n\u03b2[i, j, k] = s ijk j\u22121 l=i k1\u2208\u2126,k2\u2208\u2126 \u03b2[i, l, k 1 ]\u03b2[l, j, k 2 ](91)\nwhere \u03b2[i, j, k] is the summation of all subtrees spanning from location i to location j with label k and s jik is the local score and \u2126 is the full state space. Suppose \u2126 is large so we want to sample its subset \u2126 ij to reduce the summation computation. Suppose the proposal is q ijk where k\u2208\u2126 q ijk = 1. Then a randomized inside recursion with K 2 sample is (we omit the top K1 summation for simplicity):\n\u03b2[i, j, k] = s ijk j\u22121 l=i \u03b41\u2208\u2126 il ,\u03b42\u2208\u2126 lj 1 K 2 q il\u03b41 \u03b2[i, l, \u03b4 1 ] \u2022 1 K2q lj\u03b42 \u03b2[l, j, \u03b4 2 ](92)\nThe analysis about bias and variance is similar to the previous analysis (Sec. A) on the linear-chain case.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D.2 EXTENSIONS TO RANDOMIZED ENTROPY DP ESTIMATION", "text": "This section describes a randomized DP for estimating the entropy of a chain-structured model (HMMs and linear-chain CRFs). Recall that the core recursion of conventional entropy DP is:\np(z t = i|z t+1 = j) = \u03a6(i, j)\u03c6(x t+1 , j)\u03b1 t (i) \u03b1 t+1 (j)(93)\nH t+1 (j) = N i=1 p(z t = i|z t+1 = j)[H t (i) \u2212 log p(z t = i|z t+1 = j)](94)\nwhere H t (i) is the intermediate conditional entropy end at state i step t. In our sampled DP, we first call the sampled forward to estimate the alpha variables. Then we re-use the sampled indices for the entropy DP graph, and the core recursion becomes:\np(z t = i|z t+1 = j) = \u03a6(i, j)\u03c6(x t+1 , j)\u03b1 t (i) \u03b1 t+1 (j)(95)\nH t+1 (j) = K2 \u03b4t=1 1 K 2 \u2022 q t (\u03b4 t )p (z t = \u03b4 t |z t+1 = j)[H t (i) \u2212 logp(z t = \u03b4 t |z t+1 = j)](96)\nwhere q t is the proposal at step t. Note how we re-use the estimated alpha variables\u03b1 for our entropy DP and correct the bias of each step.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D.3 EXTENSIONS TO GENERAL SUM-PRODUCT", "text": "This section discusses the extension of sampled DP to general graph structures and message-passing algorithms, following the Bethe variational principle (Wainwright & Jordan, 2008). Recall the general message-passing algorithm computes pseudo marginals by recursively updating the message at each edge:\nM ts (x s ) \u2190 \u03ba x t \u03c6 st (x s , x t )\u03c6 t (x t ) u\u2208N (t)/s M ut (x t )(97)\nwhere M ts (x s ) denotes the message from node t to node s evaluated at X s = x s , \u03c6 st is the edge potential, \u03c6 t is the node potential, N (t)/s denotes the set of neighbor nodes of t except s, and \u03ba the normalization constant. At convergence, we have the pseudo marginals\n\u00b5 s (x s ) = \u03ba\u03c6 s (x s ) t\u2208N (s) M * ts (x s )(98)\nand the Bethe approximation of the log partition function is given by the evaluation of the Bethe variational problem:\nlog Z Bethe = \u03c6 \u00b5 + s H s (\u00b5 s ) \u2212 s,t I st (\u00b5 st )(99)\nwhere H denotes marginal entropy and I denotes marginal mutual information.\nWe consider the application of randomization to the computation of pseudo marginals and the Bethe log partition. Note that the Bethe approximation will be exact if the underlying graph is a tree. We consider a local proposalq combined with a global prior \u03c4 :\nq(x s ) = 1 2 (q(x s ) + \u03c4 (x s ))(100)\nwhere the prior \u03c4 depends on our knowledge of specific problem structures. As one can always retreat to uniform proposals, we can explore more advanced choices. Forq, one may consider: (a) local normalization as(q)(x s ) = \u03c6(x s )/ s \u03c6(x s ) and (b) pre-computed mean-field approximations from algorithms like SVI (Hoffman et al., 2013). For \u03c4 , one can further use a frequency-based empirical prior estimated from the data. To determine which nodes to perform DP on, one may consider the following principles:\n\u2022 Special nodes of interest depending on their actual meaning. For example, one may be interested in some x s = 0 (e.g., if a user is under a bank fraud threat). \u2022 Nodes with large local weight \u03c6(x s ).\n\u2022 Nodes with large global weight \u03c4 (x s ).\n\u2022 Loop elimination. If two nodes have similarly small local and global weight, we could drop those which eliminate loops in the resulting graph. Also note that we would prefer removing small-weighted nodes for loop elimination.\nWith the above principles, one may construct three subsets to perform the sum-product:\n\u2022 \u2126 1 including nodes of special interest, where we perform exact computation.\n\u2022 \u2126 2 from the top items of the proposal, where we also perform exact computation.\n\u2022 \u2126 3 by sampling from the remaining items of the proposal (optionally with loop-reduction).\nFor this set of nodes, one needs to correct the estimation by dividing the proposal probabilty: \u03c6(x s ) \u2190 \u03c6(x s )/q(x s ).\nAfter these steps, we treat nodes in \u2126 1 \u2126 2 \u2126 3 as if they are a new model, then feed them to an existing sum-product implementation.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "E EXPERIMENT DETAILS E.1 MODEL ARCHITECTURE DETAILS", "text": "For training LVMs, partition function estimation, and paraphrasing, we use the Huggingface checkpoint of GPT2 model 1 since these experimental variables are more about autoregressive language modeling and generation. For analyzing latent network topologies, we use Huggingface checkpoint of BERT base model 2 since it has been the main focus of most previous analytical work. The decoder LSTM is a one-layer LSTM with hidden state size 762, the same as the size of the contextualized embeddings. It shares the embeddings of the inferred states with the encoder, and uses its own input word embeddings, whose size is also 762. This architecture suffices for training LVMs and inferring latent networks. For paraphrase generation, we change the decoder to be conditional by: (a) using the average word embeddings of content words of the input sentence as its initial hidden state (rather than zero hidden states); (b) letting it attend to the embeddings of content words of the input sentence, and copy from them. This effectively makes this decoder conditional on the BOW of the content words of the input sentence. Then decoding a paraphrase becomes a neural version of slot filling: a sequence of latent states by traversing the network becomes a template of the sentence which we fill with words. For the 20News dataset, we use the data from the sklearn website 4 . We follow its official split and process the sentences with the BERT tokenizer.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E.3 HYPERPARAMETERS AND TRAINING STRATEGIES", "text": "To get meaningful convergence of the latent space without posterior collapse, the following techniques are important: (a). set \u03b2 in the correct range. A large \u03b2 force the posterior to collapse to a uniform prior, while a small \u03b2 encourages the posterior to collapse to a Dirac distribution. (b). use word dropout in initial training epochs, otherwise the decoder may ignore the latent code. (c). use potential normalization, otherwise the logsumexp function used in the forward algorithm may saturate and only return the max of the input, this would consequently lead the posterior to collapse to few \"activated\" states and other states will never be inferred.\nWe further note that a full DP based entropy calculation will cause memory overflow with automatic differentiation. So we approximate it with local emission entropy, i.e., the sum of the entropy of the normalized local emission factors. Compared with the full entropy, this approximation does not directly influence the transition matrix, but encourages more activated states and mitigates the posterior collapse problem. To get a full entropy regularization, one can further regularize the transition matrix towards an all-one matrix, or extend our randomized DP to entropy computation. As the current local entropy regularization is already effective for inducing a meaningful posterior, we leave the full entropy computation to future work.\nOur reported numbers are averaged over \"good enough\" runs. Specifically, for all hyperparameter configurations, we first run three random seeds and get the mean and standard deviation of the performance metrics. If the standard deviation is small enough, we report mean performance metrics (NLL, PPL, and iBLEU) and the standard deviation. If the standard deviation is large, we run extra five seeds. Then we drop runs with bad performance (usually 2-3), and compute the mean and standard deviation of the performance metrics again. In total, we experiment more than 100 runs over more than 7 hyperparameters: (a). learning rate 10 \u22123 , 10 \u22124 , 5 \u00d7 10 \u22124 ; (b). optimizer: SGD, Adam, AdamW; (c). dropout: 0.2, 0.3, 0.4; (d). \u03b2 variable: 10 \u22124 , 5 \u00d7 10 \u22124 , 10 \u22123 , 10 \u22122 ; (e). Gumbel CRF v.s. Gumbel CRF straight-through (f). K 1 v.s. K 2 ; (g). word dropout schedule; and their combinations. Note that different models may achieve best performance with different hyperparameter combinations. We make sure that all models are searched over the same set of hyperparameter combinations, and report average performance metrics over multiple seeds of the best hyperparameter configuration for each model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E.4 TIME COMPLEXITY ANALYSIS", "text": "Table 3 shows the actual efficiency comparison of our method. Time is measured by seconds per batch. For experiments with 500 and 2,000 states, we set K = 100. *When the number of states is small, our method underperforms due to the overhead of constructing the proposal. However, its advantage becomes clear as the number of states becomes large.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E.5 VISUALIZATION PROCEDURE", "text": "This section describes how we produce Fig. 3(B1-2) and Fig. 4(A1-4). We use the sklearn implementation 5 of tsne (Van der Maaten & Hinton, 2008). For Fig. 3(B1-2), the word embeddings are obtained by sampling 8,000 contextualized embeddings from the full word occurrences in the 20News training set. Then we put the sampled word embeddings and the 2,000 states into the tsne function.\nThe perplexity is set to be 30. An important operation we use, for better seperating the states from each other, it to manually set the distances between states to be large, otherwise the states would be concentrated in a sub-region, rather than spread over words. Fig. 4 is produced similarly, except we do not use word embeddings as background, and change the perplexity to be 5. For Fig. 4(A3), we connect the states if their transition potential is larger than a threshold. For Fig. 4(A4), we connect the states if their bigram frequency is larger than a threshold. All our decisions of hyperparameters are for the purpose of clear visualization which includes reducing overlapping, overcrowding, and other issues. We further note that no single visualization method can reveal the full structure of high-dimensional data, and any projection to 2-D plain inevitably induces information loss. We leave the investigation of better visualization methods to future work.", "n_publication_ref": 1, "n_figure_ref": 6}, {"heading": "E.6 STATE-WORD PAIR EXAMPLES", "text": "See Tables 8 and 9 for more sample.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E.7 STATE-STATE TRANSITION EXAMPLES", "text": "See Table 10 for transitions involving stopwords and Table 11 for transitions without stopwords. Also note their differences as transitions involving stopwords are more about syntactic constructions and transitions without stopwords are more about specific meaning.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E.8 STATE TRAVERSAL EXAMPLES", "text": "See Table 12 for more sample.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F.3 CONTROLLING NUMBER OF STATES", "text": "Figure 5 shows state frequency with different N. The long-tail behavior becomes clearer only when N is large enough (larger than 500 in our case).\nF.4 CONTROLLING K 1 V.S. K 2 RATIO Figure 6 show state frequency with different K 1 /K 2 ratios at different N. We highlight that when K 2 = 0, a pure topK summation approach would lead to posterior collapse where there exist inactive states that do not have any density. We also notice that an increasing K 2 consistently increases the frequency of tail states. This observation can be explained by the exploitation-exploration tradeoff, where increasing K 1 would encourage the exploitation of states already confident enough during training (consequently leading to high-frequency head states after convergence) while increasing K 2 would encourage exploring states that are not confident enough during training, leading to a larger tail frequency after convergence.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "F.5 COMPARISON TO RANDOMLY INITIALIZED BERT", "text": "Figure 7 shows the comparison of reconstruction NLL (\u2212 log p(x t |z t , \u2022)) between a randomly initialized BERT and a pretrained BERT. States induced from pretrained BERT are more meaningful than random, making it easier to reconstruct words based on their corresponding states.   We set the number of latent states to 2000. We use a uniform proposal for comparison, and set K 1 (top K sum size) = K 2 = 50. We use RDP with 20, 200, 400 states, which correspond to 1%, 10%, and 20% of the full number of states. We use topK summation as our baseline with 500 and 1000 states (25% and 50% of the full number of states). Figure 8 shows performance of RDP to tree structured hypergraph. Our method still outperforms topK summation (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). Table 6 shows the MSE of our method compared to topK summation, and the results are consistent with Figure 8.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "F.7 RANDOMIZED ENTROPY DP ALGORITHM", "text": "Figure 9 shows the application of RDP to entropy estimation of linear-chain CRFs. Our method consistently outperforms topK summation (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). Table 7 shows the MSE of our method compared to topK summation, and the results are consistent with Figure 9.  We highlight that when K2 = 0, a pure topK summation approach would lead to posterior collapse where there exist inactive states that do not have any density. We also notice that an increasing K2 consistently increases the frequency of tail states.    ", "n_publication_ref": 0, "n_figure_ref": 2}], "references": [{"title": "Topic extraction with non-negative matrix factorization and latent dirichlet allocation", "journal": "", "year": "", "authors": "Olivier Grisel; Lars Buitinck; Chyi-Kwei Yau"}, {"title": "A tale of a probe and a parser", "journal": "Association for Computational Linguistics", "year": "2020-07", "authors": "Josef Rowan Hall Maudslay; Tiago Valvoda; Adina Pimentel; Ryan Williams;  Cotterell"}, {"title": "Designing and interpreting probes with control tasks", "journal": "", "year": "2019", "authors": "John Hewitt; Percy Liang"}, {"title": "A structural probe for finding syntax in word representations", "journal": "", "year": "2019-06", "authors": "John Hewitt; Christopher D Manning"}, {"title": "Stochastic variational inference", "journal": "Journal of Machine Learning Research", "year": "2013", "authors": "D Matthew;  Hoffman; M David; Chong Blei; John Wang;  Paisley"}, {"title": "Efficient inference of crfs for large-scale natural language data", "journal": "", "year": "2009", "authors": "Minwoo Jeong; Chin-Yew Lin; Gary Geunbae Lee"}, {"title": "Unsupervised recurrent neural network grammars", "journal": "", "year": "2019", "authors": "Yoon Kim; Alexander M Rush; Lei Yu; Adhiguna Kuncoro; Chris Dyer; G\u00e1bor Melis"}, {"title": "Estimating gradients for discrete random variables by sampling without replacement", "journal": "", "year": "2020", "authors": "Wouter Kool; Max Herke Van Hoof;  Welling"}, {"title": "Practical very large scale crfs", "journal": "", "year": "2010", "authors": "Thomas Lavergne; Olivier Capp\u00e9; Fran\u00e7ois Yvon"}, {"title": "Posterior control of blackbox generation", "journal": "Association for Computational Linguistics", "year": "2020-07", "authors": "Lisa Xiang; Alexander Li;  Rush"}, {"title": "Raoblackwellized stochastic gradients for discrete distributions", "journal": "PMLR", "year": "2019", "authors": "Runjing Liu; Jeffrey Regier; Nilesh Tripuraneni; Michael Jordan; Jon Mcauliffe"}, {"title": "Unsupervised paraphrasing by simulated annealing", "journal": "", "year": "2020-07", "authors": "Xianggen Liu; Lili Mou; Fandong Meng; Hao Zhou; Jie Zhou; Sen Song"}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF", "journal": "Long Papers", "year": "2016-08", "authors": "Xuezhe Ma; Eduard Hovy"}, {"title": "Disentangling disentanglement in variational autoencoders", "journal": "PMLR", "year": "2019", "authors": "Emile Mathieu; Tom Rainforth; Nana Siddharth; Yee Whye Teh"}, {"title": "Cgmh: Constrained sentence generation by metropolis-hastings sampling", "journal": "", "year": "2018", "authors": "Ning Miao; Hao Zhou; Lili Mou; Rui Yan; Lei Li"}, {"title": "CGMH: Constrained sentence generation by metropolis-hastings sampling", "journal": "", "year": "2019-07", "authors": "Ning Miao; Hao Zhou; Lili Mou; Rui Yan; Lei Li"}, {"title": "Monte carlo gradient estimation in machine learning", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Shakir Mohamed; Mihaela Rosca; Michael Figurnov; Andriy Mnih"}, {"title": "Randomized automatic differentiation", "journal": "", "year": "2020", "authors": "Deniz Oktay; Nick Mcgreivy; Joshua Aduol; Alex Beatson; Ryan P Adams"}, {"title": "Rnnlogic: Learning logic rules for reasoning on knowledge graphs", "journal": "", "year": "2020", "authors": "Meng Qu; Junkun Chen; Louis-Pascal Xhonneux; Yoshua Bengio; Jian Tang"}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "journal": "", "year": "1989", "authors": "Lawerence R Rabiner"}, {"title": "Language models are unsupervised multitask learners. Unpublished manuscript", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "A primer in bertology: What we know about how bert works", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Anna Rogers; Olga Kovaleva; Anna Rumshisky"}, {"title": "Torch-struct: Deep structured prediction library", "journal": "", "year": "2020", "authors": "M Alexander;  Rush"}, {"title": "Efficient learning of sparse conditional random fields for supervised sequence labeling", "journal": "IEEE Journal of Selected Topics in Signal Processing", "year": "2010", "authors": "Nataliya Sokolovska; T Lavergne; O Capp\u00e9; Fran\u00e7ois Yvon"}, {"title": "Joint learning of a dual smt system for paraphrase generation", "journal": "Short Papers", "year": "2012", "authors": "Hong Sun; Ming Zhou"}, {"title": "Fast structured decoding for sequence models", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": "Zhiqing Sun; Zhuohan Li; Haoqing Wang; Di He; Zi Lin; Zhihong Deng"}, {"title": "An introduction to conditional random fields for relational learning. Introduction to statistical relational learning", "journal": "", "year": "2006", "authors": "Charles Sutton; Andrew Mccallum"}, {"title": "What do you learn from context? probing for sentence structure in contextualized word representations", "journal": "", "year": "2019", "authors": "Ian Tenney; Patrick Xia; Berlin Chen; Alex Wang; Adam Poliak; Thomas Mccoy; Najoung Kim; Benjamin Van Durme; Sam Bowman; Dipanjan Das; Ellie Pavlick"}, {"title": "Visualizing data using t-sne", "journal": "Journal of machine learning research", "year": "2008", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"title": "Graphical models, exponential families, and variational inference", "journal": "Now Publishers Inc", "year": "2008", "authors": "J Martin; Michael Irwin Jordan Wainwright"}, {"title": "26 ; form-of 23 ; lack-of 19 ; institute-of 16 ; case-of 15 ; capable-of 14 ; amounts-of 13 ; out-of 12 ; years-of 12 ; department-of 11 ; terms-of 11 960-458 up-to 56", "journal": "", "year": "", "authors": ""}, {"title": "up-with 16 ; problems-with 15 ; came-with 13 ; comeswith 13 ; along-with 12 ; work-with 12 ; contact-with 11 ; wrong-with 10 ; agree-with 10 ; disagree-with 9 628-150 based-on 65", "journal": "", "year": "", "authors": "## Ing"}, {"title": "##s-on 11 ; down-on 9 ; effect-on 9 ; are-on 8 ; working-on 8 ; effects-on 7 ; activities-on 7 ; depend-on 7 ; be-on 6 ; run-on 6 ; depends-on 6 477-1414 have-to 117 ; going-to 45 ; is-to 37 ; had-to 32 ; decided-to 12 ; need-to 11 ; has-to 11 ; having-to 9 ; required-to 9", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": " ##s"}, {"title": "6 ; heard-of 6 ; one-of 4 ; middle-of 4 ; version-of 4 ; beginning-of 4 ; aware-of 4 1820-276 come-out 30 ; came-out 17 ; coming-out 14", "journal": "put-out", "year": "", "authors": " ##s"}, {"title": "", "journal": "", "year": "", "authors": " ##d"}, {"title": "produced-by 6 ; followed-by 6 ; defined-by 4 ; committed-by 4 ; hit-by 4 ; supported-by 4", "journal": "", "year": "", "authors": "# "}, {"title": "128 ; i-hope 66 ; i-suspect 28 ; i-assume 24 ; i-doubt 18 ; i-suppose 11 ; i-guess 11", "journal": "", "year": "", "authors": ""}, {"title": "came-up 11 ; stand-up 11 ; set-up 11 ; bring-up 8 ; show-up 8 ; comes-up 7", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": "## Er"}, {"title": "State transition examples, with stopwords Transition Bigram -Occurrence 371-371 health-care 14 ; side-effects 8 ; im-##mun 4", "journal": "infectious-diseases", "year": "", "authors": ""}, {"title": "", "journal": "medicine", "year": "", "authors": " ##thic"}, {"title": "##mina-##tion 9 ; bash-##ing 7", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": " ##dal-##izing"}, {"title": "##ras-##ing 5", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": "##band-## Ing"}, {"title": "##ress-##ing 4 ; cab-##ling 4 ; adapt-##er 4 ; cluster-##ing 4 ; sha-##ding 4 931-931 gamma-ray 17 ; lead-acid 9", "journal": "", "year": "", "authors": ""}, {"title": "national-security 5 ; cold-blooded 4 ; health-care 4 ; human-rights 4 ; im-##moral 4 ; prophet-##ic 4", "journal": "", "year": "", "authors": ""}, {"title": "1246-1246 bit-##net 35 ; tel-##net 12 ; use-##net 7 ; phone-number 7 ; dial-##og 6", "journal": "", "year": "", "authors": ""}, {"title": "##p-site 5 ; phone-calls 5 ; bit-block 5 ; net-##com 4 ; bat-##f 4", "journal": "", "year": "", "authors": ""}, {"title": "##t-##net 4 ; phone-call 4 ; arc-##net 3", "journal": "", "year": "", "authors": ""}, {"title": "##dal-##izing 4 ; obey-##ing 4 ; robb-##ing 3", "journal": "", "year": "", "authors": ""}, {"title": "##ov-##ing 3 ; dial-##ing 3 ; contend-##ing 3", "journal": "", "year": "", "authors": ""}, {"title": "##upt-##ing 3 ; rough-##ing 3 ; contact-##ing 3 ; bash-##ing 3 ; favor-##ing 2 202-202 western-reserve 21 ; case-western 20 ; ohio-state 19 ; united-states 10 ; penn-state 5 ; african-american 5 ; north-american 5 ; middle-eastern 5 ; polytechnic-state 4", "journal": "", "year": "", "authors": ""}, {"title": "##l-bus 5 ; bit-color 5", "journal": "", "year": "", "authors": ""}, {"title": "##p-posting 3 ; computer-graphics 3 ; wire-##frame 3 ; bitgraphics", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": " ##eg"}, {"title": "6 ; es-##crow 6 ; key-es 5 ; high-power 4 ; local-bus 4 ; low-level 4 ; high-speed 3 ; minor-league 2 ; health-service 2 ; regular-season 2 ; mother-##board 2 1702-1702 mile-##age 8 ; engine-compartment 5 ; semi-auto 5", "journal": "", "year": "", "authors": " ##frame"}, {"title": "jp-##eg 11 ; encryption-algorithm 8", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": " ##per"}, {"title": "", "journal": "", "year": "", "authors": " ##mb"}, {"title": "encryption-devices 5 ; silicon-graphics 4 ; disk-drive 4 ; floppy-drive 4", "journal": "", "year": "", "authors": " ##ga"}, {"title": "##tile-##s 7 ; azerbaijani-##s 6", "journal": "", "year": "", "authors": ""}, {"title": "##tar-##s 6 ; mormon-##s 5 ; sniper-##s 5 ; physicist-##s 4", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "atomic-energy", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": " ##com"}, {"title": "via-anonymous 5 ; private-sector 5 ; available-via 4 ; general-public 4 ; communityoutreach 4", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": " ##zie"}, {"title": "ibm-pc 10 ; ms-##dos 7", "journal": "", "year": "", "authors": ""}, {"title": "-1919 3 ; energy-signature 2 ; charlotte-##sville 2 ; environment-variables 2 ; duty-cycle 2 ; second-period 2 ; spin-state 2 ; power-consumption 2", "journal": "", "year": "", "authors": " ##ian"}, {"title": "1579-1579 m-sorry 5 ; news-reports 4 ; heard-anything 4 ; ran-##ting 3", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (A): Sampled summation of an array; in the dense case the proposal is important for variance reduction, while in the long-tailed case, topK summands are important; (B): core recursion step of the Randomized Forward algorithm. We get topK and sample from the proposal (black and grey bars); Errors stem from the difference (green bars) between the oracle proposal\u00e3 and constructed proposalq; (C): Inferring latent states within the BERT representation space.We parametrize the CRF factors with vector products; the relations between states and contextualized embeddings together form a latent network (Fig.3 and 4); (D): Experimental protocol; we first study the basic properties of RDP (steps 1, 2) and then integrate RDP into a LVM for inferring the structure of the representation space (steps 3, 4). Best viewed in color.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: (A1): Frequent words partake in more latent states than rare words (presumably because they are polysemous); (A2 and A3): The distribution of states is also Zipfian, as most frequent states generate most words (the orange portion in A2 is almost indistinguishable); (B): t-SNE (Van der Maaten & Hinton, 2008) visualization of latent network induced from BERT; (B1): Words and their corresponding latent states. For states, the size of circle indicates frequency (\u2248 aggregated posterior probability) and color thickness means level of contextualization; a state with deeper blue color tends to generate content words (whose meaning is less dependent on context); lighter blue corresponds to stopwords (which are more contextualized); words are also colored by number of states (\u2248 number of linguistic roles); red color densities mean a word is generated by several states; (B2) and (C): sample from p * (x)q \u03c6 (z|x). Our method discovers a spectrum of meaningful states which exhibit both morpholigical, syntactic and semantic functionalities.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "155 | in-reality 6 | in-particular 5 | in-short 5 | in-itself 4 | in-essence 4 | in-general 4 698-145 to-believe 30 | to-prove 10 | to-assume 7 | to-check 6 | to-test 5 | to-claim 4 | to-argue 4 To + verb, infinitive 1712-698 want-to | like-to 24 | wanted-to 18 | wants-to 17 | wish-to 10 | wishes-to 9 | designed-to 6 Verb + to Passive voice", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4: (A1): Geometrical differences between top and tail states; most lexical variations are encoded by the top 500 states while remaining states represent the long tail; (A3 and A4): Network topology within top 500 states; in (A3) nodes are connected to their top 1 neighbor according to the transition matrix \u03a6 (as a proxy of the empirical prior) and in (A4) according to the most frequent bigram (as a proxy of the aggregated posterior), note how the two are correlated; (A2 and B): Highlighted bigrams and their linguistic interpretation; transitions with stopwords are more about syntax (e.g., to with infinitives or transitive verbs); transitions without stopwords are more about specific meanings. (C): paraphrasing as latent network traversal.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure5: State frequency with different N (number of states). When N = 50, the long-tail behavior is not visible. The long-tail behavior emerges only when N is large enough (larger than 500 in our case).", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure6: State frequency by controlling K1 (sum size) and K2 (sample size). We highlight that when K2 = 0,", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_7", "figure_caption": "for a camera while holding his filthy bananas 0 103 117 1061 1755 0 117 1061 1755 103 a silver clock tower under a black intersection near a tree 0 103 117 441 1755 0 117 1061 1755 103 a silver clock tower on a black intersection near a tree 117 1755 959 103 1061 0 103 103 103 clock tower in the intersection of black near silver tree a motor cycl ist motion , with a blur in the freeway 103 103 1755 0 0 959 117 1061 103 103 117 motor cycl ist in motion blur the freeway at motor cycl with lots of flowers , and v ases of it 117 1061 1082 117 0 1755 1138 1061 1595 a lot of v ases filled with lots of flowers 117 0 1755 117 1755 959 103 441 1755 103 117 1755 959 117 two people walking with dogs in the ocean on a beach with people in the background 117 441 103 1755 117 1755 117 959 two people on beach with dogs in background 117 117 1755 117 441 959 1755 1755 959 117 two people walking with dogs on the beach in the backa pastry in a piece of market area 117 1017 150 1113 0 1061 103 1061 1755 117 a woman is eating a piece of pastry at a market area 0 1755 103 1061 1755 0 1061 117 117 woman eating a pastry at a piece of market area", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results on training LVMs on MSCOCO dataset. Models are run 6 times with different random seeds. use an LSTM decoder with states identical to the encoder (762 for BERT base and GPT2 as in Wolf et al., 2020). More details on experiments and model settings can be found in Appendix E.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Paraphrase generation on the MSCOCO dataset(Fu et al., 2019). Numbers in first block taken from Fu et al.(2020). We report model performance using BLEU 4gram (B4), self BLEU 4gram (sB4), and iBLUE (iB4). Performance is averaged over 3 random seeds.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "A step-by-step implementation guide for our randomized forward algorithm is provided in Appendix section C. The comparison of RDP versus other possible solutions for scaling the structured models is provided in Appendix section B. A detailed description of the model architecture is provided in the Appendix section E.1. A detailed description of data processing is provided in the Appendix section E.2. A detailed description of training strategy, hyperparameter search strategy, and model selection, is provided in Appendix section E.3. A detailed description of visualization procedure is provided in Appendix section E.5. We will release code after the anonymity period.Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423. Yao Fu, Yansong Feng, and John P. Cunningham. Paraphrase generation with latent bag of words. In NeurIPS, 2019. Wiseman, Stuart Shieber, and Alexander Rush. Learning neural templates for text generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3174-3187, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1356. URL https://aclanthology.org/D18-1356. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6. Appendix, table of contents \u2022 Section A. Theoretical analysis of the Sampled Forward algorithm. Bias and variance. \u2022 Section B. Challenges of full DP inference under Automatic Differentiation.", "figure_data": "Sam"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Efficiency comparison with 100/ 500/ 2K states (Forward pass only).", "figure_data": "#States1005002000MethodMem Time Mem Time Mem TimeFULL (Fu et al., 2020)3.9G 0.24s 7.9G 1.07s--TOPK (Sun et al., 2019) 4.1G* 0.23s 4.1G 0.22s 5.2G 0.27sRDP (ours)4.1G* 0.24s 4.1G 0.22s 5.2G 0.27s"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Randomized Forward v.s. TopK Forward. Comparison of MSE.", "figure_data": "MethodDense Intermediate Long-tailTOPK 20% MEM (Sun et al., 2019) 3.87491.01590.1629TOPK 50% MEM (Sun et al., 2019)0.990.25110.0315RDP 1% MEM (ours)0.14610.06690.0766RDP 10% MEM (ours)0.06720.03330.0552RDP 20% MEM (ours)0.04690.02000.0264RDP 50% MEM (ours)0.00780.00410.0046"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Randomized Forward, comparison of different proposal. Same base distribution as Figure 2, all proposals use 10% memeory). RDP V.S. TOPK MSE COMPARISONTable 4 shows the mean square error (MSE) comparison between RDP and TopK on dense, intermediate and long-tail distributions. Again, MSE for RDP is significantly smaller with less memory consumption.", "figure_data": "MethodDense Intermediate Long-tailUNIFORM (baseline)0.655811.05000.1600LOCAL ONLY (ours)2.9089.17110.4817GLOBAL ONLY (ours)0.45310.09610.1003LOCAL + GLOBAL (ours) 0.02840.01730.0222F ADDITIONAL EXPERIMENT RESULTSF.1 F.2 MSE COMPARISON OVER PROPOSALSTable 5 shows the mean square error (MSE) comparison between different proposals. Our pro-posed local and global proposal outperforms a baseline uniform proposal on all distributions (dense,intermediate and long-tail)."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Randomized Inside algorithm for Tree-structured Hypergraphs. Comparison of MSE. RANDOMIZED INSIDE ALGORITHM FOR TREE-STRUCTURED HYPERGRAPH We simulate three unit cases of treecrfs (dense, intermediate and long-tail) by controlling the entropy.", "figure_data": "MethodDense Intermediate Long-tailTOPK INSIDE 20% MEM36.127527.435121.7788TOPK INSIDE 50% MEM2.84222.40432.0479RANDOMIZED INSIDE 1% MEM (ours)26.331237.669848.8638RANDOMIZED INSIDE 10% MEM (ours)1.19371.53071.3843RANDOMIZED INSIDE 20% MEM (ours)0.44550.54490.5997F.6"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Randomized entropy DP v.s. TopK entropy DP on linear-chain CRFs. Comparison of MSE. Comparison of reconstruction likelihood(log p(xt|zt, \u2022)) between a randomly initialized BERT and a pretrained BERT. States induced from pretrained BERT are more meaningful than random, making it easier to reconstruct the words based on their corresponding states. Application of RDP to tree structured hypergraph. Our method consistently outperforms topK summantion (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true partition). Application of RDP to entropy estimation of linear-chain CRFs. Our method consistently outperforms topK summantion (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the true entropy).", "figure_data": "MethodDense Intermediate Long-tailENTROPY TOPK 20% MEM443.784.358.0115ENTROPY TOPK 50% MEM131.822.11.8162ENTROPY RDP 1% MEM (ours)5.92561.98950.6914ENTROPY RDP 10% MEM (ours) 2.11681.29890.3167ENTROPY RDP 20% MEM (ours) 1.32670.73050.2071ENTROPY RDP 50% MEM (ours) 0.30170.14610.0631"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/2020.acl-main.659"}